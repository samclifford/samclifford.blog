<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Journals on Sam Clifford </title>
    <link>/./tags/journals/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2013-11-04 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>Use of credible intervals in scientific papers</title>
      <link>/./2013/11/04/use-of-credible-intervals-in-scientific-papers/</link>
      <pubDate>Mon, 04 Nov 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/11/04/use-of-credible-intervals-in-scientific-papers/</guid>
      <description>&lt;p&gt;I just got comments back from reviewers for two papers that our group has submitted. I’ve done the statistics on both papers and given that Bayesian statistical models have been used in both papers (and we’ve been pretty up front about it) we’ve used credible intervals to discuss the uncertainty in our parameter estimates. A comment I’ve received from a few reviewers (and indeed co-authors) is something along the lines of&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;please replace all instances of ‘credible interval’ with ‘confidence interval’.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While I don’t expect every scientist to be intimately familiar with Bayesian statistics, its philosophy and the fundamental differences between confidence and credible intervals, the repeated use of a term would tend to indicate that it’s not a one-off typo. If repeatedly confronted in a paper with a term that one is not familiar with, surely the prudent course of action is to do a quick Google search to see what that term means in order to better understand the paper that one is reviewing. While the war within statistics over whether the Bayesian approach is actually statistically valid is over, awareness of it in the sciences seems to be lagging far behind. Air quality journals do publish papers that have a Bayesian flavour to the statistics and I have a feeling that just about every author that submits a paper with the correct way of summarising parameter uncertainty in a Bayesian model will face the same comment, “please replace all instances of ‘credible interval’ with ‘confidence interval’”.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/10/10/posterior-samples/</link>
      <pubDate>Thu, 10 Oct 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/10/10/posterior-samples/</guid>
      <description>&lt;p&gt;Open Access publishing is an exciting new development but like any new industry there are those who would seek to unethically make a quick fortune by providing a sub-par service. &lt;em&gt;Science&lt;/em&gt; did some research into this by submitting a fake article with a number of glaring errors (such as non-existent institutions) in it to a number of journals around the world to see who’d accept it. &lt;a href=&#34;http://www.npr.org/blogs/health/2013/10/03/228859954/some-online-journals-will-publish-fake-science-for-a-fee?ft=1&amp;amp;f=1001&#34;&gt;The results are quite interesting&lt;/a&gt;. I know of a few instances of work published in a respectable journal being plagiarised and published in one of these scam journals and one case where the authors hastily retracted their submission once they found out the journal wasn’t anywhere near as prestigious as the editors made out. I support the goals of Open Access and web-based publication but be careful. I’ve been poking around the QUT Wiki looking for information on graduation. In the Higher Degree Research portal there’s a link to a brilliant article (three years old) on &lt;a href=&#34;http://www.timeshighereducation.co.uk/news/how-not-to-write-a-phd-thesis/410208.article&#34;&gt;how not to write a thesis&lt;/a&gt;. Probably good advice for any PhD student looking to graduate sooner rather than later. Elizabeth C. Matsui has been working with Roger Peng for several years; she has &lt;a href=&#34;http://simplystatistics.org/2013/10/08/the-care-and-feeding-of-the-biostatistician/&#34;&gt;some advice on how to cultivate a successful relationship with the biostatistician&lt;/a&gt; you’ve brought on to your medical science project. I think some of these could be adapted to be more general to cover dealing with statisticians. One of the biggest things I could add here is that you need to recognise that asking someone to “help with the statistics” means dedicating some serious time and effort to figuring out a conceptual model (which will be converted into a quantitative model) that addresses the scientific questions you wish to ask. Asking a statistician to “calculate the correlations” or even more broadly “do the statistics” is like asking a scientist to just “run an experiment” or “do the science”. These things all take time, energy and communication. Further to this, don’t just ask for p-values if you’re dealing with a statistician. P-values represent some probabilistic statement, such as “What is the probability of seeing a test statistic at least as extreme as this if the true value of the parameter was zero?”. Sure, you can do simple tests like ANOVA or Chi-Squared goodness of fit but the real value in working with a statistician is being able to develop models that represent assumptions about the data and then assessing whether those assumptions are justified and looking at the inferences that they provide. As my unit co-ordinator for SEB113 pointed out to me the other day, default hypothesis tests of H&lt;sub&gt;0&lt;/sub&gt;: β = 0 at a 5% significance level may not always be sensible and may not even answer the question you’re interested in answering. And finally here’s &lt;a href=&#34;http://www.youtube.com/watch?v=QPKKQnijnsM&#34;&gt;a video that’s almost a year old&lt;/a&gt; that shows a very interesting case of eliciting a distribution from the American public about what they perceive the distribution of wealth in their country is and what they think it is. Turns out the ideal is as far away from the perception as the perception is from the reality.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I know the impact factor&#39;s not the be all and end all, but...</title>
      <link>/./2013/05/09/i-know-the-impact-factors-not-the-be-all-and-end-all-but.../</link>
      <pubDate>Thu, 09 May 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/05/09/i-know-the-impact-factors-not-the-be-all-and-end-all-but.../</guid>
      <description>&lt;p&gt;In Australia, at least, the impact factor of the journals you publish in plays a large role in your advance in academia. Universities are always under pressure to publish their research in more prestigious journals, conflating the impact factor of the journal and the impact of the research published in it. There are many ways journals can game their impact factor, many ways researchers can game the indices that describe the impact of their work, etc. That said, it’s always good to aim to produce research that will be accepted in a high quality journal. I’ve been excited about the PLoS journals since their launch and I believe QUT is a subscribing member, which means our publication fees are covered. It’s one of the best Open Access journal groups around and doesn’t appear to be a cash grab like some other publishers who are attempting to use Open Access as a business model to increase profits rather than because they believe in the free dissemination of research. UPTECH collected fungi and endotoxin data at the 25 schools, and we’re about to submit the fungi paper (which means work must continue on the endotoxin paper). I was considering whether we should submit to &lt;a href=&#34;http://www.plosone.org/&#34;&gt;PLoS One&lt;/a&gt; (IF 2011: 4.092) and then had a look at what other journals they have which may be an appropriate home. I really think once we get the clinical data from &lt;a href=&#34;http://www.woolcock.org.au/&#34;&gt;our Southern collaborators&lt;/a&gt; we should aim to do the best statistical modelling we can. I’m heartened by the fact that the head of the clinical group we’re working with has a strong background in stats and a desire to learn more Bayesian statistics. I don’t know if we can pull it off, but the prospect of having something investigating the role of fungi and endotoxin on child health published in &lt;a href=&#34;http://www.plospathogens.org/&#34;&gt;PLoS Pathogens&lt;/a&gt; (IF 2011: 9.172) is exhilarating.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior Samples</title>
      <link>/./2013/01/21/posterior-samples/</link>
      <pubDate>Mon, 21 Jan 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/01/21/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://blog.revolutionanalytics.com/2013/01/elements-of-statistical-learning-2nd-ed-download.html&#34;&gt;Free download&lt;/a&gt; of Hastie, Tibshirani and Friedman’s “Elements of Statistical Learning”. Hastie and Tibshirani did a lot of the early work on Generalised Additive Models and non-parametric smoothing, albeit in a non-Bayesian framework, so this will no doubt be an invaluable resource. NASA have released &lt;a href=&#34;http://www.nasa.gov/topics/earth/features/2012-temps.html&#34;&gt;a neat visualisation&lt;/a&gt; of spatio-temporal deviation from the mean temperature going back to the 1880s. You have to be careful when generating “pretty” graphics, though, because &lt;a href=&#34;http://andrewgelman.com/2013/01/ugly-ugly-ugly/&#34;&gt;“pretty” infographics can be quite ugly and hard to decode&lt;/a&gt; when you’re not taking into account what the data actually represents. &lt;a href=&#34;http://www.biostat.jhsph.edu/~rpeng/&#34;&gt;Roger Peng&lt;/a&gt; &lt;a href=&#34;http://simplystatistics.org/2013/01/16/review-of-r-graphics-cookbook-by-winston-chang/&#34;&gt;reviews&lt;/a&gt; Winston Chang’s “R Graphics Cookbook”. The conclusion? A pretty good collection of examples about how to complete particular graphing tasks using ggplot2. It’s a simpler book than Hadley Wickham’s “ggplot2” and would be a good introduction to using ggplot2 to achieve a particular goal. &lt;a href=&#34;http://about.jstor.org/rr&#34;&gt;JSTOR’s “Register &amp;amp; Read”&lt;/a&gt; is in beta now. So if you’re not attached to a university or other institution with a JSTOR subscription, you now have the opportunity to read JSTOR articles for free (but it’s not unlimited, at least not yet). Speaking of free access to published research, &lt;a href=&#34;http://www.guardian.co.uk/science/blog/2013/jan/17/open-access-publishing-science-paywall-immoral&#34;&gt;this opinion article&lt;/a&gt; makes the point that our job as scientists is to contribute to the body of human knowledge for the benefit of society and so if a paper isn’t freely available to the public then it isn’t really “published”. Not only that, but to hide research behind a paywall is immoral. I found it very interesting even if I thought some of his arguments were a bit dismissive.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/01/10/posterior-samples/</link>
      <pubDate>Thu, 10 Jan 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/01/10/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://andrewgelman.com/2013/01/a-important-new-survey-of-bayesian-predictive-methods-for-model-assessment-selection-and-comparison/&#34;&gt;A very comprehensive paper by Vehtari and Ojanen discusses various aspects of model assessment and comparison&lt;/a&gt;. I was mentioning to a friend the other day that I don’t think model assessment is taught very well in first year statistics classes. I recognise that you can’t teach everything in a first year class, but I think we should be going beyond R&lt;sup&gt;2&lt;/sup&gt; as a summary of model performance. Even the AIC would be a good start, as it incorporates a trade-off between goodness of fit and model complexity. I am yet to fully read the paper but it’s something I should definitely get my head around. &lt;a href=&#34;http://dynamicecology.wordpress.com/2013/01/03/advice-how-to-review-a-manuscript-for-a-journal/&#34;&gt;Dynamic Ecology has some advice for how to review a journal article&lt;/a&gt;. This will become increasingly relevant for me as a junior researcher. No doubt I’ll be asked whether I could review some submitted articles for a journal in which I am publishing my papers. Having not done it before, the first one will no doubt be the toughest. Perhaps I should ask the more junior reviewer of my thesis panel for any recent experience they’ve had. &lt;a href=&#34;http://www.insidehighered.com/news/2013/01/09/jstor-offer-limited-free-access-content-1200-journals&#34;&gt;JSTOR’s “Register &amp;amp; Read”&lt;/a&gt; program is an attempt to open up access to academic publishing by allowing individuals not affiliated with an organisation with a JSTOR subscription to read three articles every two weeks for free. This will be good for when news articles discuss a study and people are interested enough to go and find the original paper but don’t want to drop $30 to read it. It’s not as far-reaching as Open Access but it’s a good step. &lt;a href=&#34;https://twitter.com/search/%23overlyhonestmethods&#34;&gt;#OverlyHonestMethods&lt;/a&gt; is a hilarious Twitter hashtag that is encouraging scientists to confess their transgressions and is &lt;a href=&#34;http://io9.com/5974256/overlyhonestmethods-is-the-postsecret-of-the-science-world-and-it-is-amazing&#34;&gt;being called&lt;/a&gt; the &lt;a href=&#34;http://www.postsecret.com/&#34;&gt;PostSecret&lt;/a&gt; of the science world.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generating mastery through immersion, imitation and failure</title>
      <link>/./2013/01/03/generating-mastery-through-immersion-imitation-and-failure/</link>
      <pubDate>Thu, 03 Jan 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/01/03/generating-mastery-through-immersion-imitation-and-failure/</guid>
      <description>&lt;p&gt;I came across a webcomic, Doodle Alley, which deals with sustainable creativity and there are a few pages I want to focus on: “&lt;a href=&#34;http://doodlealley.com/2012/10/01/taste-is-your-teacher/&#34;&gt;Taste is your teacher&lt;/a&gt;”, “&lt;a href=&#34;http://doodlealley.com/2012/10/10/be-friends-with-failure/&#34;&gt;Be friends with failure&lt;/a&gt;” and “&lt;a href=&#34;http://doodlealley.com/2012/11/21/practice-does-not-make-perfect/&#34;&gt;Practice does not make perfect&lt;/a&gt;”. They are couched in terms of art, but I think the same arguments apply to academic science as well, particularly writing papers. “Taste” focuses on immersion as a means of improving your work. To adapt the culinary example to statistics: to write a good paper I need to know what good statistics looks like; to know what good statistics looks like I need to read a lot of good statistics. This reminds me of the &lt;a href=&#34;http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1467-9868&#34;&gt;JRSS B&lt;/a&gt; papers I’ve had to read to understand spatial statistics [1, 2]. “Taste” mentions imitation as a way of producing great art and this is explored further in “Practice”. This doesn’t mean word for word plagiarising in the academic arena, but I’d say it does mean that copying the original creator’s style and extending it is part of the road to success. In writing my thesis I looked at previous PhD students’ theses which were held up as being great work. I looked at the way the thesis was structured, the way the ideas were set out within each chapter and the way the conclusion stepped through what the work had resulted in. I obviously couldn’t just “copy” the thesis, because their work is not my work and I need to make a significant original contribution, but the thesis as a whole can definitely serve as a starting point for my imitation of good art. “Friends” starts with the very sobering realisation that the more you immerse yourself in great art (science) the more you realise your art (science) is not great. The solution to this is to embrace failure as a friend and recognise that &lt;a href=&#34;http://www.youtube.com/watch?v=vrBWJLC22mE&#34;&gt;sucking at something is the first step towards being sorta good at something&lt;/a&gt;. The comic raises the point that if you never fail at something how can you grow? One of the first things you learn in impro is that failure is an indication that you have tried. If, as beginners, we never fail we are playing it safe and producing boring work. Only by taking those bold leaps and falling flat on our faces multiple times do we begin to feel comfortable with our failure. Platitudes about failure being a learning experience don’t tend to focus on the idea that failure is something to be embraced but that it’s something that just happens and there’s nothing we can do about it. Failure, in the comic, is evidence that you’re still like a baby in terms of picking up a particular skill. Babies are born knowing absolutely nothing and pick it up by repeating their attempts, failing a little less each time they try. This is the key to mastery. Immerse yourself in your field, get a sense of what great art/science/statistics/food is and keep on trying, taking those bold steps and failing a little less each time. Your first few academic papers will probably not be worth publishing in journals like JRSS B and JASA, but that’s no reason to not aim high, fail (get rejected), try again (revise) and get the best result for what you’ve produced (a less prestigious journal but it’s still published work). Mastery is built through a combination of immersing yourself in others’ great work, imitating that great work, taking bold steps, recognising that you will fail, embracing the idea that you will fail and that you will fail less each time you try not through repetition of the exact same act but by trying different things until you get something that’s “good enough”. Then by repeatedly failing and trying new things you will learn where the pitfalls are and you will fail less and, every so often, create something that is great. [1] Lindgren, F., H. Rue and J. Lindström, (2011) “An explicit link between Gaussian ﬁelds and Gaussian Markov random ﬁelds: The SPDE approach”, JRSS B (&lt;a href=&#34;http://www.math.ntnu.no/inla/r-inla.org/papers/spde-jrssb-revised.pdf&#34;&gt;pdf&lt;/a&gt;) [2] Banerjee, S., A. Gelfand, A. Finley and H. Sang, (2008) “Gaussian predictive process models for large spatial data sets”, JRSS B (&lt;a href=&#34;http://www.biostat.umn.edu/~sudiptob/ResearchPapers/BGFS.pdf&#34;&gt;pdf&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unfinnished business</title>
      <link>/./2012/08/30/unfinnished-business/</link>
      <pubDate>Thu, 30 Aug 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/08/30/unfinnished-business/</guid>
      <description>&lt;p&gt;I’ve just heard from Kerrie Mengersen that the Finnish paper got rejected by BA for not being novel enough to publish there. So now I’m in a situation where I’ve got a paper which is too methodological for an applied stats journal (and far too methodological for an atmospheric science journal) and not a novel enough methodology for a journal as theoretical as BA. If BA don’t think it’s novel enough and it’s not the first time this data’s been published we’ll struggle to get it into something like &lt;a href=&#34;http://www.rss.org.uk/site/cms/contentviewarticle.asp?article=876&#34;&gt;JRSS B&lt;/a&gt; (IF: 3.645), &lt;a href=&#34;http://rsif.royalsocietypublishing.org/&#34;&gt;JRS: Interface&lt;/a&gt; (IF: 4.402) or &lt;a href=&#34;http://www.plosone.org&#34;&gt;PLoS One&lt;/a&gt; (IF: 4.092). Our options, then, seem to be trying an Elsevier journal like &lt;a href=&#34;http://www.journals.elsevier.com/environmental-modelling-and-software/&#34;&gt;EMS&lt;/a&gt; (IF: 3.114) or &lt;a href=&#34;http://www.journals.elsevier.com/computational-statistics-and-data-analysis/&#34;&gt;CSDA&lt;/a&gt; (IF: 1.028), which I’m not keen to do, somewhere like &lt;a href=&#34;http://www.jstatsoft.org&#34;&gt;JSS&lt;/a&gt; (IF: 2.647) or a more applied journal like &lt;a href=&#34;http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1467-9876&#34;&gt;JRSS C&lt;/a&gt; (IF: 0.828, which is quite low). I’d like to put this in a statistics journal because I want to have a career as a statistician rather than just someone who can only work in aerosols. That’s one of the reasons I’m not keen to publish this somewhere like Atmospheric Environment (where both my article and Bjarke’s, the bases of this work, were published). I’m really kicking myself now for not submitting an abstract for this as a contributed talk at ISBA. I would’ve got a BA article out of it. In happier news, I gave a presentation with two other PhD students to BRAG this morning where we talked about INLA. It went well and I think we’ve convinced a few of the others that INLA is pretty cool and worth using. Edit: and I’m getting a lot of mileage out of the Finnish/finish pun.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper journals</title>
      <link>/./2012/08/17/paper-journals/</link>
      <pubDate>Fri, 17 Aug 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/08/17/paper-journals/</guid>
      <description>&lt;p&gt;My first issue of the official ISIAQ journal, &lt;a href=&#34;http://au.wiley.com/WileyCDA/WileyTitle/productCd-INA.html&#34;&gt;Indoor Air&lt;/a&gt;, arrived on my desk this morning. There are some interesting articles but I’m not particularly likely to thumb through a paper journal very often. I prefer getting an electronic copy where I can use the “Find” command to look for phrases and words in which I have a research interest. They offer an online version of the journal, so I wonder if I can contact ISIAQ and let them know I don’t want a paper copy of the journal. Again, I think &lt;a href=&#34;http://ba.stat.cmu.edu/&#34;&gt;Bayesian Analysis&lt;/a&gt; is providing a good service by providing their journal as an online, Open Access resource. Edit: Oh no. I’ve just found a very bad graph. The below figure is from Batterman et al (2012) “&lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0668.2011.00760.x/full&#34;&gt;Sources, concentrations, and risks of naphthalene in indoor and outdoor air&lt;/a&gt;”. Plots based on empirical cumulative density functions should always be checked to ensure that the resulting functions are monotonic. [caption id=“attachment_398” align=“aligncenter” width=“330”][&lt;img src=&#34;http://samcliffordinfo.files.wordpress.com/2012/08/ina_760_f4.gif&#34; title=&#34;INA_760_f4&#34; /&gt;](ina_760_f4.gif) Distributions of indoor naphthalene levels in the four study cities. Sample size = 288 (Battermen et al., 2012)[/caption] Check the plot for Ypsilanti around 75-90%. Those dips should not be there. I’m guessing it’s just a weird artifact of the way they generated the ecdf, but it’s sloppy statistics and I can’t believe the reviewer missed it. Indoor Air has an impact factor of 2.55 and was ranked as A* in 2010 by the &lt;a href=&#34;http://www.arc.gov.au/era/default.htm&#34;&gt;ARC&lt;/a&gt;. I know the impact factor only relates to citations and it’s one graph in a single issue but I always find this sort of thing disappointing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My spoon is too big</title>
      <link>/./2012/07/07/my-spoon-is-too-big/</link>
      <pubDate>Sat, 07 Jul 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/07/07/my-spoon-is-too-big/</guid>
      <description>&lt;p&gt;Rejected. I had submitted the Finnish paper to an applied statistics journal and received, within a working day or two, a response from the reviewer. They said the paper doesn’t focus as much on the application and is in fact more methodological. They go on to make a few suggestions as to how we could improve our method (mainly the forecasting and posterior summary stuff) and that we should submit it to a methods journal (I’m thinking Bayesian Analysis). Having not studied much statistics in undergrad and learning Bayesian statistics to any degree after starting my PhD, I have felt like the work I’ve been doing was just applying methods that others had developed and that I wasn’t doing much statistics research. My first paper was more or less just that, fitting a GAM to some air quality data. It’s a nice paper, I’m proud of it, and it was a very valuable piece of work in terms of me understanding GAMs and semi-parametric regression; it took a lot of work to get there. At the same time, it felt a bit like I was using an R package to do some magic. So while the Finnish paper has been rejected by a journal, I am buoyed by the reviewer’s comments about it being a well written paper that outlines a nice method with some solid statistics behind it. We have some changes to make (and I agree with the comments they make about our posterior summaries) but the thought of publishing a methods paper is very exciting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two papers submitted</title>
      <link>/./2012/07/06/two-papers-submitted/</link>
      <pubDate>Fri, 06 Jul 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/07/06/two-papers-submitted/</guid>
      <description>&lt;p&gt;In the last two weeks I’ve sent two papers to Annals of Applied Statistics and uploaded them to the arXiv. This fulfills the minimum publication requirements for a PhD by publication at QUT. So it’s a milestone on the way to completion. Now I just have to sort out a fourth paper as backup, give two presentations at next week’s conference, organise a final seminar session, compile all my work into a thesis and get it to my supervisors. On top of my immediate thesis work on spatio-temporal modelling, I’ve got a paper for Healthy Buildings on methods for analysing the UPTECH survey data that’ll get published, a paper on which I’m coauthor (particle losses in a thermodenuder system) that’s almost finished and the microbiology paper I wrote about previously. Busy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistics - not just for statisticians</title>
      <link>/./2012/06/01/statistics---not-just-for-statisticians/</link>
      <pubDate>Fri, 01 Jun 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/06/01/statistics---not-just-for-statisticians/</guid>
      <description>&lt;p&gt;An article in &lt;a href=&#34;http://theconversation.edu.au/top-cited-academics-honoured-but-wheres-the-humanity-7348&#34;&gt;The Conversation&lt;/a&gt; made the rounds in our office yesterday and prompted a discussion with one of my colleagues about the role of statistics in scientific papers. The article itself talks about Australians who received an award from Thomson Reuters on the basis of publishing frequently cited work and discusses the different culture in science and the humanities when it comes to publication and citation. The basic argument is that the awards are skewed to the sciences where publication is traditionally by journal article and citation in other journal articles is picked up more than in the humanities where monographs/books are the traditional means of disseminating information. One of the academics interviewed makes the point that science is much more collaborative, with researchers working in teams to achieve the goals of a project. One of the email discussants in our group offered the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Graham Farquhar from the Australian National University is Australia’s citation laureate. Graham’s belief is this: start with a really good hypothesis driven question that no-one has answered and answer it. Deliver the answer with robust science and, voila, you have highly cited work. Coming up with the questions is hard though. Thinking seven impossible questions before breakfast is probably a daily norm in Graham’s existence. Doing this, and maintaining a normal personality is difficult though, but he manages quite well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A colleague talked to me about this idea of Farquhar’s in terms of a paper of his that he thought did well to answer an unanswered question. The aim of the paper was to offer an explanation for a physical phenomenon that is known in aerosols but it’s not well known why this phenomenon occurs. We then got into an argument about the robustness of the science, not based on the science that had been done (it was a synthesis of previous work by other authors), but because the evidence hadn’t all been included in a model that drew all these reviews together in a statistical manner. Scientists love &lt;em&gt;t&lt;/em&gt; tests. They are simple tests which tell you if there’s a difference between two groups and are appropriate in some, but not all, instances, and are a good first step for exploratory data analysis. I was making the point that a Bayesian meta-analysis would have been far more appropriate as it’s a technique which is specifically designed to draw multiple sources of information together in a single model to provide a better estimate of an effect size. The Bayesian approach would have also helped here as a review of the literature could be used to determine priors such that even if no new data was collected for this paper, inferences could be drawn based on the current state of knowledge. I think every research group should hire a statistician, have them retrain the researchers in how to use statistics, including GLMs/GAMs, spatial analysis, time series methods, and Bayesian inference in order to build capacity within the group. The statistician can then work with the scientists to ensure that new papers include the best analysis of results possible and to also review old papers to see if there’s any low hanging fruit in terms of interesting experiments/observations which could be re-analysed with something more than ANOVA or linear regression. Statistics isn’t just for statisticians; we need to get away from the idea that doing more than the bare minimum for a paper is going too far. Apparently QUT is reviewing the way it teaches statistics to science students (something which is long overdue, my recollection is that MAB101 is an awful unit full of statistical techniques relevant to agricultural trials) so I’m hopeful that we can teach students statistical techniques that are relevant to them in an exciting way. I hope we don’t swing too far and just give them the exact tools they need and nothing else, because then we’re limiting graduates. If you’ve got the brain power to deal with atmospheric chemistry and/or physics, you should be able to handle statistics. Not everyone goes through to do advanced level statistical units but it shouldn’t be too much of a jump from collecting experimental data to analysing it in R with a package appropriate to your field. Edited to add: The point I’m trying to make here is that all researchers should have a basic understanding of exploratory/descriptive data analysis, simple GLMs (even if it’s just using glm() in R) and the ability to communicate the physical results in terms of the statistical modelling. I don’t expect that all scientists will go and become proficient in the use of Bayesian non-parametrics, but scientists should be able to start with some scatter plots, box plots and ANOVA to look for differences and then use regression to explain those differences.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gelman and Robert</title>
      <link>/./2012/05/25/gelman-and-robert/</link>
      <pubDate>Fri, 25 May 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/05/25/gelman-and-robert/</guid>
      <description>&lt;p&gt;Kerrie Mengersen, one of my supervisors, is visiting some colleagues in France at the moment. It appears that one of the outputs of this visit is a discussion paper, “In praise of the referee”, written with Nicolas Chopin, Andrew Gelman and Christian Robert (&lt;a href=&#34;http://arxiv.org/abs/1205.4304&#34;&gt;arXiv&lt;/a&gt;). There’s been a lot of discussion recently about the role of journals, publishers and reviewers in academic publishing ranging from defending the &lt;em&gt;status quo&lt;/em&gt; to totally overhauling the system by shedding paper journals and moving everything online to a distributed network of institutional ePrints repositories. The paper by Chopin et al. makes two recommendations after a lengthy discussion about where we find ourselves&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Post-publication peer review with comment threads on arXiv and/or a filtering service where instead of acting as reviewers, editorial boards pick out worthwhile new research as a list of recommended reading.&lt;/li&gt;
&lt;li&gt;A reviewer commons where academics are taught how to review and the reviewer reports (and their names) are published alongside the article.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The article is worth a read. I figured I’d also share an old Gelman blog post that he’s recently linked to, “&lt;a href=&#34;http://andrewgelman.com/2009/07/advice_on_writi/&#34;&gt;Advice on writing research articles&lt;/a&gt;”. The seven pieces of general advice are well worth remembering. It’s basically “Start with your conclusions and work backwards towards your methods”. Christian Robert has &lt;a href=&#34;http://xianblog.wordpress.com/2012/05/25/xian-australian-tour-2012/&#34;&gt;published the dates for his Australian tour&lt;/a&gt;. I’m not particularly interested in ABC but the talk on Rao-Blackwellisation looks interesting. Anyone who’s interested in learning a bit about what Bayesian simulation is, without necessarily having a statistics background, would do well to attend the public talk entitled “Simulation as a universal tool for statistics”, which you could probably consider as a popular science talk.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Elsevier - spatial statistics</title>
      <link>/./2012/05/11/elsevier---spatial-statistics/</link>
      <pubDate>Fri, 11 May 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/05/11/elsevier---spatial-statistics/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://xianblog.wordpress.com/2012/05/11/new-elsevier-journal/&#34;&gt;Christian Robert has just blogged&lt;/a&gt; about a new Elsevier journal - &lt;a href=&#34;http://www.journals.elsevier.com/spatial-statistics/&#34;&gt;Spatial Statistics&lt;/a&gt;. I think the idea of a dedicated spatial statistics journal is fantastic, but I don’t like that Open Access publishing in this journal will cost at least $3000, that the institutional subscription rate is around $650 per year and that authors can post the final version of the text (not the published paper, which is fair enough) on their personal or institutional website but not on the arXiv (so good luck with that if you’re at a university that doesn’t keep up with the times) (but you can post pre-prints to the arXiv). With mild regret, I won’t be submitting to either Spatial Statistics or &lt;a href=&#34;http://www.journals.elsevier.com/spatial-and-spatio-temporal-epidemiology/&#34;&gt;Spatial and Spatio-temporal Epidemiology&lt;/a&gt;, even though these two journals are right up my alley and published by a respected&lt;sup&gt;[&lt;em&gt;by whom?&lt;/em&gt;]&lt;/sup&gt; publisher&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
