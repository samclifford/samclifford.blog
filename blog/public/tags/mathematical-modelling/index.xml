<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Mathematical Modelling on Sam Clifford </title>
    <link>/./tags/mathematical-modelling/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2014-09-24 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2014/09/24/posterior-samples/</link>
      <pubDate>Wed, 24 Sep 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/09/24/posterior-samples/</guid>
      <description>&lt;p&gt;SEB113 students really seemed to enjoy looking at mathematical modelling last week. &lt;a href=&#34;http://books.google.com.au/books?id=nM_X1PTccloC&amp;amp;lpg=PA111&amp;amp;ots=dCzeaDpIeb&amp;amp;dq=lotka%20volterra%20fulford%20and%20barnes&amp;amp;pg=PA106#v=onepage&amp;amp;q=lotka%20volterra%20fulford%20and%20barnes&amp;amp;f=false&#34;&gt;The Lotka-Volterra equations&lt;/a&gt; continue to be a good teaching tool. A student pointed out that when reviewing the limit idea for derivatives it’d be useful to show it with approximating the circumference of a circle using a polygon. So I knocked this up: &lt;a href=&#34;https://samcliffordinfo.files.wordpress.com/2014/09/approximations.png&#34;&gt;&lt;img src=&#34;approximations.png?w=300&#34; alt=&#34;approximations&#34; /&gt;&lt;/a&gt; Are you interested in big data and/or air quality? &lt;a href=&#34;https://www.qut.edu.au/research/our-research/student-topics/big-data-and-nanoparticles-modelling-complex-spatio-temporal-variation&#34;&gt;Consider doing a PhD with me&lt;/a&gt;. This week I showed in the workshop how Markov chains are a neat application of linear algebra for dealing with probability. We used &lt;a href=&#34;http://setosa.io/blog/2014/07/26/markov-chains/&#34;&gt;this interactive visualisation&lt;/a&gt; to investigate what happens as the transition probabilities change. Zoubin Ghahramani has written &lt;a href=&#34;http://rsta.royalsocietypublishing.org/content/371/1984/20110553.abstract&#34;&gt;a really nice review paper&lt;/a&gt; of Bayesian non-parametrics that I really recommend checking out if you’re interested in the new modelling techniques that have been coming out in the last few years for complex data sets. &lt;a href=&#34;http://www.wired.com/2014/09/exercism/&#34;&gt;Exercism.io&lt;/a&gt; is a new service for learning how to master programming by getting feedback on exercises.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lotka-Volterra and Bayesian statistics and teaching</title>
      <link>/./2014/08/27/lotka-volterra-and-bayesian-statistics-and-teaching/</link>
      <pubDate>Wed, 27 Aug 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/08/27/lotka-volterra-and-bayesian-statistics-and-teaching/</guid>
      <description>&lt;p&gt;One of the standard population dynamics models that I learned in my undergrad mathematical modelling units was the Lotka-Volterra equations. These represent a very simple set of assumptions about populations, and while they don’t necessarily give physically realistic population trajectories they’re an interesting introduction to the idea that differential equations systems don’t necessarily have an explicit solution. The assumptions are essentially: prey grow exponentially in the absence of predators, predation happens at a rate proportional to the product of the predator and prey populations, birth of predators is dependent on the product of predator and prey populations, predators die off exponentially in the absence of prey. In SEB113 we cover non-linear regressions, the mathematical models that lead to them, and then show that mathematical models don’t always yield a nice function. We look at equilibrium solutions and then show that we orbit around it rather than tending towards (or away from) it. We also look at what happens to the trajectories as we change the relative size of the rate parameters. Last time we did the topic, I posted about using the logistic growth model for our Problem Solving Task and it was pointed out to me that the model has a closed form solution, so we don’t explicitly need to use a numerical solution method. This time around I’ve been playing with using Euler’s method inside JAGS to fit the Lotka-Volterra system to some simulated data from sinusoidal functions (with the same period). I’ve put a bit more effort into the predictive side of the model, though. After obtaining posterior distributions for the parameters (and initial values) I generate simulations with lsode in R, where the parameter values are sampled from the posteriors. The figure below shows the median and 95% CI for the posterior predictive populations as well as points showing the simulated data. &lt;a href=&#34;https://samcliffordinfo.files.wordpress.com/2014/08/lv2.png&#34;&gt;&lt;img src=&#34;lv2.png?w=300&#34; alt=&#34;lv&#34; /&gt;&lt;/a&gt;The predictions get more variable as time goes on, as the uncertainty in the parameter values changes the period of the cycles that the Lotka-Volterra system exhibits. This reminds me of a chat I was having with a statistics PhD student earlier this week about sensitivity of models to data. The student’s context is clustering of data using overfitted mixtures, but I ended up digressing and talking about Edward Lorenz’s discovery of chaos theory through a meteorological model that was very sensitive to small changes in parameter values. The variability in the parameter values in the posterior give rise to the same behaviour, as both Lorenz’s work and my little example in JAGS involve variation in input values for deterministic modelling. Mine was deliberate, though, so isn’t as exciting or groundbreaking a discovery as Lorenz but we both come to the same conclusion: forecasting is of limited use when your model is sensitive to small variations in parameters. As time goes on, my credible intervals will likely end up being centred on the equilibrium solution and the uncertainty in the period of the solution (due to changing coefficient ratios) will result in very wide credible intervals. It’s been a fun little experiment again, and I’m getting more and more interested in combining statistics and differential equations, as it’s a blend of pretty much all of my prior study. The next step would be to use something like MATLAB with a custom Gibbs/Metropolis-Hastings scheme to bring in more of the computational mathematics I took. It’d be interesting to see if there’s space for this sort of modelling in the Mathematical Sciences School’s teaching programs as it combines some topics that aren’t typically taught together. I’ve heard murmurings of further computational statistics classes but haven’t been involved with any planning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Combining differential equations and regression</title>
      <link>/./2014/04/22/combining-differential-equations-and-regression/</link>
      <pubDate>Tue, 22 Apr 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/04/22/combining-differential-equations-and-regression/</guid>
      <description>&lt;p&gt;Last week I gave my first lecture for the semester to the SEB113 students. While they tend to not have a particularly strong mathematics background I got some very positive feedback on how much they enjoyed learning about mathematical modelling. We revised differentiation, what derivatives are and then jumped into a bit about formulating differential equations from words that represent the assumptions that the model makes. The bulk of that week’s lecture is showing where the non-linear regression models we used in the previous week (first order compartment, asymptotic, biexponential) come from. To do this we have a chat about exponential growth and decay models as some of the easiest differential equation models to deal with. I show them how we solve the exponential model exactly and then make reference to the fact that I don’t expect them to solve these equations in this subject. We show the solutions to the DE systems and make it very clear that the non-linear regression models are the solutions to differential equations that represent different assumptions. We finish the lecture off with a section on how we can’t always get a “pen and paper” solution to differential equations and so sometimes we either simplify the system to one we can solve (alluding to perturbation methods) or give it to a numerical solver (alluding to computational mathematics). Because it’s how I learned about numerical solutions to DEs I showed the students the Lotka-Volterra model and discussed why we can’t solve X(t) and Y(t) and so have to use numerical methods. For different parameter values we get variations on the same behaviour: cyclic patterns, prey population growth followed by predator population growth followed by overconsumption of prey leading to fewer predators being born to replace the dying. Many students seemed to enjoy investigating this model in the workshops, as it’s quite different to everything we’ve learned so far. Solution is via the deSolve package in R but we introduce the students to Euler’s method and discuss numerical instability and the accumulation of numerical error. I finish off the lecture with a chat about how regression tends to make assumptions about the form of the mean relationship between variables so we can do parameter estimation and that differential equations give us a system we can solve to obtain that mean relationship. I state that while we &lt;em&gt;can&lt;/em&gt; solve the DE numerically while simultaneously estimating the parameter it is way outside the scope of the course. I had a bit of time this morning to spend on next week’s lecture material (linear algebra) so decided to have a go at numerical estimation for the logistic growth model and some data based on the Orange tree circumference data set in R with JAGS/rjags. It’s the first time I’ve had a go at combining regression and numerical solutions to DEs in the same code, so I’ve only used Euler’s method. That said, I was very happy with the solution and the code is provided below the cut. [code language=“r”] # euler.bugs model{ y[1] ~ dnorm(mu[1], tau.y) mu[1] &amp;lt;- y0 + dt * exp(lr) * y0 * (1 - y0/K) for (i in 2:n){ y[i] ~ dnorm(mu[i], tau.y) mu[i] &amp;lt;- y[i-1] + dt * exp(lr) * y[i-1] * (1 - y[i-1]/K) } for (i in 1:n){ y.p[i] ~ dnorm(mu[i], tau.y) } lr ~ dnorm(0, 1e-6) K ~ dnorm(0, 1e-6) y0 ~ dunif(0.001, 1000) tau.y ~ dgamma(0.001, 0.001) } [/code] Which can be called appropriately with [code language=“r”] library(rjags) library(ggplot2) my.orange &amp;lt;- data.frame(age=seq(100, 1900, by=200), circumference = c(32, 47, 73, 101, 134, 162, 182, 194, 205, 214)) dt &amp;lt;- 10 orange.dat &amp;lt;- data.frame(age=seq(0, 3000, by=dt),circumference=NA) orange.dat[match(table=orange.dat&lt;span class=&#34;math inline&#34;&gt;\(age, x=my.orange\)&lt;/span&gt;age),“circumference”] &amp;lt;- my.orange[, “circumference”] orange.m &amp;lt;- jags.model(file=“euler.bugs”, data=list(y=orange.dat&lt;span class=&#34;math inline&#34;&gt;\(circumference, n = nrow(orange.dat), dt=dt),inits=list(y0=50, K=500, lr=-5)) orange.b &amp;lt;- jags.samples(model=orange.m, n.iter=10000, variable.names=c(&amp;quot;y.p&amp;quot;,&amp;quot;K&amp;quot;,&amp;quot;lr&amp;quot;,&amp;quot;y0&amp;quot;)) orange.pred &amp;lt;- coda.samples(model=orange.m, n.iter=1000, variable.names=c(&amp;quot;y.p&amp;quot;)) orange.sum &amp;lt;- summary(orange.pred, q=c(0.025, 0.5, 0.975)) orange.gg &amp;lt;- data.frame(orange.sum\)&lt;/span&gt;quantiles) orange.gg&lt;span class=&#34;math inline&#34;&gt;\(age &amp;lt;- orange.dat\)&lt;/span&gt;age windows(height=3.5) ggplot(data=orange.gg, aes(x=age, y=X50.)) + geom_line() + geom_line(aes(y=X2.5.), lty=2) + geom_line(aes(y=X97.5.), lty=2) + geom_point(data= my.orange, aes(y=circumference), alpha=0.5) + theme_bw() + xlab(“Time (days)”) + ylab(“Tree circumference (mm)”) [/code] The resulting picture can be seen below. [caption id=“attachment_1226” align=“aligncenter” width=“625”][&lt;img src=&#34;predicting.png?w=625&#34; alt=&#34;Prediction of tree circumference from logistic growth differential equation&#34; /&gt;](&lt;a href=&#34;http://samcliffordinfo.files.wordpress.com/2014/04/predicting.png&#34; class=&#34;uri&#34;&gt;http://samcliffordinfo.files.wordpress.com/2014/04/predicting.png&lt;/a&gt;) Prediction of tree circumference from logistic growth differential equation[/caption]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior Samples</title>
      <link>/./2013/12/02/posterior-samples/</link>
      <pubDate>Mon, 02 Dec 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/12/02/posterior-samples/</guid>
      <description>&lt;p&gt;Thiago Martins has posted &lt;a href=&#34;http://tgmstat.wordpress.com/2013/11/28/computing-and-visualizing-pca-in-r/&#34;&gt;a neat little tutorial&lt;/a&gt; about using R to calculate and visualise Principle Components Analysis, using Fisher’s Iris data. PCA is something I’ve struggled with as I’ve gone further into statistics, as it comes across as being based on mathematics rather than statistics. I’d like to learn more about the Indian Buffet Process and associated non-parametric Bayesian methods but if I’m going to be looking at long and wide data sets (say, UPTECH questionnaire data) I’d like to have somewhere to start. It looks like this may provide that. Rasmus Bååth’s done &lt;a href=&#34;http://www.sumsar.net/blog/2013/11/easy-laplace-approximation/&#34;&gt;a tutorial on Laplace Approximations in R&lt;/a&gt; (hat tip to Matt Moores for this one). Laplace Approximations are an alternative to MCMC simulation that can provide good approximations to well-behaved posterior densities in a fraction of the time. The tutorial deals with the issue of reparameterisation for when you’ve got parameters which have bounded values (such as binomial proportions). As a piece of trivia, Thiago (above) is based at NTNU where &lt;a href=&#34;http://www.r-inla.org/&#34;&gt;R-INLA&lt;/a&gt; is developed. I’m at the &lt;a href=&#34;http://emac2013.com.au/&#34;&gt;emac2013&lt;/a&gt; conference this week. We’re about half way through day one of the talks (of three) and there’s already been some fascinating stuff. Professor Robert Mahony (ANU) gave a talk that shows that the development of more advanced unmanned aerial vehicles (UAVs, drones) involves some quite complex but elegant mathematics, involving Lie group symmetries, rather than just coming up with cooler robots. Hasitha Nayanajith Polwaththe Gallage (QUT) showed some really interesting particle method (mesh-free) modelling where forces and energies were used to determine the shape of a red blood cell that had just ejected its nucleus.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Non-statistics topics in SEB113</title>
      <link>/./2013/09/27/non-statistics-topics-in-seb113/</link>
      <pubDate>Fri, 27 Sep 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/09/27/non-statistics-topics-in-seb113/</guid>
      <description>&lt;p&gt;While SEB113 is all about Quantitative Methods in Science it’s been quite notable that the last two weeks have been mathematical rather than statistical. We started the semester with visualisation and then headed into summary statistics, confidence intervals, hypothesis tests and linear regression. We extended what we knew about linear regression into dealing with non-linear regression (using the nls function) and using multiple predictors to explain variation. In week 7, the non-linear models we used were largely presented without comment as to where they came from, although we did discuss their use to model particular curves. Last semester we planned to do mathematical modelling but Dann Mallet was away and therefore couldn’t deliver a lecture. My undergrad was in mathematical modelling and computational mathematics, so I leapt at the chance to do some mathematical modelling. I figured that the best place to start would be the non-linear models that we dealt with in week 7. I had to do a bit of digging but I found the differential equations for the asymptotic model, first order compartment model and biexponential model as well as a nice little example of how they’re used. While many students haven’t done derivatives recently I do feel like by the end of the workshops there was a bit more of an understanding and appreciation of the modelling and the mathematics behind it. I figured that seeing as we weren’t actually doing a differential equations class and could only assume Maths B it would be a bit much to discuss the calculation of the exact answers for anything beyond the exponential growth model. We talked about how these differential equations have exact solutions but that we can’t always find an exact solution so we look to numerical solutions. We worked through how you can take the approximation for the derivative and ignore the limiting case and rearrange the definition to arrive at a very simple numerical technique. Only one of the workshop rooms properly installed the deSolve library but we did have a good discussion with the groups about how the Lotka-Volterra system has no exact solution and that the solution you obtain lies on an orbit in the phase plane (but I didn’t say that was what it was called). One of my favourite moments in that workshop was when one of a group of students asked about what would happen if the logistic model overshot the carrying capacity steady state. This led into a discussion about discrete time difference equations and the stability of the equilibrium point. The book we used, Barnes and Fulford, has a nice section on the chaotic behaviour of the logistic growth difference model and how as you increase the growth rate you move from smooth evolution that doesn’t overshoot to decaying oscillations around the carrying capacity to non-decaying oscillation between two population levels, to four, eight and then an infinite number of levels (you don’t return to the same population in finite time). That seemed to really tickle them and prompted a discussion about how this sort of behaviour has been observed in real world populations (elephants in a wildlife reserve?). This week we focussed on linear algebra, compressing three weeks of lectures from last semester into one single lecture. This may have been a little rushed in the delivery, because there were so many topics to deal with, but we basically split it up so the first half was showing how to turn a mathematical problem into a matrix system and visualising the solution. The second half was then a walk through of Gauss-Jordan elimination in order to get a matrix in reduced row echelon form (RREF). There were some very salient questions about why we care about matrices being in RREF and what each of the criteria for RREF actually mean. I thought at the time that it was a bit of a distraction from the main thrust of the class but thinking back to it now I am very glad that the students asked these questions as it showed that they were attempting to engage with and understand the material rather than just dismissing it as unimportant. I like to try to point out how the various topics in SEB113 are related, so we had a talk about how making a mesh from a continuous domain to arrive at a discrete set of nodes turns our mathematical model into a system of equations that we can solve numerically (week 9). We spent a bit of time in the week 9 lecture and workshops discussing how Euler’s method uses an approximation of a time derivative to approximate the evolution of a system. In week 10 we looked at the equilibrium temperature of a domain with known boundary conditions, which I pointed out was all about discretising the second derivative from our model. In a similar vein, the solution of an overdetermined system with the normal equations was a callback to the regression we’d done, where an overdetermined system has a non-zero number of degrees of freedom. We use the normal equations to go from an overdetermined system to an exactly solvable one that minimises the sum of squares. I didn’t want to go too deep into the linear algebra of how that works but the workshop exercises featured fitting a non-linear regression with the nls function in R (week 7) and doing it “manually” by solving the normal equations in R after converting it to a linear function. I think the groups that did this one got a bit deeper understanding of how regression works, which I’m definitely happy about. I told a student that I don’t mind if they can’t reproduce a linear regression by coding it manually, so long as they walk away at the end of the semester being able to load some data into R, look at the potential relationships, model them and interpret the model output. These last few weeks have probably been some of the mathematically most challenging and I think that while not all students necessarily understand the maths that’s going on behind the R functions they at least have a bit of an appreciation for where it comes from and why its useful. I’m quite pleased by that, as it’s helping get the message across that mathematics is a useful tool for solving problems rather than just some abstract thing that “I’m never going to use once I finish high school”. The same goes for statistics. Rather than just being this painful thing that requires manual calculation of margins of error for your experiment (forget that for a joke, no one learns anything useful that way), statistics helps you uncover the patterns in your data, model them and make statements about how meaningful they are. If we can get students seeing regression as more than just “What’s the line of best fit and its R&lt;sup&gt;2&lt;/sup&gt;?” then we’ll have done a good job.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
