<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>R on Sam Clifford </title>
    <link>/./tags/r/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2015-12-21 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>R Markdown</title>
      <link>/./2015/12/21/r-markdown/</link>
      <pubDate>Mon, 21 Dec 2015 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2015/12/21/r-markdown/</guid>
      <description>&lt;p&gt;I’ve been spending a bit of time over the last few days making an R tutorial for the members of my air quality research group. Rather than being a very general introduction to the use of R, e.g. file input/output, loops, making objects, I’ve decided to show a very applied workflow that involves the actual data analysis and explaining ideas as we go along. Part of this philosophy is that I’m not going to write a statistics tutorial, opting instead to point readers to textbooks that deal with first year topics such as regression models and hypothesis tests. It’s been a very interesting experience, and it’s meant having to deal with challenges along the way such as PDF graphs that take up so much file space for how (un-)important they are to the overall guide and, thinking about how to structure the tutorial so that I can assume zero experience with R but some experience with self-directed learning. The current version can be seen &lt;a href=&#34;https://samcliffordinfo.files.wordpress.com/2015/12/tutorial-20151221.pdf&#34; title=&#34;tutorial-20151221&#34;&gt;here&lt;/a&gt;. One of the ideas that Sama Low Choy had for SEB113 when she was unit coordinator and lecturer and I was just a tutor, was to write a textbook for the unit because there wasn’t anything that really covered our approach. Since seeing computational stats classes in the USA being hosted as repositories on GitHub I think it might be possible to use R Markdown or GitBook to write an R Markdown project that could be compiled either as a textbook with exercises or as a set of slides.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Blogging about blogging</title>
      <link>/./2015/12/16/blogging-about-blogging/</link>
      <pubDate>Wed, 16 Dec 2015 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2015/12/16/blogging-about-blogging/</guid>
      <description>&lt;p&gt;I was inspired to make a website and start blogging about my work when I went to 8BNP in 2011 and met people like &lt;a href=&#34;http://canini.me/&#34;&gt;Kevin Canini&lt;/a&gt; and &lt;a href=&#34;http://www.tamarabroderick.com/&#34;&gt;Tamara Broderick&lt;/a&gt; who had websites to spruik themselves as researchers. I eventually got around to re-setting up my Wordpress account, buying a domain and setting up the whole DNS shebang. The last four years have seen some major changes in the web resources for research, with things like github taking the place of subversion and encouraging a more social and outward facing coding culture. You can blog using github now, and &lt;a href=&#34;http://www.njtierney.com/jekyll/2015/11/11/how-i-built-my-site/&#34;&gt;Nick Tierney&lt;/a&gt; (a PhD student at QUT) has made me think about whether it’s worth migrating from Wordpress to jekyll. Further exposure to R Markdown through Di Cook’s workshop at Bayes on the Beach has strengthened my belief in RStudio not just as a way to do research but to communicate it. This is even before we start considering all the things like shiny and embedded web stuff. It’ll take some work and I’m not sure I’ll have time over summer, but it’s a change that’s probably worth making.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Marrying differential equations and regression</title>
      <link>/./2015/01/12/marrying-differential-equations-and-regression/</link>
      <pubDate>Mon, 12 Jan 2015 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2015/01/12/marrying-differential-equations-and-regression/</guid>
      <description>&lt;p&gt;Professor Fabrizio Ruggeri (Milan) visited the Institute for Future Environments for a little while in late 2013. He has been appointed as Adjunct Professor to the Institute and gave a public talk with a brief overview of a few of his research interests. Stochastic modelling of physical systems is something I was exposed to in undergrad when a good friend of mine, Matt Begun (&lt;a href=&#34;http://www.sphcm.med.unsw.edu.au/research/infectious-diseases/phd-candidates&#34;&gt;who it turns out is doing a PhD&lt;/a&gt; under Professor Guy Marks, with whom ILAQH collaborates), suggested we do a joint Honours project where we each tackled the same problem but from different points of view, me as a mathematical modeller, him as a Bayesian statistician. It didn’t eventuate but it had stuck in my mind as an interesting topic. In SEB113 we go through some non-linear regression models and the mathematical models that give rise to them. Regression typically features a fixed equation and variable parameters and the mathematical modelling I’ve been exposed to features fixed parameters (elicited from lab experiments, previous studies, etc.) and numerical simulation of a differential equation to solve the system (as analytic methods aren’t always easy to employ). I found myself thinking “I wonder if there’s a way of doing both at once” and then shelved the thought because there was no way I would have the time to go and thoroughly research it. Having spent a bit of time thinking about it, I’ve had a crack at solving an ODE within a Bayesian regression model (Euler’s method in JAGS) for logistic growth and the Lotka-Volterra equations. I’ve started having some discussions with other mathematicians about how we marry these two ideas and it looks like I’ll be able to start redeveloping my mathematical modelling knowledge. This is somewhere I think applied statistics has a huge role to play in applied mathematical modelling. Mathematicians shouldn’t be constraining themselves to iterating over a grid of point estimates of parameters, then choosing the one which minimises some L&lt;sup&gt;&lt;em&gt;p&lt;/em&gt;&lt;/sup&gt;-norm (at least not without something like Approximate Bayesian Computation). I mean, why explore regions of the parameter space that are unlikely to yield simulations that match up with the data? If you’re going to simulate a bunch of simulations, it should be done with the aim of not just finding the most probable values but characterising uncertainty in the parameters. A grid of values representing a very structured form of non-random prior won’t give you that. Finding the maximum with some sort of gradient-based method will give you the most probable values but, again, doesn’t characterise uncertainty. Sometimes we don’t care about that uncertainty, but when we do we’re far better off using statistics and using it properly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The problem with p values</title>
      <link>/./2014/09/18/the-problem-with-p-values/</link>
      <pubDate>Thu, 18 Sep 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/09/18/the-problem-with-p-values/</guid>
      <description>&lt;p&gt;A coworker sent me &lt;a href=&#34;http://theconversation.com/the-problem-with-p-values-how-significant-are-they-really-20029?utm_medium=email&amp;amp;utm_campaign=Latest+from+The+Conversation+for+12+November+2013&amp;amp;utm_content=Latest+from+The+Conversation+for+12+November+2013+CID_036de2eab3f9b92457ea3d5b919247bc&amp;amp;utm_source=campaign_monitor&amp;amp;utm_term=The%20problem%20with%20p%20values%20how%20significant%20are%20they%20really&#34;&gt;this article&lt;/a&gt; about alternatives to the default 0.05 p value in hypothesis testing as a way to improve the corpus of published articles so that we can actually expect reproducability and have a bit more faith that these results are meaningful. The article is based on a paper published in the &lt;a href=&#34;http://www.pnas.org/content/early/2013/10/28/1313476110.abstract&#34;&gt;Proceedings of the National Academy of Sciences&lt;/a&gt; which talks about mapping Bayes Factors to p values for hypothesis tests so that there’s a way to think about the strength of the evidence. The more I do and teach statistics the more I detest frequentist hypothesis testing (including whether a regression coefficient is zero) as a means of describing whether or not something plays a “significant” role in explaining some physical phenomenon. In fact, the entire idea of statistical significance sits ill with me because the way we tend to view it is that 0.051 is not significant and 0.049 is significant, even though there’s only a very small difference between the two. I guess if you’re dealing with cutoffs you’ve got to put the cutoff somewhere, but turning something which by its very nature deals with uncertainty into a set of rigid rules about what’s significant and what’s not seems pretty stupid. &lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;My distaste for frequentist methods means that even for simple linear regressions I’ll fire up JAGS in R and fit a Bayesian model because I fundamentally disagree with the idea of an unknown but fixed true parameter. Further to this, the nuances of p values being distributed uniformly under the Null hypothesis means that we can very quickly make incorrect statements.&lt;/span&gt; I agree with the author of the article that shifting hypothesis testing p value goal posts won’t achieve what we want and I’ll have a bit closer a read of the paper. For the time being, I’ll continue to just mull this over and grumble when people say “statistically significant” without any reference to a significance level. NB: this post has been in an unfinished state since last November, when the paper started getting media coverage.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>That feeling when former students contact you</title>
      <link>/./2014/09/02/that-feeling-when-former-students-contact-you/</link>
      <pubDate>Tue, 02 Sep 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/09/02/that-feeling-when-former-students-contact-you/</guid>
      <description>&lt;p&gt;Last year I had a student in SEB113 who came in to the subject with a distaste for mathematics and statistics; they struggled with both the statistical concepts and the use of R throughout the semester and looked as though they would rather be anywhere else during the collaborative workshops. This student made it to every lecture and workshop though and came to enjoy the work of using R for statistical analysis of data; and earned a 7 in the unit.&lt;/p&gt;
&lt;p&gt;I just got an email from them asking for a reference for their VRES (Vacation Research Experience Scheme) project application. Not only am I proud of this student for working their butt off to get a 7 in a subject they disliked but came to find interesting, but I am over the moon to hear that they are interested in undertaking scientific field research. This student mentions how my “passion for teaching completely transformed my (their) view of statistics”, and their passion for the research topic is reflected in the email.&lt;/p&gt;
&lt;p&gt;This sort of stuff is probably the most rewarding aspect of lecturing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lotka-Volterra and Bayesian statistics and teaching</title>
      <link>/./2014/08/27/lotka-volterra-and-bayesian-statistics-and-teaching/</link>
      <pubDate>Wed, 27 Aug 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/08/27/lotka-volterra-and-bayesian-statistics-and-teaching/</guid>
      <description>&lt;p&gt;One of the standard population dynamics models that I learned in my undergrad mathematical modelling units was the Lotka-Volterra equations. These represent a very simple set of assumptions about populations, and while they don’t necessarily give physically realistic population trajectories they’re an interesting introduction to the idea that differential equations systems don’t necessarily have an explicit solution. The assumptions are essentially: prey grow exponentially in the absence of predators, predation happens at a rate proportional to the product of the predator and prey populations, birth of predators is dependent on the product of predator and prey populations, predators die off exponentially in the absence of prey. In SEB113 we cover non-linear regressions, the mathematical models that lead to them, and then show that mathematical models don’t always yield a nice function. We look at equilibrium solutions and then show that we orbit around it rather than tending towards (or away from) it. We also look at what happens to the trajectories as we change the relative size of the rate parameters. Last time we did the topic, I posted about using the logistic growth model for our Problem Solving Task and it was pointed out to me that the model has a closed form solution, so we don’t explicitly need to use a numerical solution method. This time around I’ve been playing with using Euler’s method inside JAGS to fit the Lotka-Volterra system to some simulated data from sinusoidal functions (with the same period). I’ve put a bit more effort into the predictive side of the model, though. After obtaining posterior distributions for the parameters (and initial values) I generate simulations with lsode in R, where the parameter values are sampled from the posteriors. The figure below shows the median and 95% CI for the posterior predictive populations as well as points showing the simulated data. &lt;a href=&#34;https://samcliffordinfo.files.wordpress.com/2014/08/lv2.png&#34;&gt;&lt;img src=&#34;lv2.png?w=300&#34; alt=&#34;lv&#34; /&gt;&lt;/a&gt;The predictions get more variable as time goes on, as the uncertainty in the parameter values changes the period of the cycles that the Lotka-Volterra system exhibits. This reminds me of a chat I was having with a statistics PhD student earlier this week about sensitivity of models to data. The student’s context is clustering of data using overfitted mixtures, but I ended up digressing and talking about Edward Lorenz’s discovery of chaos theory through a meteorological model that was very sensitive to small changes in parameter values. The variability in the parameter values in the posterior give rise to the same behaviour, as both Lorenz’s work and my little example in JAGS involve variation in input values for deterministic modelling. Mine was deliberate, though, so isn’t as exciting or groundbreaking a discovery as Lorenz but we both come to the same conclusion: forecasting is of limited use when your model is sensitive to small variations in parameters. As time goes on, my credible intervals will likely end up being centred on the equilibrium solution and the uncertainty in the period of the solution (due to changing coefficient ratios) will result in very wide credible intervals. It’s been a fun little experiment again, and I’m getting more and more interested in combining statistics and differential equations, as it’s a blend of pretty much all of my prior study. The next step would be to use something like MATLAB with a custom Gibbs/Metropolis-Hastings scheme to bring in more of the computational mathematics I took. It’d be interesting to see if there’s space for this sort of modelling in the Mathematical Sciences School’s teaching programs as it combines some topics that aren’t typically taught together. I’ve heard murmurings of further computational statistics classes but haven’t been involved with any planning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Running Bayesian models</title>
      <link>/./2014/08/05/running-bayesian-models/</link>
      <pubDate>Tue, 05 Aug 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/08/05/running-bayesian-models/</guid>
      <description>&lt;p&gt;I came across a post via r/Bayes about different ways to run Bayesian hierarchical linear models in R, a topic I talked about recently at a two day workshop on using R for epidemiology. &lt;a href=&#34;http://www.sumsar.net/blog/2013/06/three-ways-to-run-bayesian-models-in-r/&#34;&gt;Rasmus Bååth&lt;/a&gt;’s post details the use of JAGS with rjags, STAN with rstan and LaplacesDemon. JAGS (well, rjags) has been the staple for most of my hierarchical linear modelling needs over the last few years. It runs within R easily, is written in C++ (so is relatively fast), spits out something that the coda package can work with quite easily, and, above all, makes it very easy to specify models and priors. Using JAGS means never having to derive a Gibbs sampler or write out a Metropolis-Hastings algorithm that requires to you to really think about jumping rules. It’s Bayesian statistics for those who don’t have the time/inclination to do it “properly”. It has a few drawbacks, though, such as not being able to specify improper priors (but this could be seen as a feature rather than a bug) with distributions like dflat() and defining a Conditional Autoregressive prior requires specifying it as a multivariate Gaussian. That said, it’s far quicker than using OpenBUGS and JAGS installs fine on any platform. After reading the post’s section on STAN I decided that it was time to give it another go. Downloading the latest version of R and Rtools would surely give me a better experience than last time where it wouldn’t even detect the compiler properly. Putting everything in DOS-friendly file structures with short names meant that everything went off without a hitch and I was able to get the toy example running. Andrew Gelman, one of the developers of STAN, has a &lt;a href=&#34;http://andrewgelman.com/2014/01/21/everything-need-know-bayesian-statistics-learned-eight-schools/&#34;&gt;post on his blog&lt;/a&gt; by Phillip Price about the eight schools example, a really introduction to hierarchical linear modelling and meta-analysis. STAN is a bit more forgiving than JAGS when it comes to priors; any stochastic node that isn’t given a prior is given a flat prior by default. Whether or not &lt;a href=&#34;http://arxiv.org/abs/1403.4630&#34;&gt;Thiago Martins and Dan Simpson&lt;/a&gt; would be happy with that remains to be seen. STAN looks very promising, and it’s been included in the third edition of &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/&#34;&gt;Gelman’s BDA book&lt;/a&gt; (which I still need to buy). The other strategy I tried recently was coding up a Metropolis-Hastings sampler using &lt;a href=&#34;http://theoreticalecology.wordpress.com/2010/09/17/metropolis-hastings-mcmc-in-r/&#34;&gt;the guide from Florian Hartig at Theoretical Ecology&lt;/a&gt;. Choosing a jumping rule was difficult, as I had different parameters to deal with and a single jumping rule wouldn’t do. I tried &lt;a href=&#34;http://projecteuclid.org/euclid.bj/1130077595&#34;&gt;adaptive MCMC&lt;/a&gt; and ended up going down the rabbit hole of log-precisions, block-updates and ended up with very poor mixing and convergence. Finding a decent jumping rule is probably what prevents me from going back to using adaptive MCMC as I did for &lt;a href=&#34;http://eprints.qut.edu.au/72987/&#34;&gt;a book chapter on Bayesian splines&lt;/a&gt;. I eventually settled on writing out a full Gibbs scheme and coding it up in MATLAB. This is very fast (MATLAB’s better at loops than R is) and gives me good convergence. I’m not a fan of MATLAB’s plotting, though, so may end up importing the results into R so I’ve got ggplot2 handy. Big thanks to Zoé van Havre for her help with the Gibbs scheme. I’ve got a PhD student who’s going to be dealing with Bayesian modelling. He’s picking up R quite quickly and is doing his best with Bayesian statistics. It’s all in WinBUGS at the moment, though, which is going to limit the amount of progress we can make. I’d love to be able to code up a bunch of JAGS models and let them run on the supercomputer once we get our great big data set ready for a well-planned set of analyses. I’ve got less time to do the modelling myself these days and find myself wishing I had a clone to do the work. I guess that’s part of the training aspect of PhD supervision, making sure your student can do the implementation when you describe a piece of analysis that you propose. It’s still difficult for me working in a science group as the only statistician, as most of my statistics discussions are people asking for my help rather than us collaborating as equals. I enjoy working with others on interesting modelling problems, and it’d be good to work with other statisticians. While I’m now in the Mathematical Sciences School I don’t think I’ve capitalised yet on the connections I’ve got there in terms of directing my own research down the statistics route. With the UPTECH analysis being the major focus of my research at the moment, it’s tricky to allocate brain space to what I want to be doing next.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior Samples</title>
      <link>/./2014/07/30/posterior-samples/</link>
      <pubDate>Wed, 30 Jul 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/07/30/posterior-samples/</guid>
      <description>&lt;p&gt;Interested in collaborative use of R, MATLAB, etc. for analysis and visualisation within a webpage? &lt;a href=&#34;http://nbviewer.ipython.org/gist/msund/403910de45e282d658fa&#34;&gt;Combining plotly and iPython&lt;/a&gt; can help you with that. Cosmopolitan (yes, &lt;em&gt;that&lt;/em&gt; Cosmopolitan) has &lt;a href=&#34;http://www.cosmopolitan.com/career/news/a29534/get-that-life-emily-graslie-science/&#34;&gt;a great article&lt;/a&gt; interviewing Emily Graslie, Chief Curiosity Officer at the Field Museum in Chicago. She discusses being an artist and making the transition into science, science education and YouTube stardom. A few of the PhD students in my lab have asked if I could run an introduction to R session. I’d mentioned the CAR workshop that I’d be doing but the cost had put them off. Luckily, there are alternatives like &lt;a href=&#34;https://www.datacamp.com/courses/introduction-to-r&#34;&gt;Datacamp&lt;/a&gt;, &lt;a href=&#34;https://www.coursera.org/&#34;&gt;Coursera&lt;/a&gt; and &lt;a href=&#34;http://www.lynda.com/R-tutorials/R-Statistics-Essential-Training/142447-2.html&#34;&gt;Lynda&lt;/a&gt;. Coursera’s next round of “Data Science”, delivered by Johns Hopkins University, starts next Monday (Course 1 - &lt;a href=&#34;https://www.coursera.org/specialization/jhudatascience/1&#34;&gt;R Programming&lt;/a&gt;). So get in there and learn some R! I’m considering recommending some of these Coursera courses to my current SEB113 students who want to go a bit further with R, but the approach that they take in these online modules is quite different to what we do in SEB113 and I don’t want them to confuse themselves.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning different programming languages</title>
      <link>/./2014/07/02/learning-different-programming-languages/</link>
      <pubDate>Wed, 02 Jul 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/07/02/learning-different-programming-languages/</guid>
      <description>&lt;p&gt;One of the biggest changes I noticed moving from doing statistics in Minitab (in first year data analysis) to doing statistics in R (in third year statistical inference) was that R encourages you to write functions. Normally this is done by writing functions in R’s own language (that call other functions, also written in R, which eventually call functions written in C) but it’s also possible to make use of other languages to do the heavy lifting. This isn’t unique to R, of course; MATLAB encourages the use of MEX files to improve run-times when you need to call the same custom function over and over again. I’ve really only used high level languages to do my statistics, making use of other peoples’ optimised code that do the things that I want. I’ve seen the development of &lt;a href=&#34;http://eprints.qut.edu.au/43469/&#34;&gt;pyMCMC&lt;/a&gt; by reseachers at QUT and someone from my NPBayes reading group made quite heavy use of RCpp in his thesis. Python and C++ are probably the two languages that would be the most useful to learn given their ubiquity and reputation. I have been putting off learning these for years as I know that there’s a large time investment required to become proficient in programming and no external pressure to learn (unlike learning R as part of my PhD thesis work). There’s no doubt that writing optimised code is a desirable thing to do, and that knowing more than one programming language (and how to use them together) gives you a much richer toolbox for numerically solving problems. I’m now at a point, though, where it looks like I may need to bite the bullet and pick up C++. JAGS, which I use through rjags in R, is a stable, fast platform for MCMC-based inference. It’s written in C++ and notifies you every time you load it in R that it has loaded the basemod and bugs modules. There are additional modules available (check in -3.4.064) and &lt;a href=&#34;http://www.cidlab.com/prints/wabersich2013extending.pdf&#34;&gt;it’s possible to write your own&lt;/a&gt;, as long as you know C++. I’m at a point with the work I’ve been doing on &lt;a href=&#34;http://pubs.acs.org/doi/abs/10.1021/es403721w&#34;&gt;estimating personal dose of ultrafine particles&lt;/a&gt; that I’d like to make the modelling more Bayesian, which includes figuring out a way to include the deposition model in the MCMC scheme (as I’d like to put a prior on the shape parameter of the reconstructed size distribution). My options seem to be either writing a JAGS module that will allow me to call a C++ified version of the function or to abandon JAGS and write a Gibbs sampler (or Metropolis-Hastings, but Gibbs will likely be quicker given the simplicity of the model I’m interested in). Either solution will stretch me as a programmer and probably give me a better understanding of the problem. Eubank and Kupresanin’s “Statistical Computing in C++ and R” is staring at me from the shelf above my desk.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2014/07/02/posterior-samples/</link>
      <pubDate>Wed, 02 Jul 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/07/02/posterior-samples/</guid>
      <description>&lt;p&gt;ARC Discovery Projects have been returned to their authors, and we are putting our responses together for the rejoinders. Interesting to see that we got a comment suggesting that we use the less restrictive CC-by instead of CC-by-nc-sa as we’d suggested. We weren’t successful in our Linkage Project applications, which is disappointing as they were interesting projects (well, we thought so). Continuing to bring research funding in is an ongoing struggle for all research groups and I feel it’s only going to get harder as the new federal government’s research priorities appear to be more aligned to medical science that delivers treatments than to our group’s traditional strengths. SEB113 is pretty much completely over for the semester, with marks having been entered for almost every student. Overall I think the students did fairly well. We had some issues with the timetable this semester. Ideally, we’d like the Lecture, then all of the computer labs, then all of the workshops, so that we can introduce a statistical idea, show the code and then apply the idea and code in a group setting. Next semester, we have the lecture followed immediately by the workshops with the computer labs dotted throughout the remainder of the week. This has provided us with an opportunity to try some semi-flipped classroom ideas, where students are able/expected to do the computer lab at home at their own pace rather than watch a tutor explain it one line at a time at the front of a computer lab. I’m teaching part of a &lt;a href=&#34;http://www.eventbrite.com.au/e/r-statistical-language-for-air-pollution-epidemiology-tickets-12043581677&#34;&gt;two day course&lt;/a&gt; on the use of R in air pollution epidemiology. My part will introduce Bayesian statistics with a brief overview, a discussion about prior distributions as a means of encoding &lt;em&gt;a priori&lt;/em&gt; beliefs about model parameters, and discuss the use of Bayesian hierarchical modelling (as opposed to more traditional ANOVA techniques) as a way of making the most of the data that’s been collected. The other two presenters are &lt;a href=&#34;http://researchers.uq.edu.au/researcher/2181&#34;&gt;Dr Peter Baker&lt;/a&gt; and &lt;a href=&#34;http://researchers.uq.edu.au/researcher/2530&#34;&gt;Dr Yuming Guo&lt;/a&gt;. The course is being run by the CAR-CRE, who partially fund my postdoctoral fellowship. I had meant to post this back when they were doing the rounds, but there’s &lt;a href=&#34;http://www.tylervigen.com/&#34;&gt;a bunch of plots&lt;/a&gt; that attempt to show that correlation isn’t causation and that spurious correlations exist in large data sets. &lt;a href=&#34;https://tom-christie.github.io/articles/correlation/&#34;&gt;Tom Christie has responded&lt;/a&gt; to this by going over the fact that correlation in time series isn’t as simple as in the case of independent, identically distributed data. One should be careful that one’s criticism of bad statistics is itself founded on good statistics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The ongoing crusade against Excel-based analysis</title>
      <link>/./2014/05/14/the-ongoing-crusade-against-excel-based-analysis/</link>
      <pubDate>Wed, 14 May 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/05/14/the-ongoing-crusade-against-excel-based-analysis/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;One of the things I catch myself saying quite often in SEB113 is “This is new. It’s hard. But remember, you weren’t born knowing how to walk. You learned it”, as my way of saying that it’s okay to not understand this straight away, it takes time, practice and determination. I often say this in response to students complaining about learning R to do their data analysis. It’s actually got to the point where t&lt;/span&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;he unit co-ordinator suggested I get a t-shirt printed with “You weren’t born knowing how to walk” on the front and “So learn R” on the back.&lt;/span&gt; One of the reasons I’m so keen to push new students into learning R is that while Excel can do some of the simpler calculations required in the first year of a science degree it is often completely inadequate for doing data analysis as a professional scientist, or even in an advanced level university course. I actually saw a senior researcher in a 3 day Bayesian statistics course try to avoid using R to code a Gibbs sampler by getting it up and running in Excel. They managed it, but it took minutes to run what the rest of us could compute in a second (and it was for a trivially simple problem). There are &lt;a href=&#34;http://www.asq904.org/StatisticalFlawsInExcel.pdf&#34;&gt;problems with Excel&lt;/a&gt;, such as its inability to deal with the standard deviation of a group of very large numbers due to its bizarre formulation. Apparently the secret to sane use of Excel is to &lt;a href=&#34;http://www.r-bloggers.com/excel-fanaticism-and-r/&#34;&gt;only use it for data storage&lt;/a&gt;. This guiding principle has meant that I no longer manipulate my data in Excel. Even with time stamp information I’ll fire up the &lt;a href=&#34;http://cran.r-project.org/web/packages/lubridate/index.html&#34;&gt;lubridate&lt;/a&gt; package to convert from one format to another. I’m slowly exploring the &lt;a href=&#34;http://blog.datascienceretreat.com/&#34;&gt;Hadleyverse&lt;/a&gt; and that sort of approach is filtering through into SEB113 where we’re teaching the use of ggplot2 and reshape2 within RStudio. These are all powerful tools that simplify data analysis and avoid the hackish feel that much Excel-based analysis has, where pivot tables are a thing and graphs are made by clicking and dragging a selection tool down the data (which can lead to &lt;a href=&#34;http://en.wikipedia.org/wiki/Growth_in_a_Time_of_Debt&#34;&gt;some nasty errors&lt;/a&gt;). The fact that these powerful tools that make data analysis simple are free is another reason to choose R over Excel. I’m not on the “Open Source Software and provision of all code is mandatory” bandwagon as others seem to be when it comes to analysis being replicable. I agree it’s a worthwhile goal but it’s not a priority for me. That said, though, I definitely support encouraging the use of free software (in both senses) in education on the grounds of equity of access. I had a chat with some students in SEB113 yesterday about why we’re teaching everything in R given that the SEB114 staff use a combination of Excel, MATLAB (and maybe even other packages I don’t know about). If we were to teach analysis the way that the SEB114 lecturers do it themselves, we’d have to teach multiple packages to multiple disciplines. Even discounting the fact that everything we teach is implemented in R, that R is free (unlike Excel and MATLAB), cross-platform (Excel on Linux? Try OpenOffice/OfficeLibre) and extensible (MATLAB has toolboxes, Excel has add-ins, R has a nice package manager) was a big plus for students who said that being able to work on assignments at home was valuable and so paying for software would make study difficult. Convincing students to use R can be difficult, especially if they have no programming background, but ultimately they seem to accept that R is powerful, can do more than Excel and that writing reusable code makes future analysis easier. Convincing SEB114 academics that teaching their students to use R is a good idea is probably a harder sell, given that they’ve got years of experience with other tools. It’s still only semester 3 of the new Bachelor of Science course so we’ll have to see how this plays out over the years to come.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Combining differential equations and regression</title>
      <link>/./2014/04/22/combining-differential-equations-and-regression/</link>
      <pubDate>Tue, 22 Apr 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/04/22/combining-differential-equations-and-regression/</guid>
      <description>&lt;p&gt;Last week I gave my first lecture for the semester to the SEB113 students. While they tend to not have a particularly strong mathematics background I got some very positive feedback on how much they enjoyed learning about mathematical modelling. We revised differentiation, what derivatives are and then jumped into a bit about formulating differential equations from words that represent the assumptions that the model makes. The bulk of that week’s lecture is showing where the non-linear regression models we used in the previous week (first order compartment, asymptotic, biexponential) come from. To do this we have a chat about exponential growth and decay models as some of the easiest differential equation models to deal with. I show them how we solve the exponential model exactly and then make reference to the fact that I don’t expect them to solve these equations in this subject. We show the solutions to the DE systems and make it very clear that the non-linear regression models are the solutions to differential equations that represent different assumptions. We finish the lecture off with a section on how we can’t always get a “pen and paper” solution to differential equations and so sometimes we either simplify the system to one we can solve (alluding to perturbation methods) or give it to a numerical solver (alluding to computational mathematics). Because it’s how I learned about numerical solutions to DEs I showed the students the Lotka-Volterra model and discussed why we can’t solve X(t) and Y(t) and so have to use numerical methods. For different parameter values we get variations on the same behaviour: cyclic patterns, prey population growth followed by predator population growth followed by overconsumption of prey leading to fewer predators being born to replace the dying. Many students seemed to enjoy investigating this model in the workshops, as it’s quite different to everything we’ve learned so far. Solution is via the deSolve package in R but we introduce the students to Euler’s method and discuss numerical instability and the accumulation of numerical error. I finish off the lecture with a chat about how regression tends to make assumptions about the form of the mean relationship between variables so we can do parameter estimation and that differential equations give us a system we can solve to obtain that mean relationship. I state that while we &lt;em&gt;can&lt;/em&gt; solve the DE numerically while simultaneously estimating the parameter it is way outside the scope of the course. I had a bit of time this morning to spend on next week’s lecture material (linear algebra) so decided to have a go at numerical estimation for the logistic growth model and some data based on the Orange tree circumference data set in R with JAGS/rjags. It’s the first time I’ve had a go at combining regression and numerical solutions to DEs in the same code, so I’ve only used Euler’s method. That said, I was very happy with the solution and the code is provided below the cut. [code language=“r”] # euler.bugs model{ y[1] ~ dnorm(mu[1], tau.y) mu[1] &amp;lt;- y0 + dt * exp(lr) * y0 * (1 - y0/K) for (i in 2:n){ y[i] ~ dnorm(mu[i], tau.y) mu[i] &amp;lt;- y[i-1] + dt * exp(lr) * y[i-1] * (1 - y[i-1]/K) } for (i in 1:n){ y.p[i] ~ dnorm(mu[i], tau.y) } lr ~ dnorm(0, 1e-6) K ~ dnorm(0, 1e-6) y0 ~ dunif(0.001, 1000) tau.y ~ dgamma(0.001, 0.001) } [/code] Which can be called appropriately with [code language=“r”] library(rjags) library(ggplot2) my.orange &amp;lt;- data.frame(age=seq(100, 1900, by=200), circumference = c(32, 47, 73, 101, 134, 162, 182, 194, 205, 214)) dt &amp;lt;- 10 orange.dat &amp;lt;- data.frame(age=seq(0, 3000, by=dt),circumference=NA) orange.dat[match(table=orange.dat&lt;span class=&#34;math inline&#34;&gt;\(age, x=my.orange\)&lt;/span&gt;age),“circumference”] &amp;lt;- my.orange[, “circumference”] orange.m &amp;lt;- jags.model(file=“euler.bugs”, data=list(y=orange.dat&lt;span class=&#34;math inline&#34;&gt;\(circumference, n = nrow(orange.dat), dt=dt),inits=list(y0=50, K=500, lr=-5)) orange.b &amp;lt;- jags.samples(model=orange.m, n.iter=10000, variable.names=c(&amp;quot;y.p&amp;quot;,&amp;quot;K&amp;quot;,&amp;quot;lr&amp;quot;,&amp;quot;y0&amp;quot;)) orange.pred &amp;lt;- coda.samples(model=orange.m, n.iter=1000, variable.names=c(&amp;quot;y.p&amp;quot;)) orange.sum &amp;lt;- summary(orange.pred, q=c(0.025, 0.5, 0.975)) orange.gg &amp;lt;- data.frame(orange.sum\)&lt;/span&gt;quantiles) orange.gg&lt;span class=&#34;math inline&#34;&gt;\(age &amp;lt;- orange.dat\)&lt;/span&gt;age windows(height=3.5) ggplot(data=orange.gg, aes(x=age, y=X50.)) + geom_line() + geom_line(aes(y=X2.5.), lty=2) + geom_line(aes(y=X97.5.), lty=2) + geom_point(data= my.orange, aes(y=circumference), alpha=0.5) + theme_bw() + xlab(“Time (days)”) + ylab(“Tree circumference (mm)”) [/code] The resulting picture can be seen below. [caption id=“attachment_1226” align=“aligncenter” width=“625”][&lt;img src=&#34;predicting.png?w=625&#34; alt=&#34;Prediction of tree circumference from logistic growth differential equation&#34; /&gt;](&lt;a href=&#34;http://samcliffordinfo.files.wordpress.com/2014/04/predicting.png&#34; class=&#34;uri&#34;&gt;http://samcliffordinfo.files.wordpress.com/2014/04/predicting.png&lt;/a&gt;) Prediction of tree circumference from logistic growth differential equation[/caption]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A few R things</title>
      <link>/./2014/04/10/a-few-r-things/</link>
      <pubDate>Thu, 10 Apr 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/04/10/a-few-r-things/</guid>
      <description>&lt;p&gt;“&lt;a href=&#34;http://hackerretreat.com/r-good-parts/&#34;&gt;R: The Good Parts&lt;/a&gt;” is an attempt to showcase the best way to do things in R. I’m not yet at the stage of dealing with absolutely massive data sets but things will be heading that way for me if aerosol samplers continue to measure at higher frequencies. Left out of the article is a discussion of &lt;a href=&#34;http://blog.rstudio.org/2014/01/17/introducing-dplyr/&#34;&gt;dplyr&lt;/a&gt;; I’m still using functions from the apply family! Maybe I should also get used to using &lt;a href=&#34;http://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.pdf&#34;&gt;data.table&lt;/a&gt;. (Update: I’m now using data.table and its syntax to apply functions across grouping levels that I’ve set as keys. This is amazing). While we’ve been incorporating a few of the mathematical needs of SEB114 into SEB113 it looks like we may need to go a bit further with incorporating the R needs. I hadn’t really thought about plotting a specific function (other than a line y = ax + b) in the workshops but it looks like a few earth sciences students need to plot the function π x / (1+x)&lt;sup&gt;2&lt;/sup&gt;. So we’ll have to take stock over the next six months of what the experimental science lecturers want to put in their units and how we can help support that (also how we can get the science lecturers to help reinforce statistical modelling over statistical testing).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2014/03/05/posterior-samples/</link>
      <pubDate>Wed, 05 Mar 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/03/05/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.ntnu.edu/&#34;&gt;NTNU&lt;/a&gt; in Trondheim, Norway, has &lt;a href=&#34;https://sites.google.com/a/r-inla.org/www/events/openphd-grants&#34;&gt;five PhD fellowships&lt;/a&gt; open. Visualising homicide rates in Mexico &lt;a href=&#34;http://lcolladotor.github.io/2014/02/26/excited-by-willingness-to-help-get-things-done/&#34;&gt;using R and GitHub&lt;/a&gt; (via &lt;a href=&#34;http://www.statisticsblog.com/2014/03/the-week-in-stats-mar-3rd-edition/&#34;&gt;Probability and Statistics Blog&lt;/a&gt;). If you’re in Melbourne, Australia, you should consider popping along to &lt;a href=&#34;http://thelaborastory.com/&#34;&gt;Laborastory&lt;/a&gt; at the Cider House (Brunswick), where five scientists get up on the first Tuesday of the month to tell the stories of the heroes and history of their field. A friend of mine went along this week and enjoyed it immensely. SEB113 has started again. I’ve already done 5 workshops (I have three a week). Introducing a whole new cohort of students to R, RStudio and ggplot. We did some paper plane throwing last week and had a look at how simple usage of faceting, colouring and stacking histograms can reveal both overall variation and group-to-group variation. A few students are still bewildered by the idea of writing code to make pictures but they recognise that it’s just a case of needing practice.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2014/02/05/posterior-samples/</link>
      <pubDate>Wed, 05 Feb 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/02/05/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://andrewgelman.com/2014/01/21/everything-need-know-bayesian-statistics-learned-eight-schools/&#34;&gt;Everything I need to know about Bayesian Statistics, I learned in eight schools&lt;/a&gt;. At first I thought this meant not really understanding it until having worked in many places with different people but it’s actually a reference to a particular example of hierarchical modelling. &lt;a href=&#34;http://www.youtube.com/watch?v=qRSfxSRdL5Y&#34;&gt;Hadley Wickham talking about dplyr&lt;/a&gt;. Very fancy. &lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34;&gt;I’m finally attempting to install RStan&lt;/a&gt;. My computer is misbehaving. And it’s ARC Discovery Project writing time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior Samples</title>
      <link>/./2013/12/02/posterior-samples/</link>
      <pubDate>Mon, 02 Dec 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/12/02/posterior-samples/</guid>
      <description>&lt;p&gt;Thiago Martins has posted &lt;a href=&#34;http://tgmstat.wordpress.com/2013/11/28/computing-and-visualizing-pca-in-r/&#34;&gt;a neat little tutorial&lt;/a&gt; about using R to calculate and visualise Principle Components Analysis, using Fisher’s Iris data. PCA is something I’ve struggled with as I’ve gone further into statistics, as it comes across as being based on mathematics rather than statistics. I’d like to learn more about the Indian Buffet Process and associated non-parametric Bayesian methods but if I’m going to be looking at long and wide data sets (say, UPTECH questionnaire data) I’d like to have somewhere to start. It looks like this may provide that. Rasmus Bååth’s done &lt;a href=&#34;http://www.sumsar.net/blog/2013/11/easy-laplace-approximation/&#34;&gt;a tutorial on Laplace Approximations in R&lt;/a&gt; (hat tip to Matt Moores for this one). Laplace Approximations are an alternative to MCMC simulation that can provide good approximations to well-behaved posterior densities in a fraction of the time. The tutorial deals with the issue of reparameterisation for when you’ve got parameters which have bounded values (such as binomial proportions). As a piece of trivia, Thiago (above) is based at NTNU where &lt;a href=&#34;http://www.r-inla.org/&#34;&gt;R-INLA&lt;/a&gt; is developed. I’m at the &lt;a href=&#34;http://emac2013.com.au/&#34;&gt;emac2013&lt;/a&gt; conference this week. We’re about half way through day one of the talks (of three) and there’s already been some fascinating stuff. Professor Robert Mahony (ANU) gave a talk that shows that the development of more advanced unmanned aerial vehicles (UAVs, drones) involves some quite complex but elegant mathematics, involving Lie group symmetries, rather than just coming up with cooler robots. Hasitha Nayanajith Polwaththe Gallage (QUT) showed some really interesting particle method (mesh-free) modelling where forces and energies were used to determine the shape of a red blood cell that had just ejected its nucleus.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>More on the use of R and ggplot in SEB113</title>
      <link>/./2013/11/20/more-on-the-use-of-r-and-ggplot-in-seb113/</link>
      <pubDate>Wed, 20 Nov 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/11/20/more-on-the-use-of-r-and-ggplot-in-seb113/</guid>
      <description>&lt;p&gt;One of the benefits of waking up stupidly early is that you can organise to have coffee and a chat with a friend before work and still get there on time. A friend who I haven’t seen in a while contacted me recently to ask a few questions about learning R for data analysis and visualisation. While they won’t need to formally learn statistics and visualisation for their work it certainly doesn’t hurt to be able to generate better analysis of data and make more informative and easy to interpret graphs. My friend hasn’t done any statistics since high school Maths B, approximately ten years ago, which makes them similar to many of my SEB113 students. They have done a bit of programming along the way as a hobby, which will of course be a huge help. Having downloaded R and had a crack at a ggplot2 tutorial, they were confident that they &lt;em&gt;could&lt;/em&gt; learn what was going on even though they didn’t really understand what was going on in the tutorial. We sat down with the tutorial and some avocado on toast and worked through what the arguments for each function represented and what the data frame was made of, how ggplot has a grammar of graphics and how we can continue to add elements to the code to change the plot. To an extent, the ability to work through but not explain what some code is doing is typical of an SEB113 student in the first half of the subject (where we provide the code and get them to run it). It’s not until later in the semester, when the computer labs stop, that we expect that they can turn their ideas into code (and they’re welcome to cannibalise the code we provide) to write their quantitative workbooks. I suggested the &lt;a href=&#34;http://samclifford.info/2013/11/11/coursera-courses-on-statistics/&#34; title=&#34;Coursera courses on statistics&#34;&gt;Coursera course that started yesterday&lt;/a&gt; as a way to get a bit more familiar with how R works and get recognition of the completion of the course (which isn’t a recognised qualification but is evidence of being interested enough to pursue it). These days I’m always on the lookout for better ways to introduce SEB113 students to R and ggplot2 and I found the following tutorials (and have passed them on to my friend and the SEB113 teaching team) via Matt Asher’s “&lt;a href=&#34;http://www.statisticsblog.com/2013/11/the-week-in-stats-nov-18th-edition/&#34;&gt;Statistics Blog&lt;/a&gt;” and I have copied and pasted the text directly:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Martin Johnsson wrote a series of five well-written tutorials called &lt;em&gt;A slightly different introduction to R,&lt;/em&gt; with tips for beginner R users. Here are the links to parts &lt;a href=&#34;http://bit.ly/1fzSWRq&#34;&gt;I&lt;/a&gt;, &lt;a href=&#34;http://bit.ly/18pRHiz&#34;&gt;II&lt;/a&gt;, &lt;a href=&#34;http://bit.ly/19if90E&#34;&gt;III&lt;/a&gt;, &lt;a href=&#34;http://bit.ly/183o9eb&#34;&gt;IV&lt;/a&gt; and &lt;a href=&#34;http://bit.ly/1i7x0j0&#34;&gt;V&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I had no idea that the &lt;a href=&#34;http://cran.r-project.org/web/packages/coefplot/index.html&#34;&gt;coefplot&lt;/a&gt; package existed! That’s going to make visualisation of fitted linear models much easier for our students, as we’ve previously had them using geom_segment to manually plot estimates and confidence intervals. This is part of what I love about R, compared to, say, SAS. There’s a huge community of people working out there to add extra functionality to an open source project by building on each others’ work. GGally and coefplot both require ggplot2 and have got a lot of really nice functions that extend the publication quality graphics of ggplot2. The community is quite active and if you can think of a question for R there’s probably an answer out there already.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coursera courses on statistics</title>
      <link>/./2013/11/11/coursera-courses-on-statistics/</link>
      <pubDate>Mon, 11 Nov 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/11/11/coursera-courses-on-statistics/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;I was looking into Coursera courses for a colleague who wants their postgrad students to have a stronger background in statistics. These students are coming from a background of having done science but not necessarily taken any mathematics or statistics electives in a while. It’d be good for them to learn R for their data analysis, too, seeing as most other statistics packages cost money and Excel, while readily available within universities and other research laboratories, is utterly terrible for proper data analysis.&lt;/span&gt; I managed to find the following courses which look relevant to the students and perhaps might even be good for any SEB113 students seeking to consolidate their statistical knowledge. This may also be relevant for current researchers looking to refamiliarise themselves with statistics and learn a good software package while they’re at it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/course/biostats&#34;&gt;Mathematical Biostatistics Boot Camp 1&lt;/a&gt;. Starting November 18 2013 and running for seven weeks. This requires some knowledge of calculus but looks like a really good course. Uses R.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/course/compdata&#34;&gt;Computing for Data Analysis&lt;/a&gt;. Starting January 6 2014 and running for four weeks. Uses R.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/course/statreasoning&#34;&gt;Statistical Reasoning for Public Health: Estimation, Inference, &amp;amp; Interpretation&lt;/a&gt;. Starting January 21 2013 and running for eight weeks.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/course/statistics&#34;&gt;Data Analysis and Statistical Inference&lt;/a&gt;. Starting February 17 2013 and running for ten weeks. Uses R.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These courses are all free and some of them provide you with a statement of accomplishment upon successful completion, which could be a useful addition to your CV if you want to show prospective employers that you’re enthusiastic about learning and applying statistics. Edit: there’s also &lt;a href=&#34;https://novoed.com/numbers-life-quantway&#34;&gt;this&lt;/a&gt; “Numbers for Life” from Novo Ed. Johns Hopkins’ Open Courseware has the following neat courses:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://ocw.jhsph.edu/index.cfm/go/viewCourse/course/introbiostats/coursePage/index/&#34;&gt;Introduction to Biostatistics&lt;/a&gt;. This is a bit like if SEB113 focussed on tests rather than models.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ocw.jhsph.edu/index.cfm/go/viewCourse/course/BiostatisticsLectureSeries05/coursePage/index/&#34;&gt;Biostatistics Lecture Series&lt;/a&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;. A discussion not so much about statistics but the way statistics is practised.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;Methods in Biostatistics&lt;/span&gt; &lt;a href=&#34;http://ocw.jhsph.edu/index.cfm/go/viewCourse/course/MethodsInBiostatisticsI/coursePage/index/&#34;&gt;I&lt;/a&gt; &lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;and&lt;/span&gt; &lt;a href=&#34;http://ocw.jhsph.edu/index.cfm/go/viewCourse/course/methodsinbiostatisticsii/coursePage/index/&#34;&gt;II&lt;/a&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;Statistics for Laboratory Scientists&lt;/span&gt; &lt;a href=&#34;http://ocw.jhsph.edu/index.cfm/go/viewCourse/course/StatisticsLaboratoryScientistsI/coursePage/index/&#34;&gt;I&lt;/a&gt; &lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;and&lt;/span&gt; &lt;a href=&#34;http://ocw.jhsph.edu/index.cfm/go/viewCourse/course/statisticslaboratoryscientistsii/coursePage/index/&#34;&gt;II&lt;/a&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ocw.jhsph.edu/index.cfm/go/viewCourse/course/EssentialsProbabilityStatisticalInference/coursePage/index/&#34;&gt;Essentials of Probability and Statistical Inference IV&lt;/a&gt;. GAMs, CARTs, neural networks. Good for postgrad level statisticians.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ben Fitzpatrick also introduced me to &lt;a href=&#34;http://www.lynda.com/&#34;&gt;lynda&lt;/a&gt;, which has a few short courses on statistics and R.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/10/04/posterior-samples/</link>
      <pubDate>Fri, 04 Oct 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/10/04/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://dynamicecology.wordpress.com/2013/09/26/wiwacs-vs-zombie-ideas/&#34;&gt;Zombie ideas in ecology&lt;/a&gt; make it difficult to publish results that go against them, but how prevalent are they? &lt;a href=&#34;http://mathesaurus.sourceforge.net/octave-r.html&#34;&gt;R for MATLAB users&lt;/a&gt;. This would have been handy in SEB113 if we were still using MATLAB. Perhaps we should give it as a general resource for stats classes in the mathematics program at QUT. The US government shutdown is having &lt;a href=&#34;http://edition.cnn.com/2013/10/03/opinion/urry-nasa-shutdown/index.html?hpt=hp_c2&#34;&gt;quite severe effects on science&lt;/a&gt;, such as threatening the launch of the next &lt;a href=&#34;http://edition.cnn.com/2013/10/03/us/shutdown-mars-mission/index.html&#34;&gt;Mission to Mars&lt;/a&gt;. This is pretty awful given it was &lt;a href=&#34;http://www.news.com.au/technology/sci-tech/happy-birthday-nasa-shut-it-down/story-fn5fsgyc-1226731242175&#34;&gt;NASA’s 50th birthday&lt;/a&gt; earlier this week.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Non-statistics topics in SEB113</title>
      <link>/./2013/09/27/non-statistics-topics-in-seb113/</link>
      <pubDate>Fri, 27 Sep 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/09/27/non-statistics-topics-in-seb113/</guid>
      <description>&lt;p&gt;While SEB113 is all about Quantitative Methods in Science it’s been quite notable that the last two weeks have been mathematical rather than statistical. We started the semester with visualisation and then headed into summary statistics, confidence intervals, hypothesis tests and linear regression. We extended what we knew about linear regression into dealing with non-linear regression (using the nls function) and using multiple predictors to explain variation. In week 7, the non-linear models we used were largely presented without comment as to where they came from, although we did discuss their use to model particular curves. Last semester we planned to do mathematical modelling but Dann Mallet was away and therefore couldn’t deliver a lecture. My undergrad was in mathematical modelling and computational mathematics, so I leapt at the chance to do some mathematical modelling. I figured that the best place to start would be the non-linear models that we dealt with in week 7. I had to do a bit of digging but I found the differential equations for the asymptotic model, first order compartment model and biexponential model as well as a nice little example of how they’re used. While many students haven’t done derivatives recently I do feel like by the end of the workshops there was a bit more of an understanding and appreciation of the modelling and the mathematics behind it. I figured that seeing as we weren’t actually doing a differential equations class and could only assume Maths B it would be a bit much to discuss the calculation of the exact answers for anything beyond the exponential growth model. We talked about how these differential equations have exact solutions but that we can’t always find an exact solution so we look to numerical solutions. We worked through how you can take the approximation for the derivative and ignore the limiting case and rearrange the definition to arrive at a very simple numerical technique. Only one of the workshop rooms properly installed the deSolve library but we did have a good discussion with the groups about how the Lotka-Volterra system has no exact solution and that the solution you obtain lies on an orbit in the phase plane (but I didn’t say that was what it was called). One of my favourite moments in that workshop was when one of a group of students asked about what would happen if the logistic model overshot the carrying capacity steady state. This led into a discussion about discrete time difference equations and the stability of the equilibrium point. The book we used, Barnes and Fulford, has a nice section on the chaotic behaviour of the logistic growth difference model and how as you increase the growth rate you move from smooth evolution that doesn’t overshoot to decaying oscillations around the carrying capacity to non-decaying oscillation between two population levels, to four, eight and then an infinite number of levels (you don’t return to the same population in finite time). That seemed to really tickle them and prompted a discussion about how this sort of behaviour has been observed in real world populations (elephants in a wildlife reserve?). This week we focussed on linear algebra, compressing three weeks of lectures from last semester into one single lecture. This may have been a little rushed in the delivery, because there were so many topics to deal with, but we basically split it up so the first half was showing how to turn a mathematical problem into a matrix system and visualising the solution. The second half was then a walk through of Gauss-Jordan elimination in order to get a matrix in reduced row echelon form (RREF). There were some very salient questions about why we care about matrices being in RREF and what each of the criteria for RREF actually mean. I thought at the time that it was a bit of a distraction from the main thrust of the class but thinking back to it now I am very glad that the students asked these questions as it showed that they were attempting to engage with and understand the material rather than just dismissing it as unimportant. I like to try to point out how the various topics in SEB113 are related, so we had a talk about how making a mesh from a continuous domain to arrive at a discrete set of nodes turns our mathematical model into a system of equations that we can solve numerically (week 9). We spent a bit of time in the week 9 lecture and workshops discussing how Euler’s method uses an approximation of a time derivative to approximate the evolution of a system. In week 10 we looked at the equilibrium temperature of a domain with known boundary conditions, which I pointed out was all about discretising the second derivative from our model. In a similar vein, the solution of an overdetermined system with the normal equations was a callback to the regression we’d done, where an overdetermined system has a non-zero number of degrees of freedom. We use the normal equations to go from an overdetermined system to an exactly solvable one that minimises the sum of squares. I didn’t want to go too deep into the linear algebra of how that works but the workshop exercises featured fitting a non-linear regression with the nls function in R (week 7) and doing it “manually” by solving the normal equations in R after converting it to a linear function. I think the groups that did this one got a bit deeper understanding of how regression works, which I’m definitely happy about. I told a student that I don’t mind if they can’t reproduce a linear regression by coding it manually, so long as they walk away at the end of the semester being able to load some data into R, look at the potential relationships, model them and interpret the model output. These last few weeks have probably been some of the mathematically most challenging and I think that while not all students necessarily understand the maths that’s going on behind the R functions they at least have a bit of an appreciation for where it comes from and why its useful. I’m quite pleased by that, as it’s helping get the message across that mathematics is a useful tool for solving problems rather than just some abstract thing that “I’m never going to use once I finish high school”. The same goes for statistics. Rather than just being this painful thing that requires manual calculation of margins of error for your experiment (forget that for a joke, no one learns anything useful that way), statistics helps you uncover the patterns in your data, model them and make statements about how meaningful they are. If we can get students seeing regression as more than just “What’s the line of best fit and its R&lt;sup&gt;2&lt;/sup&gt;?” then we’ll have done a good job.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior Samples</title>
      <link>/./2013/09/11/posterior-samples/</link>
      <pubDate>Wed, 11 Sep 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/09/11/posterior-samples/</guid>
      <description>&lt;p&gt;Roger Peng and Jeff Leek are going to be &lt;a href=&#34;http://simplystatistics.org/2013/08/29/the-return-of-the-stat-computing-for-data-analysis-data-analysis-back-on-coursera/&#34;&gt;re-offering their Coursera courses&lt;/a&gt; on data analysis, including the use of R. A Creative Commons-licensed textbook on Bayesian statistics that teaches in Python rather than mathematics? Sure. &lt;a href=&#34;http://www.greenteapress.com/thinkbayes/thinkbayes.pdf&#34;&gt;Have this one&lt;/a&gt;. &lt;a href=&#34;http://www.youtube.com/watch?v=fCuZ5UUrKrM&#34;&gt;Let’s discuss gender bias in STEM fields&lt;/a&gt;. Australia has a new government and &lt;a href=&#34;http://www.abc.net.au/science/articles/2013/09/10/3845442.htm&#34;&gt;that new government has flagged&lt;/a&gt; that it will be keeping a closer eye on the Australian Research Council’s grants process in order to “crack down on Labor’s (the former government) addition to waste by auditing increasingly ridiculous research grants”. The Liberal-National Party coalition government has a history of doing this; former Minister for Education, Brendan Nelson, &lt;a href=&#34;http://www.themonthly.com.au/issue/2006/may/1294984634/gideon-haigh/nelson-touch&#34;&gt;personally vetoed grants for humanities projects&lt;/a&gt; that had been approved through the ARC’s independent peer review process.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using ggplot2 in SEB113</title>
      <link>/./2013/09/10/using-ggplot2-in-seb113/</link>
      <pubDate>Tue, 10 Sep 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/09/10/using-ggplot2-in-seb113/</guid>
      <description>&lt;p&gt;One of the big pieces of feedback we got during last semester’s SEB113 class was that the programming was difficult to understand and reproduce. While the subject is not a programming subject, we do use R quite heavily for all of the data analysis. Maths B isn’t a pre-requisite for SEB113 and I’d wager that even fewer of the students entering the ST01 Bachelor of Science program have taken senior IPT/ICT subjects at their high schools than have taken Maths B. This semester we introduced R in the very first lecture and gave it a bit of a context. This means that students are aware from the get-go that they will be learning statistics through data analysis on a computer. The lectorials introduce the concepts and provide code for the resulting plots and analysis, the computer labs show how to do that particular form of analysis in R and then the collaborative workshops reinforce the labs by getting groups to work through the analysis of some problem using the statistical concepts and code that they’ve learned that week. One of the biggest stumbling blocks last semester was the inconsistency in the way visualisation was done in R. We used a combination of base graphics, trellis graphics in the lattice package, heatmaps and dendrograms from other packages and had to turn to yet another package to get colorbars for the heatmaps. Part of the fine-tuning this semester has been employing someone (who also does the labs) to rewrite the graphics in the labs in terms of Hadley Wickham’s &lt;a href=&#34;http://ggplot2.org/&#34;&gt;ggplot2&lt;/a&gt; library. This brings consistency to the graphical aspect of the unit and the plot geometries are named explicitly so that it’s clear what style of plot you’ll be generating. I was quite sceptical of ggplot2 when I first saw it, as the only exposure I had to it was the default options for a scatterplot with points. Sure, that’s pretty boring, but the fact that you can make a faceted grid (or wrap it using facet_wrap instead of facet_grid) means that investigating the use of small multiples is so much easier. Small multiples is a visualisation technique developed by &lt;a href=&#34;http://www.edwardtufte.com/tufte/&#34;&gt;Edward Tufte&lt;/a&gt; to allow the reader to see how the relationship between two variables changes as you also vary one or two other (categorical) covariates. Doing this in lattice required specifying a formula, similar to the way you specify a model in lm, but lattice is so different from the base graphics that you lose consistency. I’m touching up this week’s workshop at the moment and I’m really noticing where the graphics code has been greatly simplified by access to a grammar of graphics for a powerful set of plotting routines. The &lt;a href=&#34;http://cran.r-project.org/web/packages/GGally/index.html&#34;&gt;GGally&lt;/a&gt; pacakge provides things like ggpairs, which does what pairs does in the base graphics but gives you the correlation above the diagonal and the scatterplots below the diagonal. This makes for more informative graphs with the beauty of the ggplot style. As far as I can tell we’re hearing fewer complaints about the programming and the visualisation is happening much quicker in the workshops this semester as ggplot2’s documentation is amazing and it’s often a choice of geometry (and changing one or two options) rather than a choice of library (and changing the entire approach to the coding). The use of ggplot2 has made teaching visualisation much simpler and we’re now getting through the workshops quite quickly because the visualisation is no longer a huge stumbling block.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Abnormal normals</title>
      <link>/./2013/09/05/abnormal-normals/</link>
      <pubDate>Thu, 05 Sep 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/09/05/abnormal-normals/</guid>
      <description>&lt;p&gt;I’m doing thesis revisions at the moment and one of the examiners has asked that I refit some of the models in one of the chapters. The chapter corresponds to a book chapter that I wrote with one of my supervisors where we looked at Bayesian spline regression. This was pretty early in my PhD and represented a stepping stone from the frequentist approach that Simon Wood uses in his mgcv pacakge (used for my first paper) and the Bayesian approach to Generalised Additive Models with autoregressive errors (based on Siddartha Chib’s 1993 paper). One thing that I’ve discovered that really bugs me in MATLAB is that normpdf (density function of a normal distribution) is specified in terms of a mean and standard deviation whereas mvnpdf (density function of a multivariate normal) is specified in terms of a mean vector and covariance matrix. The result is that normpdf(0,0,1.5) and mvnpdf(0,0,1.5) give different results! In my mind, the univariate normal is a special case of the multivariate normal and the usage should be consistent.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>More on regression</title>
      <link>/./2013/08/29/more-on-regression/</link>
      <pubDate>Thu, 29 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/29/more-on-regression/</guid>
      <description>&lt;p&gt;This week we moved on to regression with a categorical covariate, which we’ve used in the context of estimating the mean dissolved oxygen across three spatial locations or the mean grain size of different types of rock. I think one of the biggest stumbling blocks to our learning with R is the way that lm() and associated functions deal with factor terms. Personally, I am not a fan of the way it chooses the first alphabetic level as the baseline. I know you can relevel the factor levels to choose another one but I’d be much happier with a sum to zero constraint so that the effects are all departures from the mean. This is the way that R-INLA does it and, to me, it makes a lot more sense. Factor terms (categorical variables) are essentially a random effects mean, especially in a Bayesian setting where everything can be treated as a random effect. That lm() makes us choose a baseline and treats the other effects as differences to that baseline means we end up with a coefficients table which is more difficult to interpret. One option is to omit the intercept term from the model, with a function call like lm(data=my.data, y ∼ factor(x) - 1) but that still doesn’t give you a sense of the overall mean. In any case, the mixture of new regression techniques and hypothesis testing for whether or not some parameter is equal to zero is proving difficult. The difference between the t and standard Normal distributions seems to not be particularly well understood and while I’ve tried to make the link between a 95% confidence interval and hypothesis testing at a 5% level of significance quite explicit, the fact remains that these are both new concepts which are being taught by a relatively inexperienced lecturer to students whose mathematical literacy is generally not at the level of those I’ve tutored in subjects where Maths B was explicitly a pre-requisite rather than assumed knowledge. I managed to explain the heavy tails issue in class with a little bit of pantomime, showing how one might paint the tails of the t and Normal distributions and run out of paint at different values of t (or x) based on how much paint you had to use at values far away from the mean. I think about a quarter of the class was struggling with the diagram of overlapping triangles which was meant to be a “zoomed in” version of the point where the density functions of the Normal and t cross over as they get further away from the mean. The lectorials are recorded so I’ll be interested to see how it translates to a radio play setting. The computer labs are apparently quite dense at the moment, with a lot of fairly new ideas being reinforced in a 50 minute block. We’re quite fortunate to have all the labs before all the workshops this semester, so the lab is basically “here’s the code to do what was shown in the lectorials” and then the workshops are designed to implement the code for some problem and generate a bit of discussion. I think this week was probably one of the hardest, conceptually, because it brings together regression for categorical explanatory variables (a straight line is easier to understand than mutually exclusive sets of points), hypothesis testing, the t distribution and confidence intervals. I have uploaded some of last semester’s slides on the central limit theorem for those who may need them, but I think it’s more a familiarity and practice thing than the material being inherently inaccessible. Next week we’ll be moving on to different families for Generalised Linear Models and the use of the nls() function to fit non-linear models such as asymptotic, compartment and bi-exponential. I’m not such a fan of nls (or even nlme) but we can hardly teach them how to use something like WinBUGS to define their own custom mean functions (because if you struggle with the t distribution you’re going to have kittens trying to deal with using the Beta distribution as a conjugate prior for the Binomial model) or even throw them into using gam() from mgcv. I wouldn’t be averse to teaching Generalised Additive Models in an advanced follow-up unit for this subject. If we did that, we could remove the GLMs from SEB113 (which we’ve only introduced this semester) and spend some time on random effects models. I think such a subject would require a much stronger background in mathematics, so students may need to take MAB120/125 and MAB121/126 before attempting such a unit. Still, food for thought as QUT continues to develop the new Bachelor of Science course.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Test statistics and regression parameters</title>
      <link>/./2013/08/23/test-statistics-and-regression-parameters/</link>
      <pubDate>Fri, 23 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/23/test-statistics-and-regression-parameters/</guid>
      <description>&lt;p&gt;This week in SEB113 we’ve started on regression with some simple linear models with one explanatory variable. As not everyone has a particularly strong statistics background (high school Maths B) there are definitely some challenges.&lt;/p&gt;
&lt;p&gt;The big one seems to be moving from the Normal distribution, which everyone seems to get, for estimating the confidence interval of the mean, towards the t distribution for calculating confidence intervals for regression parameters. Putting the t distribution in the context of estimating quality between batches of Guiness helps a little with the question “Where did this even come from?” but doesn’t address the mathematics of it. Plotting a few different t distributions with varying degrees of freedom helps make the point that the t approaches the Normal when the degrees of freedom goes to infinity but does nothing to explain what the degrees of freedom actually are.&lt;/p&gt;
&lt;p&gt;I’ve found that explaining the data as a resource for fitting the regression model can be handy. For a data set with &lt;em&gt;n&lt;/em&gt; points you have a maximum of &lt;em&gt;n&lt;/em&gt; degrees of freedom. Each time you add a parameter to your regression model you consume a degree of freedom because you’re imposing a constraint, such as “there is a straight line”. If we had one data point in our data set and wanted to know the mean of the data we would know it exactly, there would be no uncertainty left in our estimate (and therefore zero degrees of freedom). If we had two data points and wanted the mean there would be some amount of uncertainty left because there’s now some variation in our data (we would have one degree of freedom left). If we had two points and wanted a line of best fit we would be back to zero degrees of freedom because we have completely characterised the trend in the data set by joining the two points.&lt;/p&gt;
&lt;p&gt;If we fit a regression with a total of &lt;em&gt;k&lt;/em&gt; parameters on the right hand side of &lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt; = &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;0&lt;/sub&gt; + &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt; &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;1_i_&lt;/sub&gt; + &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt; &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;2_i_&lt;/sub&gt; + … + &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;k-1&lt;/sub&gt; &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;-1_i _&lt;/sub&gt; + ϵ&lt;sub&gt;i&lt;/sub&gt; (a mean and some effects of explanatory variables) we would have &lt;em&gt;n&lt;/em&gt;-&lt;em&gt;k&lt;/em&gt; degrees of freedom. As you have fewer degrees of freedom left your t distribution ends up with more mass further from the mean. This means that you’re more uncertain about the value of the parameter because you’re using your data to estimate other parameters.&lt;/p&gt;
&lt;p&gt;Coming from a Bayesian perspective and looking at this sort of mathematics it’s easy to conclude that the model states that the parameters have a t distribution. This is, of course, completely incorrect because the parameters are fixed constants with unknown values. This idea totally throws me as I’ve been working with Bayesian analysis for the last few years (with the exception of perhaps one paper) and I’m used to thinking of all parameters as being random variables.&lt;/p&gt;
&lt;p&gt;This is related to the hypothesis testing and confidence interval issues that I have with the way first year statistics is taught. Confidence intervals, as I’ve mentioned previously, are counter-intuitive. They are the things that are random in our estimates of the true values of parameters. I like the approach that we’re taking where we look at whether the 95% confidence interval covers zero in order to make statements about whether or not the parameter plays a role in explaining the variation that we are modelling. I don’t like that we then calculate p values for testing the hypothesis that that parameter is equal to zero. These tests are statements about the probability of seeing the test statistic or more extreme given the model that we’re working with. It’s all backwards and leads inexperienced students to make statements such as “We accept the null hypothesis” and “The variable is statistically insignificant”, both of which are nails on a chalkboard to my ears.&lt;/p&gt;
&lt;p&gt;I know that we can’t teach statistics the way I would like to teach it, as these are science students who will be entering a field where ANOVA and t tests are still commonly used not as exploratory data analysis but as the basis for inference. I am very thankful for the fact that we are moving away from testing and towards modelling, and I’ve been trying to make the point in lectures that modelling allows us to do prediction, whereas testing allows us to only talk about what we’ve seen. If we can make sure the students can fit a model in R and use it to predict and/or make inferences I think we’ll have done our jobs because that is far more than I was able to do when I finished MAB101 ten years ago, when everything required we look through page after page of statistical table, hunting the right p value.&lt;/p&gt;
&lt;p&gt;Edit: 150 posts!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/08/22/posterior-samples/</link>
      <pubDate>Thu, 22 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/22/posterior-samples/</guid>
      <description>&lt;p&gt;Life can be tough when you’re the &lt;a href=&#34;http://simplystatistics.org/2013/08/09/embarrassing-typos-reveal-the-dangers-of-the-lonely-data-analyst/&#34;&gt;only statistician in your group&lt;/a&gt;. In addition to attempting to communicate statistical ideas with scientists who may only understand statistics as far as linear regression and ANOVA, the communication runs both ways and everyone needs to be clear on what’s being done, how it’s being done, and how you write that up. Check all papers for comments before they’re submitted, otherwise you may end up leaving yourself open to falsifying your data. SAS still dominates job advertisements for statisticians (&lt;a href=&#34;http://simplystatistics.org/2013/08/08/data-scientist-is-just-a-sexed-up-word-for-statistician/&#34;&gt;or data scientists&lt;/a&gt; if there is indeed a distinction) but &lt;a href=&#34;http://blog.revolutionanalytics.com/2013/08/job-trends-for-statistics-packages.html&#34;&gt;R jobs are on the rise&lt;/a&gt;. It’d be interesting to see if this is related to the cost of licensing and training for SAS. I was checking out &lt;a href=&#34;http://3.14-pi.net/&#34;&gt;a website&lt;/a&gt; that was posted on &lt;a href=&#34;http://www.itsokaytobesmart.com/&#34;&gt;Joe Hanson’s Tumblr&lt;/a&gt; when my supervisor walked in and asked what I was looking at. The site lists off the digits of π and plays a kick drum sounds for 0, a hi hat for 1, and tones on a scale for 2 to 9. “What purpose does this have?” I don’t know, but it’s a neat little thing and there’s no repeating melody thanks to the irrationality of π. This reminds me a little of Brian Eno’s “&lt;a href=&#34;http://en.wikipedia.org/wiki/Brian_Eno#Generative_music&#34;&gt;Generative Music&lt;/a&gt;”, which uses some initial sounds to generate unique melodies, through adjustable parameters, that may not repeat for long, long, long periods of time (of the order of thousands of years, potentially). Eno released &lt;a href=&#34;http://www.youtube.com/watch?v=KE86wbZhlWYwhich&#34;&gt;an entire album&lt;/a&gt; of such music but I much prefer “&lt;a href=&#34;http://www.youtube.com/watch?v=RfKcu_ze-60&#34;&gt;Ambient 1: Music for Airports&lt;/a&gt;” which uses tape loops of varying lengths and phasing to create musical patterns whose lengths are &lt;a href=&#34;http://en.wikipedia.org/wiki/Commensurability_(mathematics)&#34;&gt;incommensurable&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/08/02/posterior-samples/</link>
      <pubDate>Fri, 02 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/02/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://lindsaybradford.wordpress.com/2013/07/25/the-database-design-goggles-they-do-nothing/&#34;&gt;Database design is important&lt;/a&gt;, especially if someone else has to work with your database. It’s not really something we teach in undergraduate science, perhaps the American model of requiring a certain number of credits from certain fields would help remedy this. Ever wanted a glimpse of &lt;a href=&#34;http://xianblog.wordpress.com/2013/07/22/bayes-notebook/&#34;&gt;Bayes’ notebook&lt;/a&gt;? Laura McInerney’s comments on the benefits of &lt;a href=&#34;http://thesiswhisperer.com/2013/07/31/in-praise-of-the-small-conference/&#34;&gt;small conferences&lt;/a&gt; are similar to my experience with &lt;a href=&#34;http://bayesian.org/node/1657&#34;&gt;8 BNP&lt;/a&gt; in Veracruz, Mexico. As long as you’re within the niche field this sort of conference is a great experience. I felt a little like an outsider at 8 BNP because while I was interested in non-parametrics and was working on smoothing, a lot of people were working on things that I had no experience with which are actually the central elements of the field. I got to learn about a lot of neat things, hear some great talks and meet lots of amazing people, but I don’t think I was steeped in NP Bayes enough to really get the most out of the conference. My research went a bit away from NP Bayes these last few years so I didn’t get to put anything together for 9 BNP in Amsterdam. Perhaps ISBA 2014 in Cancún, Mexico will provide a bit more of a chance to get back to that work. We’re teaching R in SEB113. Perhaps any students reading this might be interested in these &lt;a href=&#34;http://www.computerworld.com/s/article/9239799/60_R_resources_to_improve_your_data_skills&#34;&gt;60 R resources&lt;/a&gt;. I use multiple monitors at work but really enjoyed the virtual monitors setup in Gnome when I ran Ubuntu. &lt;a href=&#34;http://lifehacker.com/5616859/is-the-multiple+monitor-productivity-boost-a-myth&#34;&gt;It turns out&lt;/a&gt; that having a large canvas of pixels, rather than multiple monitors, is the key to workplace productivity. My work setup has two widescreen monitors side by side in portrait orientation. This doesn’t work particularly well with programs that assume you’re using a single landscape monitor (such as RStudio) or give you a single window with multiple documents inside that each have focus one at a time (Microsoft Office, why can’t I have a spreadsheet on each monitor?) but it means I don’t have to keep switching back and forth between TeXStudio and RStudio when I’m writing up my analysis. &lt;a href=&#34;http://chronicle.com/article/Introduction-to-Ancient/140475/&#34;&gt;Flipped classes&lt;/a&gt; are an interesting model for education. I remember taking an Honours level mathematical modelling course a few years ago where the three hours of lecture time allocated us were used to discuss concepts and do modelling. We would read a chapter from the textbook in the lead-up to the class and then have a talk about what it meant and then work out a model based on a case study. I don’t know how well a truly flipped class would translate to a group bigger than about 30 students, but Sue Savage (QUT) tells me that the new lecture theatres in P block are designed to facilitate small group discussions within lectures. Daina Taimiņa explains &lt;a href=&#34;https://www.youtube.com/watch?v=w1TBZhd-sN0&#34;&gt;hyperbolic geometry&lt;/a&gt; with crochet. Every once in a while something similar pops up and I can’t help but get excited. &lt;a href=&#34;http://longnow.org/essays/richard-feynman-connection-machine/&#34;&gt;Daniel Hills recalls his memories&lt;/a&gt; of working with Richard Feynman on developing a massive parallel computer in the 1980s.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples - SEB113 edition</title>
      <link>/./2013/07/10/posterior-samples---seb113-edition/</link>
      <pubDate>Wed, 10 Jul 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/07/10/posterior-samples---seb113-edition/</guid>
      <description>&lt;p&gt;I’m going to be frank, a lot of this relates to SEB113 - Quantitative Methods for Science, a subject I tutored last semester. One of the students from SEB113 last semester, Daniel Franks, is &lt;a href=&#34;https://twitter.com/dpfscience&#34;&gt;live-tweeting his Bachelor of Science degree&lt;/a&gt;. Daniel was in my workshop group last semester and I recognise some of the events he talks about in his timeline. It’s interesting to see his perspective not just on SEB113 but on the other three units that form the first semester of QUT’s new Bachelor of Science course. SEB113 is getting a small makeover for Semester 2. One of the things we’re considering is the use of ggplot2 instead of a combination of the base graphics package, heatmaps from dendrograms with colorbars from yet another package, etc. Lattice doesn’t have the nicest interface and it’s nigh on impossible to add elements afterwards (I hate you, levelplot). It’s possible to do &lt;a href=&#34;http://gettinggeneticsdone.blogspot.com.au/2010/01/ggplot2-tutorial-scatterplots-in-series.html&#34;&gt;small multiples in ggplot2&lt;/a&gt; fairly easily. We ought to be sticking to the same steps in data analysis as we did last semester, and Daniel’s tweets refer to an experience in class last semester where we discussed drawing the analysis method out of exploratory plots of the data, rather than trying to pick the “best” model &lt;em&gt;a priori&lt;/em&gt; and making the data fit the model. &lt;a href=&#34;http://simplystatistics.org/2013/06/27/what-is-the-best-way-to-analyze-data/&#34;&gt;Roger Peng’s got a good five step&lt;/a&gt; technique for analysing data:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory analysis&lt;/li&gt;
&lt;li&gt;Model fitting&lt;/li&gt;
&lt;li&gt;Model building&lt;/li&gt;
&lt;li&gt;Sensitivity analysis&lt;/li&gt;
&lt;li&gt;Reporting&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Via &lt;a href=&#34;http://www.quantumforest.com/2013/07/flotsam-13-early-july-links/&#34;&gt;Luis Apiolaza at Quantum Forest&lt;/a&gt; I’ve stumbled across &lt;a href=&#34;http://www.statschat.org.nz/&#34;&gt;Thomas Lumley&lt;/a&gt;’s Tumblr, where he’s doing some personal blogging about statistics. An &lt;a href=&#34;http://notstatschat.tumblr.com/post/54011641155/where-is-bayesian-introductory-statistics-better&#34;&gt;interesting post&lt;/a&gt; of his is on the role of Bayesian stats in introductory classes. I would love to turn SEB113 into a Bayesian statistics based class but for the time being I will have to settle for it dealing with modelling over tests (which is still a big win, pedagogically). Teaching Bayesian statistics generally relies on a good grounding in calculus, otherwise writing down full conditionals is going to be quite difficult. When people tell me that statistics is so different to mathematics I like to point out that it’s just a combination of calculus, linear algebra and some discrete mathematics. &lt;a href=&#34;http://magazine.amstat.org/blog/2013/07/01/calculus-and-statistics/&#34;&gt;Daniel Kaplan writes at &lt;strong&gt;AMSTAT&lt;/strong&gt;News&lt;/a&gt; about ditching mathematical formalism to make statistics more accessible. The American undergraduate model is very different to what we have in Australia, but I take his point about a first year calculus class not being as relevant to graduates as a first year statistics course that teaches statistical thinking over statistical calculation. I really like the focus in SEB113 on modelling using R rather than statistical tests by hand with pages of tables (as MAB101 was when I did my Bachelor of Science). If people finish SEB113 knowing how to read their data in to R and perform a Generalised Linear Model I think we’ll have done our job. If they want to go on to further statistics from there, the statistics units in the School of Mathematical Sciences work from a calculus perspective and while they require a calculus pre-requisite (MAB121 or MAB122 for those QUT students reading) you could do a lot worse than taking MAB210 and MAB314. I hope a follow-up data analysis course will be offered to Bachelor of Science students that builds on SEB113 and covers some more advanced topics and introduces enough mathematics to make those topics worthwhile. We’ll have to see how it all unfolds as this first cohort make their way through.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/06/24/posterior-samples/</link>
      <pubDate>Mon, 24 Jun 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/06/24/posterior-samples/</guid>
      <description>&lt;p&gt;I was a bit disappointed that it wasn’t about machines doing automatic analysis for us, but this article, “&lt;a href=&#34;http://simplystatistics.org/2013/06/14/the-vast-majority-of-statistical-analysis-is-not-performed-by-statisticians/&#34;&gt;The vast majority of statistical analysis is not performed by statisticians&lt;/a&gt;”, is a bit of a wake-up call for those statisticians who haven’t realised that we need to improve the way we teach statistics and interact with non-statisticians. I don’t think we have enough statisticians in the world to do all the analysis that needs doing, so we need to focus on training scientists and others better so that we don’t leave them stuck in a culture of bad regression and t-tests in Excel. Gianluca Baio (UCL) has &lt;a href=&#34;http://www.statistica.it/gianluca/Talks/INLA.pdf&#34;&gt;a really nice introduction&lt;/a&gt; to &lt;a href=&#34;http://www.r-inla.org/&#34;&gt;INLA&lt;/a&gt; with a comparison to &lt;a href=&#34;http://mcmc-jags.sourceforge.net/&#34;&gt;JAGS&lt;/a&gt;. I started using JAGS when WinBUGS/OpenBUGS was becoming too slow for the analysis I was doing but the major paper of my thesis uses INLA for spatio-temporal analysis. I still use both programs and when faced with a new problem will usually start in JAGS as it’s quite flexible in the way you set up priors. INLA has its advantages as well, one of them being that it will fit a Poisson likelihood to non-integer data very well. There’s &lt;a href=&#34;http://blogs.plos.org/attheinterface/2013/06/19/why-art-and-science/&#34;&gt;a neat little article on the PLoS blog&lt;/a&gt; about linkages between art and science and how the involvement of art in research (beyond making prettier plots, which is really more an issue of design than art) can lead to better scientific outcomes. Radford Neal has just announced &lt;a href=&#34;http://radfordneal.wordpress.com/2013/06/22/announcing-pqr-a-faster-version-of-r/&#34;&gt;pqR&lt;/a&gt;, “pretty quick R”, which is designed to make use of multiple cores wherever possible and avoid unnecessarily onerous computation. It’s not available for Mac/Windows yet, so I won’t be able to look at it for the time being. I wonder if QUT’s HPC group would consider making it available on the supercomputer.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/06/13/posterior-samples/</link>
      <pubDate>Thu, 13 Jun 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/06/13/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://themonkeycage.org/2013/05/29/how-to-make-an-academic-conference-and-more-accessible-and-more-interesting-live-streaming-and-conference-hashtags&#34;&gt;Josh Tucker at the Monkey Cage&lt;/a&gt; and &lt;a href=&#34;http://simplystatistics.org/2013/06/11/why-not-have-a-future-of-the-field-session-at-a-conference-with-only-young-speakers/&#34;&gt;Jeff Leek at Simply Statistics&lt;/a&gt; have some good ideas and goals as to how to make academic conferences more interesting. Opening up the conference with live streaming is a cheap way to engage the wider public (but risks potential attendees deciding to stay home and watch rather than engaging with the rest of the conference). A youth session is also a really good way to engage in some inter-generational communication by giving the younger researchers a stage for setting out their vision. &lt;a href=&#34;http://f1000research.com/data-policies&#34;&gt;These journals&lt;/a&gt; wouldn’t view publication of data as prior publication when assessing an article based on that data. I spent last week in Sydney with Guy Marks’ group at the &lt;a href=&#34;http://www.woolcock.org.au/&#34;&gt;Woolcock Institute of Medical Research&lt;/a&gt;. I first met Marks at Bayesian data analysis course run by Kerrie Mengersen; he &lt;a href=&#34;http://sydney.edu.au/medicine/people/academics/profiles/guym.php&#34;&gt;does work&lt;/a&gt; on tuberculosis in Vietnam. Christian Robert, a collaborator of Mengersen’s, just gave 40 students in Ho Chi Minh City a crash course in Bayesian statistics. The slides are available &lt;a href=&#34;http://xianblog.wordpress.com/2013/06/05/teaching-bayesian-statistics-in-hochimin-city/&#34;&gt;here&lt;/a&gt;. Speaking of Bayesian stats, the term “derp” is apparently a &lt;a href=&#34;http://au.businessinsider.com/sorry-haters-derp-isnt-going-away-2013-6&#34;&gt;Bayesian way of describing the way someone believes something so strongly&lt;/a&gt; (their prior) that they can not be swayed by data. I’m going to be doing a lot more work teaching people how to use R in the near future. &lt;a href=&#34;http://www.computerworld.com/s/article/9239625/Beginner_s_guide_to_R_Introduction&#34;&gt;This looks like a handy resource for beginners&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Meta-analysis? Meta-regression?</title>
      <link>/./2013/05/14/meta-analysis-meta-regression/</link>
      <pubDate>Tue, 14 May 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/05/14/meta-analysis-meta-regression/</guid>
      <description>&lt;p&gt;Meta-analysis with a covariate feels really weird. I’m wanting to compare the relationship between the distributions of the mean concentration of endotoxin in the air and in dust samples across 50 locations. I wasn’t sure I did it the right way but the posterior estimates are consistent with my naïve approach of regressing the means of the air and dust samples. It’s important to account for the variability when doing this sort of &lt;em&gt;post hoc&lt;/em&gt; analysis because a point estimate of the mean doesn’t reflect anywhere near the full set of knowledge you have about your parameters of interest. On an unrelated note, another UPTECH paper has been &lt;a href=&#34;http://pubs.acs.org/doi/abs/10.1021/es400041r&#34;&gt;published&lt;/a&gt;. This one’s looking at spatial variation of particle number concentration in the school environment. Congratulations to Farhad Salimi, the first author of this paper, on the publication of his first paper. Farhad’s one of the PhD students on the UPTECH project and is due to finish his thesis later this year. I’ve worked with him on two of his papers (this one and another which has been submitted) and he’s really thrown himself into learning how to use R. This has not only made it easier for me to collaborate with him but it’s also made his analysis possible.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An argument for why careful coding in R is better than an Excel spreadsheet</title>
      <link>/./2013/04/18/an-argument-for-why-careful-coding-in-r-is-better-than-an-excel-spreadsheet/</link>
      <pubDate>Thu, 18 Apr 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/04/18/an-argument-for-why-careful-coding-in-r-is-better-than-an-excel-spreadsheet/</guid>
      <description>&lt;p&gt;I spend a fair amount of energy encouraging my coworkers to use R because you can write reusable code that means checking for errors is simple and you can re-run it on different data sets without having to manually copy and paste all your data and fix up any indexing if it’s the wrong length. There’s been a bit of buzz recently about how two economists really dropped the ball when they published a paper showing that high levels of national debt as a percentage of GDP slows economic growth. I won’t write any more about it because it’s been done by better writers and statisticians, but I’d like to bring your attention to the following articles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&#34;line-height:12px;&#34;&gt;&lt;a href=&#34;http://www.nber.org/papers/w15639.pdf&#34;&gt;The original working paper&lt;/a&gt; (National Bureau of Economic Research)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.peri.umass.edu/236/hash/31e2ff374b6377b2ddec04deaa6388b1/publication/566/&#34;&gt;Replicating the results&lt;/a&gt; (Political Economy Research Institute) &lt;a href=&#34;http://www.nextnewdeal.net/rortybomb/researchers-finally-replicated-reinhart-rogoff-and-there-are-serious-problems&#34;&gt;was only possible by making huge methodological and data selection mistakes&lt;/a&gt; (Next New Deal)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.guardian.co.uk/commentisfree/2013/apr/16/unemployment-reinhart-rogoff-arithmetic-cause&#34;&gt;This may have led to massive avoidable unemployment around the world&lt;/a&gt; (The Guardian)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://andrewgelman.com/2013/04/16/memo-to-reinhart-and-rogoff-i-think-its-best-to-admit-your-errors-and-go-on-from-there/&#34;&gt;The authors should gracefully retract, admit they were wrong and move on&lt;/a&gt; (Andrew Gelman’s blog)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://andrewgelman.com/2013/04/17/data-problems-coding-errors-what-can-be-done/?utm_source=feedly&#34;&gt;And to use R&lt;/a&gt; (Phil at Gelman’s blog) &lt;a href=&#34;http://andrewgelman.com/2013/04/17/excel-bashing/?utm_source=feedly&#34;&gt;rather than Excel&lt;/a&gt; (Andrew Gelman)&lt;/li&gt;
&lt;li&gt;There are many reasons to not use Excel, such as it providing the wrong answer for &lt;a href=&#34;http://people.stern.nyu.edu/jsimonof/classes/1305/pdf/excelreg.pdf&#34;&gt;paired t-tests&lt;/a&gt; and &lt;a href=&#34;http://or.nps.edu/faculty/PaulSanchez/oa4333/handouts/Excel/excel2000.pdf&#34;&gt;being terrible at generating Normal random numbers&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>New INLA stuff makes me happy</title>
      <link>/./2013/04/16/new-inla-stuff-makes-me-happy/</link>
      <pubDate>Tue, 16 Apr 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/04/16/new-inla-stuff-makes-me-happy/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;www.r-inla.org&#34;&gt;R-INLA&lt;/a&gt; is a really neat use of GMRFs for computing posteriors for quite complicated Bayesian Latent Gaussian Models. I used it for spatio-temporal modelling in my PhD and had to feel my way through a lot based on an old demo which was purely spatial. As I got further and further into my PhD I saw extensions for R-INLA being written thanks to a few visits from, and email correspondence with, &lt;a href=&#34;http://www.math.ntnu.no/~daniesi/&#34;&gt;Dr Daniel Simpson&lt;/a&gt;, and the help list on the R-INLA site where Dan, Håvard Rue and Finn Lindgren are very quick with a reply. A few days ago I got an email from Rue telling me he’d been made aware of &lt;a href=&#34;http://arxiv.org/abs/1206.3833&#34;&gt;one of my thesis papers&lt;/a&gt; and if I wouldn’t mind having a look at running it with the new testing distribution of R-INLA. It’s the first time I’ve looked at the code again since submitting the paper for publication and it seems that an awful lot of work has been put into internal optimisation. The code for running my model requires less manual tuning now and I’m excited about using it in follow-up papers where I’ll be looking at more of the UPTECH data. There’s also a &lt;a href=&#34;http://www.r-inla.org/examples/tutorials/spde-tutorial&#34;&gt;new tutorial for spatial modelling with INLA&lt;/a&gt;, written by &lt;a href=&#34;http://www.leg.ufpr.br/~elias/&#34;&gt;Elias T. Krainski&lt;/a&gt;, which covers a number of topics such as a simple spatial regression, a spatial model with misalignment and non-stationary spatial models (which I’ve seen talked about a few times but there’s very little documentation about them). I think R-INLA, particularly the spatial modelling, has really come a long way over the last few years and it’s encouraging to see it being taken up at QUT where students would probably have used WinBUGS in the past. While there are some limitations in terms of the flexibility of the classes of models that can be fit in BUGS versus R-INLA I’d much rather do any spatial, spatio-temporal or non-parametric smoothing in R-INLA.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Something I&#39;ve learned for BUGS/JAGS</title>
      <link>/./2013/03/19/something-ive-learned-for-bugs/jags/</link>
      <pubDate>Tue, 19 Mar 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/03/19/something-ive-learned-for-bugs/jags/</guid>
      <description>&lt;p&gt;Just quickly, I’m using JAGS with rjags to run some models for this fungal concentration paper I’m writing with our Finnish visitor (who leaves in a few months!). There are three levels of hierarchy in the experiment&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span style=&#34;line-height:12px;&#34;&gt;school (i = 1 to 25)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;measurement site within school (j = 1 to 3)&lt;/li&gt;
&lt;li&gt;sample number at measurement site (k = 1 to K&lt;sub&gt;j&lt;/sub&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;which mean that I was constructing my data frame as a three dimensional array, y[i,j,k] where there were uneven numbers of samples at each site within each school. Furthermore, modelling y[i,j,k] ~ dnorm(mu[i,j,k],tau.y) was giving me problems because I couldn’t tell JAGS to monitor a three dimensional parameter. I figured out that it’d just be easier to treat the levels of measurement site and sample number as covariates that I could use as index counters in JAGS. I’ve got much simpler code now and JAGS is monitoring mu[i] (i = 1 to 261). Edit: It’s been a busy month. I hope to finally publish that blog post about SEB113 soon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New things in Science and Engineering at QUT</title>
      <link>/./2013/02/18/new-things-in-science-and-engineering-at-qut/</link>
      <pubDate>Mon, 18 Feb 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/02/18/new-things-in-science-and-engineering-at-qut/</guid>
      <description>&lt;p&gt;Today was the first day of O week at QUT, a time when the relative calm of the summer break is disturbed by an influx of 17 year olds and university-run activities that always seem to generate a lot of noise. Is it possible to be a grumpy old man a week shy of 29? I received an email from my supervisor this morning asking if I could take over from one of the other PhD students in our group who had fallen ill last week and not recovered in time for a presentation this morning. The presentation, scheduled for 9am, was to be the first of the inaugural Nanotechnology and Molecular Science HDR (Higher Degree Research, i.e. Doctoral and Masters students) symposium. I’ve been moaning quietly, since starting my PhD in the School of Physical and Chemical Sciences, that the physics discipline had nothing like the School of Mathematics’ &lt;a href=&#34;http://samclifford.info/2012/09/07/qut-school-of-mathematics-postgrad-day-day-1/&#34; title=&#34;QUT School of Mathematics Postgrad Day - Day 1&#34;&gt;Postgrad Day&lt;/a&gt;. I really like Postgrad Day as it’s a good way to see what the other postgrad students are working on, what the research foci are within the school, and for students to improve their public speaking skills by delivering their research to a room of their peers and the other researchers in the school in an environment which is much more supportive than any conference is likely to be. The NMS HDR symposium brought together a number of students and staff from optics, aerosol science, nanomaterials, biotechnology, forensics and other fields within the discipline and allows them to see, perhaps for the first time, the research that others around them are doing. Even though my lab, ILAQH, is part of the Institute for Health and Biomedical Innovation, the distance between us and the remainder of IHBI is probably greater than just the physical distance between the two campuses. We do not seem to be particularly engaged with the culture of the remainder of IHBI and it’s very rare that our group will make the trek across to Kelvin Grove to see a presentation that is a short elevator ride away from the bulk of the IHBI membership. I have really only been to IHBI a few times. The two most recent appearances have been for the IHBI Olympics (a week of activities where research domains compete against each other in fun activities such as Iron Chef and photo scavenger hunt) in 2011 where I performed as part of the Health and Human Wellbeing domain’s talent quest entry, a four person improvisation troupe called “Ha ha… what?”, and to present the work that the PhD students of the UPTECH project had been working on (where we killed half an hour of time before the presentations by playing impro warm-up games). Continuing in this spirit of improvising in front of scientists, I spoke to the NMS HDR symposium at 45 minutes’ notice and in an eight minute talk managed to touch on the key points of the UPTECH project, explaining a small fraction of the science and discussing the richness of the dataset, the questions it will allow us to answer, and the diverse range of people we have involved in the project. I was told by one of the research staff in our group afterwards that it was refreshing to see a talk with no slides and that they were impressed at the quality of a talk that contained such a small amount of preparation and wondered whether I could give a presentation without speaking. Professor Dennis Arnold, the organiser of the symposium, is now based on the same floor as me; he is one of a handful of people on our floor who are not members of ILAQH. I asked him if he thought the day was a success and he was very positive. I sincerely hope that the NMS HDR symposium continues next year and well into the future, as a way to foster interest across the traditional divide of physics vs chemistry. I had to duck out of the symposium early to attend a meeting about one of the new units in the revamped Bachelor of Science degree. Dr Sama Low Choy, one of my supervisors, has asked me to run one of the collaborative workshops in the new &lt;a href=&#34;http://www.qut.edu.au/study/unit-search/unit?unitCode=SEB113&amp;amp;idunit=44499&#34;&gt;quantitative methods unit&lt;/a&gt; (she says it’s because of my impro skills). Today was one of the planning days where we got to grips with the structure of the unit, the way the workshops are to be run and how what we are doing is significantly different to anything we’ve done before. I’ll write more about it later, such as after my first tutorial, but it’s very exciting to see QUT break with tradition and make this unit happen. Through case studies with data sets relevant to their discipline, students will learn about quantitative methods in mathematics and statistics. We are ditching &lt;em&gt;t&lt;/em&gt; tests, removing the need for statistical tables, adding structure to the group work to ensure people don’t get to ride on the effort of others and teaching R and MATLAB in a first year unit that only supposes Maths B. I’m really excited that we’re teaching first year students how to use software that is free (well, at least R is) and far more powerful than Microsoft Excel. One of the problems with MAB101, the old unit, was that the computation was done in Minitab, a piece of software that I’ve never known any researcher to use. One of the workshop leaders said that they want to go back and do undergrad again knowing that this unit now exists; I don’t blame them. This will definitely be an exciting year for me, academically. A new course with new units, new facilities in the Science and Engineering Centre, new collaboration opportunities and the chance to pick somewhere new to move to at the end of the year.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Presenting data and mathematical ideas</title>
      <link>/./2012/12/15/presenting-data-and-mathematical-ideas/</link>
      <pubDate>Sat, 15 Dec 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/12/15/presenting-data-and-mathematical-ideas/</guid>
      <description>&lt;p&gt;I was hanging out with some friends the other night and the topic of conversation turned to an infographic that one of the guys was working on for a friend of his, &lt;a href=&#34;http://onceround.com/2012/12/11/red-dwarf-smeg-count/&#34;&gt;the number of times the word “smeg” (or a variant) appears in each episode&lt;/a&gt; of &lt;a href=&#34;http://www.imdb.com/title/tt0094535/&#34;&gt;Red Dwarf&lt;/a&gt; (made in &lt;a href=&#34;http://processing.org/&#34;&gt;Processing&lt;/a&gt;). We got chatting about data visualisation and ended up talking about Edward Tufte and his approach to the various aspects of using data to show information. Apparently his brother have him a copy of &lt;a href=&#34;http://www.edwardtufte.com/tufte/books_vdqi&#34;&gt;Tufte’s book&lt;/a&gt; last year and he was kind enough to loan it to me. &lt;a href=&#34;http://staff.qut.edu.au/staff/lowchoy/&#34;&gt;One of my supervisors&lt;/a&gt; is a huge fan of Tufte’s approach and it’s worn off on me. Throughout my thesis I’ve moved away from giant scatter plots of the data to summary plots that don’t use more “ink” than is necessary to show the information. An example he gives in the book is the boxplot, which typically contains a lot of redundant information. The image below shows Tufte’s stripped down boxplot and the default R boxplot for the same data. In the more traditional boxplot, the maximum of the data (within 1.5 IQR) is represented by the end of the whisker as well as a hinge. The hinge isn’t necessary and neither is the box which marks the 25th and 75th percentiles as the other end of the whisker represents these. With no box, there’s no need for a horizontal stripe for the median, so it can be represented as a dot.&lt;a href=&#34;http://samcliffordinfo.files.wordpress.com/2012/12/boxplots.png&#34;&gt;&lt;img src=&#34;boxplots.png?w=300&#34; alt=&#34;boxplots&#34; /&gt;&lt;/a&gt; There are some plots in my thesis papers of which I’m quite proud and I will upload some of them here once the papers are finalised. My Tufte-mad supervisor has even commented on how my plots have become quite minimalist, something she attributes to the 2000s/10s despite Tufte’s work dating back to the 1970s. I’ve noticed that the R packages I’ve tended to use (&lt;a href=&#34;http://r-inla.org/&#34;&gt;R-INLA&lt;/a&gt;, &lt;a href=&#34;http://cran.r-project.org/web/packages/mgcv/index.html&#34;&gt;mgcv&lt;/a&gt;) have quite simple graphics. While there are some fantastic plotting packages such as &lt;a href=&#34;http://ggplot2.org/&#34;&gt;ggplot2&lt;/a&gt; that make it quite easy to produce very pretty graphics, I feel far more familiar mucking around with the base graphics system to add points, lines and polygons to a blank plot. If you take the approach that Tufte does, that the ink on the page should represent data, and that there should be no extraneous elements to your plot (such as cross-hatching a barplot or colour where it doesn’t convey information) then it’s not hard to shy away from packages that do a lot of very nice, but ultimately data-poor, plotting. But graphs on the printed page are not the only way to represent data or mathematical or statistical concepts. The &lt;a href=&#34;http://www.youtube.com/watch?v=jK7xPo1YXzY&#34;&gt;Museum of Mathematics&lt;/a&gt; in New York looks to have a lot of really cool displays of a wide range of mathematical concepts, for example. My Advanced Calculus lecturer, Dr Jack Wrigley, had a background in education and often used props in class to illustrate ideas, such as holding pieces of paper against a balloon to give us a visual representation of a tangent and normal surfaces. I don’t often have props when doing improvised theatre but academic presentation is, at the end of the day, just another type of performance. Part of the work I’ve been doing on modelling temporal trends from split panel design data involves modelling penalised random walks where the random walk is on a torus that represents a joint term for the hour of the day and the day of the week. This “hour of the week” term has 168 unique values, but we want to smooth both in the day to day and hour to hour direction, rather than just looking at the circle formed by gluing Saturday 11pm to Midnight and then Sunday 1am. Some people might be very good at visualising Markov random fields through their precision matrix but there will be many people in my PhD final seminar audience who are not postgraduate level statisticians. For the purpose of explaining one of the key ideas in my thesis, I am considering bringing an inflatable pool ring and a marker in order to draw the smoothing directions on the torus that represents the product of two circular spaces. If this doesn’t make up for the conceded pass I got in Jack Wrigley’s class I have no idea what will.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2012/12/11/posterior-samples/</link>
      <pubDate>Tue, 11 Dec 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/12/11/posterior-samples/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;span style=&#34;line-height:12px;&#34;&gt;&lt;a href=&#34;http://www.statisticsblog.com/2012/12/information-graphics/&#34;&gt;Nice infographic of Mars missions&lt;/a&gt; showing all attempts and planned attempts&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;As much as I love using .csv files to send data back and forth, especially on GitHub, apparently I should &lt;a href=&#34;http://www.win-vector.com/blog/2012/12/pleases-stop-using-excel-like-formats-to-exchange-data/&#34;&gt;stop using Excel-like files to exchange data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;There are some really lovely visualisations of quite profound and complex scientific ideas among &lt;a href=&#34;http://www.wired.com/wiredscience/2012/12/science-figures-2012/&#34;&gt;The best scientific figures of 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;As someone who espouses the beauty of LaTeX and &lt;a href=&#34;http://samclifford.info/2012/08/12/credibility-of-information-given-font-choice/&#34;&gt;the importance of proper font choice&lt;/a&gt;, &lt;a href=&#34;http://typeanatomy.com/&#34;&gt;The Anatomy of Type&lt;/a&gt; is a book I’m considering buying. Maybe I should get it for my sister for Christmas.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sydneypadua.com/2dgoggles/lovelace-the-origin-2/&#34;&gt;A charming comic with an origin story for Ada Lovelace&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And, yes, I do plan on writing more about what I’m actually doing! I’m just working on thesis stuff a lot and trying to organise my final seminar. I’ve got some thoughts on presentations that I’m writing up, so stay tuned. One more &lt;a href=&#34;http://exp.lore.com/post/37688902608/60-seconds-of-awe-in-this-timelapse-of-the&#34;&gt;* Stunning time lapse photography of the eclipse.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2012/12/05/posterior-samples/</link>
      <pubDate>Wed, 05 Dec 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/12/05/posterior-samples/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.youtube.com/watch?v=5UYMnzXQEtw&amp;amp;feature=plcp&#34;&gt;&lt;span style=&#34;line-height:12px;&#34;&gt;MechBass - a final year engineering project to build a mechanical bass guitar&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://students.washington.edu/mclarkso/documents/line%20styles%20Ver2.pdf&#34;&gt;I had no idea I could change line caps &lt;em&gt;within R.&lt;/em&gt; I was just starting to get back into Corel Draw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://news.sciencemag.org/sciencenow/2012/10/dance-your-phd-and-the-winner-is.html&#34;&gt;Dance your PhD winner is a materials scientist from the University of Sydney&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.lhup.edu/~dsimanek/whoops.htm&#34;&gt;So often we see three gears together in a logo in such a way that it would impossible to move them&lt;/a&gt;. &lt;a href=&#34;http://www.youtube.com/watch?v=I9IBQVHFeQs&#34;&gt;This video shows three interlocking gears that all rotate together.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2012/11/23/posterior-samples/</link>
      <pubDate>Fri, 23 Nov 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/11/23/posterior-samples/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://littlestat.com/&#34;&gt;&lt;span style=&#34;line-height:12px;&#34;&gt;LittleStat is an online R tool for simple data analysis&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.readmatter.com&#34;&gt;Matter&lt;/a&gt; is a new essay-based science/tech publication that got its start on &lt;a href=&#34;http://www.kickstarter.com/projects/readmatter/matter/&#34;&gt;Kickstarter&lt;/a&gt; - this issue is about people who feel like they have an extra limb.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://0fps.wordpress.com/2012/11/19/conways-game-of-life-for-curved-surfaces-part-1/&#34;&gt;Conway’s Game of Life as a smooth PDE system&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://statisticalsocietyaustralia.wordpress.com/2012/11/20/november-question/&#34;&gt;SSAI wants to know what the biggest challenges in modern Bayesian statistics are&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.r-bloggers.com/RUG/2012/11/video-simpler-tricks-and-tools-help-debugging-git-latex-and-workflow-with-r-by-prof-rob-hyndman/&#34;&gt;SimpleR tricks and tools: Help, debugging, git, LaTeX, and workflow with R by Prof Rob Hyndman&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2012/11/14/posterior-samples/</link>
      <pubDate>Wed, 14 Nov 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/11/14/posterior-samples/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://xkcd.com/1133/&#34;&gt;Randall Munroe explains Saturn V using only the thousand most commonly used English words, and it’s hilarious&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://andrewgelman.com/2012/11/16808/#comment-109366&#34;&gt;Speaking of Randall Munroe, here’s his response to Gelman’s criticism of Munroe’s Bayesian vs Frequentist comic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://simplystatistics.org/post/35563800852/some-thoughts-on-teaching-r-to-50-000-students&#34;&gt;Some (but not my) thoughts on teaching R to 50,000 students&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.abc.net.au/local/stories/2012/11/13/3631638.htm&#34;&gt;There was an eclipse visible in Australia on the morning of November 14 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://musicmachinery.com/2012/11/12/the-infinite-jukebox/&#34;&gt;It’s getting hammered at the moment, but The Infinite Jukebox analyses songs for similar phrases and builds a series of paths between similar sections so that you can have an infinitely long song.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The Infinite Jukebox reminds me a bit about &lt;a href=&#34;http://www.youtube.com/watch?v=UqzVSvqXJYg&#34;&gt;Brian Eno’s work on generative systems&lt;/a&gt; for rule-based music. This in turn reminds me of Brian Eno’s album “Discreet Music”, which features an &lt;a href=&#34;http://www.youtube.com/watch?v=7-Vq4pmzMaE&#34;&gt;automatically played song and three re-arrangements of Pachelbel’s “Canon in D”.&lt;/a&gt; The album is designed to sit in the background and be fairly unobtrusive, but it’s hard not to notice the familiar phrases of “Canon in D”. I find this album good as background music at work. The generative systems video above takes me back to my undergraduate studies in mathematics, where we learned about &lt;a href=&#34;http://staff.qut.edu.au/staff/malletd/&#34;&gt;mathematical modelling using cellular automata&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2012/11/09/posterior-samples/</link>
      <pubDate>Fri, 09 Nov 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/11/09/posterior-samples/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://simplystatistics.org/post/34563838584/computing-for-data-analysis-simply-statistics-edition&#34;&gt;&lt;span style=&#34;line-height:12px;&#34;&gt;Content from very popular online R course run through Coursera to be published online soon&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.statisticsblog.com/2012/10/recommendation-of-the-week/&#34;&gt;Convinced your fancy analysis technique is the right approach? Run it on noise and see what you get&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://scienceseeker.org/&#34;&gt;Science news from science newsmakers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://statisfaction.wordpress.com/2012/11/06/just-for-the-fun-of-it/&#34;&gt;Should ecologists become Bayesians?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you’ve got a colour graph but want to print it black and white, make sure you check what it looks like first. &lt;a href=&#34;http://i.imgur.com/krzQu.jpg&#34;&gt;Otherwise you might struggle to make your point.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://andrewgelman.com/2012/11/poll-aggregation-and-election-forecasting/&#34;&gt;Andrew Gelman on poll aggregation and election forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.amazon.com/The-Signal-Noise-Predictions-Fail-but/dp/159420411X/ref=zg_bs_books_2&#34;&gt;Nate Silver’s book is #2 on the Amazon best-seller list at the moment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://simplystatistics.org/post/34635539704/on-weather-forecasts-nate-silver-and-the&#34;&gt;Speaking of Nate Silver, here’s a blog post on the politicisation of statistical literacy. (Bonus round: Ctrl-F “Schwimmer”. Extra bonus round: Andrew Gelman comments, saying “I wrote about this on my blog”, just like everyone else on the internet)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Turning tables into graphs</title>
      <link>/./2012/10/30/turning-tables-into-graphs/</link>
      <pubDate>Tue, 30 Oct 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/10/30/turning-tables-into-graphs/</guid>
      <description>&lt;p&gt;One of the things I’ve noticed from working with scientists (of various background) is that they love tables full of means and standard deviations as a way of summarising the variability in some data or regression parameters. &lt;a href=&#34;http://andrewgelman.com/2012/10/communication-is-a-central-task-of-statistics-and-ideally-a-state-of-the-art-data-analysis-can-have-state-of-the-art-displays-to-match/&#34;&gt;Andrew Gelman’s latest discussion of a paper&lt;/a&gt; makes the point that tables of numbers are awful and that a well made graphic does a good job of conveying the uncertainty. He refers in his comment to a paper he wrote, “Let’s Practice What We Preach: Turning Tables into Graphs” [1], which shows how graphs can be better at summarising variability, often in less space than a table. Another thing I really like about the paper is that it endorses the use of R/S/S+ for plotting and faults Excel for not offering enough control to the user (and it makes ugly graphs anyway). I’m a big fan of using graphs because numbers don’t really mean that much to me, especially when dealing with things like splines and random walk models for non-linear function estimation. The UPTECH papers I’m writing on the fungal data and nanotracer measurements have a lot of graphs where previously there were tables or Excel plots which weren’t as easy to interpret. I’ve been spending quite a bit of time on them so that we can present to our readers, for example, just how different the means are in our hierarchical Bayesian model. I think tables have a place and I use them in my own papers. I’m using a table in a spatial modelling paper to describe the prevailing winds and local geography at each of 13 measurement locations. There’s a map of the locations so that I don’t have to put things like “location” in the table. A list of features doesn’t translate as well to a plot as spatial locations do. I don’t think it’s appropriate to list row upon row of means, standard deviations, quantiles, etc. Long/wide tables of model fit criteria such as MSE, AIC, R&lt;sup&gt;2&lt;/sup&gt;, adjusted R&lt;sup&gt;2&lt;/sup&gt;, etc. are incredibly boring and do not scale well when you’re comparing more than, say, three models. I think I might try to send this paper around my group as an attempt to convince them to abandon tables in favour of concise graphs. With the uptake of R among some of the more senior researchers/staff looking promising, I think it’s a message that might actually get some traction. [1] Gelman, Andrew, Pasarica, C., and Dodhia, R. (2002). Let’s practice what we preach: turning tables into graphs. American Statistician 56, 121-130. [&lt;a href=&#34;http://www.stat.columbia.edu/~gelman/research/published/dodhia.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A goal for 2013</title>
      <link>/./2012/10/17/a-goal-for-2013/</link>
      <pubDate>Wed, 17 Oct 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/10/17/a-goal-for-2013/</guid>
      <description>&lt;p&gt;I made a comment earlier today that seemed like a bit of a throwaway at the time but I think it’s well worth pursuing.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;By the time I leave here next year, everyone in this group will be using R.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A few people in my group have been really quick to start using it for their analysis and their work is much improved because of it. A handful of others have been a bit reluctant to get started but admit to wanting to learn how and have started to pick up some short guides, books, etc. and ask me for help from time to time. The other members of the group can see that it’s useful but are probably reluctant to invest the time to learn and make do with Excel for the time being. The goal is to get the reluctant people confident with playing around and making mistakes, asking the more gung-ho members for help, while I teach a few basic concepts in a small group environment to those who are completely unfamiliar with R. I think it’s doable. There are plenty of ways to use R for analysis and visualisation, whether it’s via command line on QUT’s supercomputer, using &lt;a href=&#34;http://rstudio.org/download/desktop&#34;&gt;RStudio&lt;/a&gt;, &lt;a href=&#34;http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/&#34;&gt;R Commander&lt;/a&gt;, &lt;a href=&#34;http://www.revolutionanalytics.com/&#34;&gt;Revolution Analytics&lt;/a&gt;, &lt;a href=&#34;http://rforge.net/JGR/&#34;&gt;JGR&lt;/a&gt;, or some other variant.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Educational testing and statistical testing</title>
      <link>/./2012/09/24/educational-testing-and-statistical-testing/</link>
      <pubDate>Mon, 24 Sep 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/09/24/educational-testing-and-statistical-testing/</guid>
      <description>&lt;p&gt;When the Australian Government introduced &lt;a href=&#34;http://www.naplan.edu.au/&#34;&gt;NAPLAN&lt;/a&gt; and the &lt;a href=&#34;http://www.myschool.edu.au/&#34;&gt;MySchool&lt;/a&gt; website I was very worried about league tables being drawn up, the ranking of schools and the stigmatisation of poorer schools as being “bad” and parents opting to not send their kids there. I don’t have a problem with benchmarks for students, letting parents know how their kids are developing and ensuring that governments are able to target their resources where they’re needed. What I do have a problem with is this bizarre notion that “accountability” means the government throws good policy, teachers and kids under a bus to appease parents or the more neoliberal elements of the national media. If it’s not clear, I have grave concerns about the impact on our education system of publicly releasing nation-wide summary statistics of how well the students are doing at each school. I think the USA’s focus on standardised testing and the awful notion of “merit based pay” threaten the integrity of public education. Having said that, the collection of this data and its appropriate analysis provide governments with a very good tool for assessing their policies and determining where to spend the finite amount of money they have. &lt;a href=&#34;http://www.quantumforest.com/2012/09/new-zealand-school-performance-beyond-the-headlines/&#34;&gt;This post&lt;/a&gt; from Quantum Forest shows how a naive analysis of literacy versus socioeconomics in New Zealand can give a very misleading picture. To cut a long story short, there’s a lot of variability that plotting a trend line or some averages doesn’t take into account. The post is worth a read and doesn’t go into a huge amount of statistical detail but explains, with some well described R code, the use of boxplots for quick summaries of data that show the variability inherent in the data. The author also discusses how this exploration can lead to appropriate modelling which takes the variability into account. This is the sort of exploration that should form the basis of any academic analysis of any data and I’m grateful to the author for explaining it simply and providing R code and the publicly available data. To me, statistics is all about quantifying uncertainty; Bayesian statistics even moreso. Confidence/credible intervals are not just something we calculate to check that something’s significant at the 5% level, they’re how we represent how certain we are about our parameter estimates. Not stating the uncertainty in one’s analysis may as well be a cardinal sin and no one should get away with providing a plot or parameter value without an estimate of its variability. It doesn’t matter if you’re a first year student, research scientist, public servant or journalist, you need to include uncertainty or else you’re lying to your audience (a lie of omission, but still a lie). With a bit of luck, there’s a statistician working at a major news service who can make use of this when the next MySchool report comes out and some statisticians working in the Department of Education who are able to make use of good statistical modelling to make suggestions to the Minister regarding funding allocation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My take on the Bayesian updating video</title>
      <link>/./2012/08/27/my-take-on-the-bayesian-updating-video/</link>
      <pubDate>Mon, 27 Aug 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/08/27/my-take-on-the-bayesian-updating-video/</guid>
      <description>&lt;p&gt;I’m referring here to my &lt;a href=&#34;http://samclifford.info/2012/08/25/438/&#34;&gt;last post&lt;/a&gt; where I reblogged something that &lt;a href=&#34;http://bayesianbiologist.com/&#34;&gt;Bayesian Biologist&lt;/a&gt; &lt;a href=&#34;http://bayesianbiologist.com/2012/08/17/an-update-on-visualizing-bayesian-updating/&#34;&gt;had written&lt;/a&gt;. I’ve modified the code (and included it below) to tweak it to my tastes and show the different behaviour of the posterior under a uniform Beta(1,1) prior and an informative Beta(20,20) prior. Rather than flipping a fair coin I’ve assumed that the coin is unfair (p=0.25) and the Beta(20,20) prior is chosen by the naive observer who believes, with a high degree of certainty, that the coin is fair. This is, to me, an illustration of the power of uninformative priors (something Laplace seemed keen on in Binomial experiments). It’s also a good demonstration of how given enough data the posteriors will converge to the same result. A persistent criticism of Bayesian analysis is that the priors are so subjective that anyone can come up with their own prior and get a different result to another observer. As more data is collected, the influence of the prior in the posterior is diminished. We see (in the 1.08MB animation below the cut) that the Beta(1,1) prior is more sensitive to each success and failure and the posterior mean approximates p much quicker than the more “certain” Beta(20,20) prior. The take home message? Be more vague than you think is warranted just in case your prior is not diffuse enough. To generate the gif I’ve used the instructions in the video in the last lines of the code. &lt;a href=&#34;updating.gif&#34;&gt;&lt;img src=&#34;http://samcliffordinfo.files.wordpress.com/2012/08/updating.gif&#34; title=&#34;Bayesian updating&#34; /&gt;&lt;/a&gt; [sourcecode language=“r”] ## Corey Chivers, 2012 ## ## modifications by ## ## Sam Clifford, 2012 ## sim_bayes &amp;lt;- function(p=0.5,N=100,y_lim=20,a_a=2,a_b=10,b_a=8,b_b=3) { ## Simulate outcomes in advance outcomes&amp;lt;-sample(1:0,N,prob=c(p,1-p),replace=TRUE) success&amp;lt;-cumsum(outcomes) for(frame in 1:N) { png(paste(“plots/”,1000+frame,“.png”,sep=“”)) curve(dbeta(x,a_a,a_b),xlim=c(0,1),ylim=c(0,y_lim),col=‘green’,xlab=‘p’,ylab=‘Posterior Density’,lty=2) curve(dbeta(x,b_a,b_b),col=‘blue’,lty=2,add=TRUE) lines(x=c(p,p),y=c(0,y_lim),lty=2,lwd=1,col=“grey60”) i &amp;lt;- frame # i don’t like having the leftovers on the screen – Sam #for(i in 1:frame) #{ # curve(dbeta(x,a_a+success[i]+1,a_b+(i-success[i])+1),add=TRUE,col=rgb(0,100,0,(1-(frame-i)/frame) * 100,maxColorValue=255)) # curve(dbeta(x,b_a+success[i]+1,b_b+(i-success[i])+1),add=TRUE,col=rgb(0,0,100,(1-(frame-i)/frame) * 100,maxColorValue=255)) #} curve(dbeta(x,a_a+success[i]+1,a_b+(i-success[i])+1),add=TRUE,col=rgb(0,100,0,255,maxColorValue=255),lwd=2) curve(dbeta(x,b_a+success[i]+1,b_b+(i-success[i])+1),add=TRUE,col=rgb(0,0,100,255,maxColorValue=255),lwd=2) # modifications to make prior explicit in legend legend(‘topleft’,legend=c( paste(‘Observer A - Beta(’, a_a , ‘,’ , a_b , ‘)’,sep=“”),paste(‘Observer B - Beta(’, b_a , ‘,’ , b_b , ‘)’,sep=“”)),lty=1,col=c(‘green’,‘blue’)) text(0.75,17,label=paste(success[i],“successes,”,i-success[i],“failures”)) dev.off() } # Sam # system(’mencoder mf://plots/*.png -mf fps=15:type=png -ovc copy -oac copy -o plots/output.avi’) # we’ll use GIMP rather than mencoder # &lt;a href=&#34;http://www.youtube.com/watch?v=u5_3MGP2Lj4&amp;amp;feature=player_detailpage#t=265s&#34; class=&#34;uri&#34;&gt;http://www.youtube.com/watch?v=u5_3MGP2Lj4&amp;amp;feature=player_detailpage#t=265s&lt;/a&gt; } sim_bayes(a_a=1,a_b=1,b_a=20,b_b=20,p=0.25,N=250) [/sourcecode]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A few things from today</title>
      <link>/./2012/08/16/a-few-things-from-today/</link>
      <pubDate>Thu, 16 Aug 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/08/16/a-few-things-from-today/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://xianblog.wordpress.com/&#34;&gt;Christian Robert&lt;/a&gt; is visiting my stats research group at the moment. This morning he spoke to our group’s fortnightly meeting about a few issues in model choice and how to improve our Bayesian computation. The meeting was fairly well attended and was in fact &lt;a href=&#34;http://conidialcoleopticide.wordpress.com/&#34;&gt;Luisa&lt;/a&gt;‘s first BRAG meeting. The model choice stuff was interesting and involved a discussion of Bayes factors, the BIC and DIC, and posterior predictive densities and checks. I’m not especially familiar with the Bayes factor approach (despite our group reading a paper about them recently) but did pick up the criticism from Robert that while the BIC is consistent with Bayes factors, it’s not a very Bayesian approach and is a limited representation of the impact of the prior. There were some good little bits about the need to ensure that when using the BF to choose between nested models you don’t end up using priors that are so diffuse that there’s no concentration of support in the posterior. He also spoke briefly about the Kullback-Leibler Divergence and got into a discussion with Chris Strickland about model averaging with the BF to compare candidate models. Clearly I’ve got some reading to do. We didn’t have much time to talk about improved mixing for MCMC but Robert did offer tempering as a way to come up with better proposals. The basic idea is that instead of using the posterior, π(θ)×f(x’θ), you look at {π(θ)×f(x’θ)}^α (for α &amp;lt;&amp;lt; 1) to flatten out the posterior in order to allow a fuller exploration of the target distribution. This may end up giving you more density in the tails of the posterior (which is probably multidimensional) but the argument could be made (perhaps by Aad van der Vaart) that it’s more important to have a vague posterior that better captures the truth than a very tight posterior CI around something which is wrong. Tempering is apparently Robert’s “go to” trick when his MCMC is slowly mixing. I know Sama Low Choy, one of my supervisors, has mentioned it a few times. So I might try looking at it for the Finnish paper’s successors. One of my aerosol science group’s PhD students, Tenzin Wangchuk, gave his confirmation seminar today. Tenzin is with us for three months every year and spends the rest of his time in Bhutan. Tenzin’s work will look at indoor air quality in rural Bhutan and what factors (e.g. cooking, outdoor air exchange, fuel types) influence the concentration of fine particles and gaseous pollutants. I had a few meetings with Tenzin last time he was here, where we discussed approaches to data analysis, and experimental design. Tenzin’s a really nice guy; one of my favourite things about working at &lt;a href=&#34;http://www.ilaqh.qut.edu.au/&#34;&gt;ILAQH&lt;/a&gt; is the opportunity to meet people from around the world who have different experiences and interesting projects. Tenzin’s work is similar to t&lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0668.2010.00679.x/full&#34;&gt;he studies ILAQH have done in Laos&lt;/a&gt; but don’t focus on the health aspect. It’ll be interesting to see the results of his work. I’m also trying to get a beginners’ R reading group up and running between ILAQH and BRAG. A few ILAQH members attended a two day course run by one of the PhD students in BRAG but I’ve really found the reading group structure to be quite useful for learning from a shared position of ignorance of, but interest in, a topic. I think the real strength of the model is that it encourages attendees to be self-reliant for their learning while still providing an environment for seeking help. If you don’t do the readings and the work then it’s very hard to keep up with what the group is doing and to participate in the discussion. So hopefully it discourages the kind of person who wants to sit back and have everyone else do the work (undergrad group assignments are the worst for this) or at least encourages them to do the work. I’ve got a few PhD students expressing interest so far, as well as one or two of the academics! I’ll come along to the first few meetings to get the ball rolling, get everyone aware of each others’ learning styles and backgrounds, give an overview of what R is and then give them a few resources and references to get started before leaving them to their own devices to study what they want to learn.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Minimal version control lesson - use it</title>
      <link>/./2012/07/31/minimal-version-control-lesson---use-it/</link>
      <pubDate>Tue, 31 Jul 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/07/31/minimal-version-control-lesson---use-it/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.win-vector.com/blog/2012/07/minimal-version-control-lesson-use-it/&#34;&gt;From the Win-Vector blog&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There is no excuse for a digital creative person to not use some sort of version control or source control. In the past disk space was too dear, version control systems were too expensive and software was not powerful enough; this is no longer the case. Unless your work is worthless both &lt;a href=&#34;http://www.win-vector.com/blog/2009/06/public-service-article-back-up/&#34;&gt;back it up&lt;/a&gt; &lt;em&gt;and&lt;/em&gt; version control it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is probably the best advice I can give anyone about why using git is a good idea. There’s no reason to take care of work that is not valuable. If you’re doing any kind of research that requires writing, back it up and use version control (especially if you’re coding). The above article gives a nice, simple introduction to using git without heading off into lengthy discussions about the role of branches and how to fork repositories. Given that I’ve been encouraging people in my group to move into using git and R this might be a good article to pass around.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Online courses in Bayesian statistics</title>
      <link>/./2012/07/30/online-courses-in-bayesian-statistics/</link>
      <pubDate>Mon, 30 Jul 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/07/30/online-courses-in-bayesian-statistics/</guid>
      <description>&lt;p&gt;I just got an email from Jannah Baker (fellow QUT PhD student and student assistant chair of the SSAI) about three training courses that are being run online through &lt;a href=&#34;http://statistics.com&#34;&gt;statistics.com&lt;/a&gt;. Bill Bolstad will be running an introduction to Bayesian statistics from &lt;a href=&#34;http://www.statistics.com/bayesian/&#34;&gt;August 17 to September 14&lt;/a&gt;. Peter Congdon will be running a course on Bayesian computation from &lt;a href=&#34;http://www.statistics.com/BayesianComputing/&#34;&gt;September 21 to October 19&lt;/a&gt; and then another course on MCMC for regression from &lt;a href=&#34;http://www.statistics.com/MCMC/&#34;&gt;November 16 until December 14&lt;/a&gt;. All three of these courses consist of four sessions which will touch on various aspects of Bayesian inference. If you’re interested in learning about Bayesian inference and how to perform Bayesian regression in a computational environment (WinBUGS and R, as far as I can tell) then you might want to consider these courses. They aren’t free (so if you’re looking for free, I suggest checking out &lt;a href=&#34;http://samclifford.info/2012/07/15/revision-revised/&#34; title=&#34;Revision, revised&#34;&gt;videolectures&lt;/a&gt;, getting &lt;a href=&#34;http://samclifford.info/2012/07/13/where-to-start-if-youre-going-to-revise-statistics/&#34; title=&#34;Where to start if you&amp;#39;re going to revise statistics&#34;&gt;a good book&lt;/a&gt; and looking at &lt;a href=&#34;http://cran.r-project.org/web/views/Bayesian.html&#34;&gt;relevant R packages and tutorials&lt;/a&gt;).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Changes in the way my group does work</title>
      <link>/./2012/07/19/changes-in-the-way-my-group-does-work/</link>
      <pubDate>Thu, 19 Jul 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/07/19/changes-in-the-way-my-group-does-work/</guid>
      <description>&lt;p&gt;Yesterday I had a 45 minute chat with two of the PhD students in my group about statistics. I had made some comments at one of their talks at Healthy Buildings about the summary statistics they were using for a ratio of indoor and outdoor particle concentrations and they wanted to catch up to discuss that and my first paper on using GAMs for temporal trends and meteorology. I think I’ve managed to convince both of them that using R is a good idea and that using spline models will help look at non-linear effects. They’re going to read some of the references that I listed. We also had a chat about how ANOVA’s a good start for data analysis and I suggested that they read Gelman’s paper (recently referred to in my &lt;a href=&#34;http://samclifford.info/2012/07/12/plenary-speech/&#34; title=&#34;Plenary speech&#34;&gt;plenary speech&lt;/a&gt;). I think the &lt;a href=&#34;http://cran.r-project.org/web/packages/openair/index.html&#34;&gt;openair&lt;/a&gt; package is something they might look at given one of the other students in their room uses it. Openair uses mgcv to do some of its non-parametric estimation. I’m also starting a paper with one of our postdocs looking at personal exposure to ultrafine particles. We’ve got some very interesting data from quite a new instrument and the way the experiment was designed we are going to be able to answer some very interesting questions. The colleague who will probably be the primary author has to move back overseas for at least a few months so we’ll be collaborating from across an ocean or two. Our meetings recently have involved having an instance of R open to look at some data and a LaTeX document for writing down what we decide. Our postdoc asked me to email them the file but instead we sat down and set up a GitHub account and I showed that it wasn’t particularly hard to operate git in terms of basic functionality. They’ve used MATLAB before and have been getting used to using R, so this shouldn’t be too hard for them. So it’s nice to see a few little changes here and there not just in terms of how we approach a problem but with the tools we use to make our jobs easier and to improve the quality of our output.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Starting more papers</title>
      <link>/./2012/07/17/starting-more-papers/</link>
      <pubDate>Tue, 17 Jul 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/07/17/starting-more-papers/</guid>
      <description>&lt;p&gt;One of the benefits of being a statistically curious researcher is that you get to read about all sorts of cool stuff. The UPTECH project is generating a huge amount of time series data, some of which have change points, non-linear behaviour, trends, and all sorts of other quirks. I’ve spent most of my time learning about the use of splines but over the last year have been exposed to Gaussian processes (and I guess I would say splines are a special case) and Gaussian Markov Random Fields. I’ve been having the occasional chat with the other researchers about how to analyse the time series data they’re working with and have stumbled across some really neat methods. Apart from the work I’ve been doing on spline models with my Finnish collaborators, interesting ideas for analysing time series data include Treed Linear Models, Treed Gaussian Processes [1,2] and Dirichlet Process Mixtures of GLMs [3]. The tree nature of the first two models I mentioned is apparent in its partitioning of the covariate space into regions in which the behaviour is locally linear. Change points are placed where the behaviour changes and each partition has its own linear mean and its own variance estimate. This is a fairly simple model to fit but it’s a bit limited by its only using linear functions. The treed GP relaxes this and spends its time fitting a more GP within each partition, with the focus on the covariance relationship. The third, DP mixtures of GLMs gives much smoother estimates of the mean and credible interval and has some really nice properties courtesy of the DP (which looks to be superior to tree based clustering). I find the tree structure of these models quite interesting and the treed linear model appears to be, conceptually, a mix of a multiple changepoint model and a piecewise linear regression spline with wombling knots. I’m not 100% sure how to apply these but an initial chat makes me think they will be very applicable and I’m looking forward to some exploratory data analysis. [1] [Gramacy, R. B. (2007). tgp: An r package for bayesian nonstationary, semiparametric nonlinear regression and design by treed gaussian process models. Journal of Statistical Software 19(9), 1-46.](&lt;a href=&#34;http://www.jstatsoft.org/v19/i09/&#34; class=&#34;uri&#34;&gt;http://www.jstatsoft.org/v19/i09/&lt;/a&gt;) [2] [Gramacy, R. B. and H. K. H. Lee (2008). Bayesian treed gaussian process models with an application to computer modeling. Journal of the American Statistical Association 103(483), 1119-1130.](&lt;a href=&#34;http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000689&#34; class=&#34;uri&#34;&gt;http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000689&lt;/a&gt;) (&lt;a href=&#34;http://arxiv.org/abs/0710.4536&#34;&gt;arXiv preprint&lt;/a&gt;) [3] [Hannah, L. A., D. M. Blei, and W. B. Powell (2011). Dirichlet process mixtures of generalized linear models. Journal of Machine Learning Research 12, 1923-1953.](&lt;a href=&#34;http://www.cs.princeton.edu/~blei/papers/HannahBleiPowell2011.pdf&#34; class=&#34;uri&#34;&gt;http://www.cs.princeton.edu/~blei/papers/HannahBleiPowell2011.pdf&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Where to start if you&#39;re going to revise statistics</title>
      <link>/./2012/07/13/where-to-start-if-youre-going-to-revise-statistics/</link>
      <pubDate>Fri, 13 Jul 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/07/13/where-to-start-if-youre-going-to-revise-statistics/</guid>
      <description>&lt;p&gt;I’d like to think it was my plenary speech that has spurred this, but given that they didn’t see it I’m not sure that it was, but one of the academics in my lab has decided it’s time to refresh their statistical knowledge. I think this is great, because they got their PhD a long time ago and have probably been using the same statistical methods for at least the last ten years. The book they’ve decided to use is the &lt;a href=&#34;http://www.amazon.com/Schaums-Outline-Statistics-Murray-Spiegel/dp/0070602816&#34;&gt;Schaum’s Outline of Statistics&lt;/a&gt;. I’ve used books from this range before to revise linear algebra, differential equations, etc. and have even taught small courses (privately) based on the topics they cover and in the order they cover them. A flick through the book confirmed that it was full of frequentist testing and other similar statistical methods that I spent my talk saying were a good start but not the end of the analysis when writing a scientific paper. The book’s online summary says it covers the use of MINITAB. I’m glad to see that it discusses the use of software other than Excel to perform analysis but I recommend people use books in Springer’s “&lt;a href=&#34;http://www.springer.com/series/6991?detailsPage=titles&#34;&gt;Use R!&lt;/a&gt;” range because R is free (in terms of both speech and beer) and is much more flexible than MINITAB in terms of programming it and running different types of analysis. Where MINITAB is based on a point and click GUI, making it great for a first year statistics class where students may not be familiar with programming, R is driven by the command line and is more easily scripted. Learning to use R means giving yourself the opportunity to use the many &lt;a href=&#34;http://cran.r-project.org/web/packages/available_packages_by_name.html&#34;&gt;packages&lt;/a&gt; that extend its functionality. The books I’ve used in the Use R! range include &lt;a href=&#34;http://www.springer.com/statistics/computational+statistics/book/978-0-387-93836-3&#34;&gt;A Beginner’s Guide to R&lt;/a&gt; and &lt;a href=&#34;http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-92297-3&#34;&gt;Bayesian Computation with R&lt;/a&gt;. While I’d definitely recommend these Use R! books, as they’re aimed at people wanting to use R to do better analysis, there are a few others that I’ve found incredibly useful. It’s important for me to point out that my background differs from my colleague’s so they may not find the books as relevant or accessible. &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/&#34;&gt;Gelman et. al - Bayesian Data Analysis&lt;/a&gt;. Around my stats group, this book is called “The Bible”. It’s probably the best textbook I’ve come across. It’s full of information, tutorials, detailed descriptions of the theory and methods and how they can be applied. This is certainly a graduate level statistics textbook, though, and it assumes calculus at what I’d say is probably a second year mathematics degree level. You may find this book difficult if you don’t feel comfortable with multiplying integrals together (which is really what Bayesian analysis is). I sent my colleague a link to &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/research/published/AOS259.pdf&#34;&gt;Gelman’s Annals of Applied Statistics article on ANOVA&lt;/a&gt;. Might as well plug &lt;a href=&#34;http://andrewgelman.com/&#34;&gt;Gelman’s blog&lt;/a&gt; while I’m talking about him. &lt;a href=&#34;http://www.amazon.com/Biostatistics-Bayesian-Introduction-Probability-Statistics/dp/0471468428&#34;&gt;Woodworth - Biostatistics: a Bayesian introduction&lt;/a&gt;. I found this very useful in giving a more applied approach to introductory Bayesian statistics. There’s a good review of the book &lt;a href=&#34;http://www.theannals.com/content/39/7/1376.2.full&#34;&gt;here&lt;/a&gt; and I agree with the reviewers about the importance of the preface in that it talks about the philosophy of statistics in science and discusses the differences between frequentist and Bayesian statistics. The book walks the reader through a lot of the topics which frequentist statistics deals with but in a Bayesian setting. I find this sort of comparison very useful (and appreciate when Mike Jordan says that a lot of machine learning techniques are just Bayesian statistics with a different name) as most people who have taken a statistics class will have seen linear modelling, ANOVA and a little about statistical design. The book also introduces the use of &lt;a href=&#34;http://www.openbugs.info/w/&#34;&gt;WinBUGS&lt;/a&gt; as a tool for Bayesian modelling. As an aside, I attended an introductory course run by my supervisor, Kerrie Mengersen, where she was teaching us how to use R to write a Gibbs sampler for a very simple problem and how to do it in WinBUGS as well. One of the other attendees, the leader of a medical science research group, had it in their head that they would use Excel to write the Gibbs sampler because it provides nice reports (summary stats, plots, etc.) through a plugin they had. Comparing the time it took WinBUGS and R to run the code against the Excel’s run time was probably what convinced me that Excel was one of the worst pieces of software that one could use for statistics. Great for spreadsheets, awful for statistics. A non-technical book which does a good job extending the philosophical discussion to the history of Bayesian statistics and its use in solving some very complex problems is &lt;a href=&#34;http://www.amazon.com/Theory-That-Would-Not-Die/dp/1452636850&#34;&gt;Sharon Bertsch McGrayne’s The Theory That Would Not Die&lt;/a&gt; (which I like to think of as the “A Brief History of Time” of Bayesian statistics). It’s very readable and really drives home the importance of Bayesian statistics and the profundity of Bayes and Laplace in developing this approach. I really don’t think there’s much use revising the basics of frequentism as I disagree with its interpretation of probability and find the idea of confidence intervals problematic. Hypothesis testing is also another problem that I have with frequentism and I think we’re going to see a lot of scientific papers in the near future converting the p values for their ANOVA into a “sigma” level as a result of CERN’s announcement of the 5 sigma certainty of their search for a new boson. Tony O’Hagan has &lt;a href=&#34;http://bayesian.org/forums/news/3648&#34;&gt;a good post&lt;/a&gt; about the “sigma” issue on the ISBA forums. Edited to add: TL;DR? Got a maths degree and some familiarity with stats? Read Gelman. Don’t have a maths degree? Read Woodworth. Want to understand what Bayesian stats is but don’t want to read a textbook? Read McGrayne. Want to know how to use R? Read a Use R! book. Once you’ve got a decent understanding of what statistics is, read papers for specific topics because there is almost never a book about what you want.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LaTeX and git</title>
      <link>/./2012/06/13/latex-and-git/</link>
      <pubDate>Wed, 13 Jun 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/06/13/latex-and-git/</guid>
      <description>&lt;p&gt;At the request of ihrhove I’ve decided to talk a little bit about using git and LaTeX together. I currently have two private git repositories; one for the Finnish paper and the other for all of my thesis work. I’ve talked previously about the Finnish paper so I’ll give a brief overview of how I use it with my thesis but you’ll need to keep in mind that I don’t have it shared with anyone because my supervisors don’t use git and nor do they edit the documents I work on directly (two print out draft papers and write on them, the third (who has used CVS/SVN in the past) uses Foxit to annotate PDFs directly and send them back to me. To start (and possibly end, if you’re easily convinced) with, LaTeX is just code. So to me there’s no reason why you can’t use any service you’d normally use for code for LaTeX. Everything that is directly being used in a paper comes under my version control with git. Each paper in my thesis repository has its own folder. Within that folder there is a LaTeX subfolder, where I keep everything needed for the writing of the paper, and an R or MATLAB folder depending on what program I’m using to do the modelling (and all the code goes into the repository). Within the LaTeX folder I have a whole bunch of .tex files and a folder where I store the images to be included in the paper. One of my favourite commands in LaTeX is . Every section in a paper has its own LaTeX source file. I find that this helps me navigate my work when I’m writing, especially when making corrections. Each file gets worked on separately and I save frequently. If I’m finished dealing with a section or I’m heading off for a break I will save everything and commit the current changes with a note about which section I’ve been focussing on. I picked this based writing up in my Honours degree when I got sick of having screen after screen of text. If I want to omit a section in a draft I can just comment out the line. Reorganising sections and maybe even subsections, becomes an issue of swapping two or three lines of LaTeX rather than copying and pasting giant blocks of text. I’m a sucker for vector graphics so I will use PDF graphs and pdflatex wherever I can. Occasionally I succumb to using PGF/TikZ for a while but usually have to generate so many different styles of plots that I don’t bother. So anyway, PDF graphics. These are really quite small and can be stored in git no trouble at all. I know git’s more or less useless for version control and revision of binary files (but PDF and EPS files are quite different) but I find it useful to be able to overwrite my graphs and still have the older versions available through reverting to a previous commit rather than making endless folders called “oldgraphics”. The root of my thesis repository has a folder called “Bibliography” which is where a monolithic bibtex file called “allpapers.bib” is stored. Because I will cite the same references across multiple papers I find the idea of having separate bibliography databases a bit silly. I use JabRef to edit this, by the way. All my \bibliography commands point to ../../Bibliography/allpapers.bib. I’ve even got a template for papers with that line in it so that I don’t even have to think about how I do my referencing. With regards to the Finnish paper, this compartmentalisation reduces, even further, the risk of conflicts. Committing changes to one section at a time means the commit messages are often quite descriptive without having to be quite long. The mixture of a few lines of changes and a brief summary means it’s easy to see what’s happened in the changelog. I also use git to keep track of side projects that have popped up during my thesis. Coworkers will often come to me with a question about some data analysis or if I can write a script to make a certain repetitive task as automatic as possible. Each coworker gets a subfolder within a /Side Projects/ folder and within those there are folders for each little project. If I worked in a group where use of git was widespread I would consider making a separate project for each person and inviting them as a collaborator. I kind of wish that QUT had a git server (the school of IT had a subversion server but I really dislike SVN after discovering git) and that scientists were encouraged to use R/MATLAB/SAS for their statistics and modelling instead of Excel. I think it’d a great way to foster collaboration and have people be able to work on a project and make changes, share their code with their coworkers, etc. without sending code and draft papers around via email. Actually a private git server without the account level limitations that github imposes would be an invaluable tool, especially if you could just open up your repositories to the QUT community to show what you’re doing and provide colleagues with usable code for statistical analysis, image manipulation tools, etc. And if someone within the university came across your work and liked it, you would potentially have another paper to work on within the uni.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ad hoc collaboration</title>
      <link>/./2012/06/05/ad-hoc-collaboration/</link>
      <pubDate>Tue, 05 Jun 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/06/05/ad-hoc-collaboration/</guid>
      <description>&lt;p&gt;Rbloggers have &lt;a href=&#34;http://www.r-bloggers.com/announcing-rpubs-a-new-web-publishing-service-for-r/&#34;&gt;announced the launch&lt;/a&gt; of &lt;a href=&#34;http://www.rpubs.com/&#34;&gt;RPubs&lt;/a&gt;, a free service which makes it easy to publish code and analysis on the web. It’s based on RStudio and the markdown package and looks like a great way for people to show analysis to co-workers who might not have R on their computers when you don’t feel like writing a report. I really like this idea and might end up using it in my office to show what we can do with statistics. Another thing I’ve been thinking about is the potential to use an Apple TV and its &lt;a href=&#34;http://www.apple.com/au/appletv/airplay/&#34;&gt;screen sharing&lt;/a&gt; capabilities to do presentation work from iPhones, iPads and Mac computers. A lot of people in my office have iPhones, so an Apple TV hooked up to a HDMI screen (surely universities just leave these lying around) might be a good way to get a group of people to take some notes or share prepared slides with a small room of people. For example, if people had a PDF version of slides on their iPhones they could take control of the Apple TV and use their iPhone to flick through the slides, allowing everyone to stay in their seats and control the slideshow from their own device. I was excited by Google Wave when it first launched, as it combined a lot of what I liked about Gmail, Google Docs and Google Chat with an extension system, making it an incredibly powerful and flexible platform for collaborative work. Unfortunately it was released prematurely and died off after a flurry of uptake. Google Plus doesn’t really make up for it, either. I really liked the idea of collaboratively writing a document and being able to add in a voting gadget to resolve whether a section should be included. I used it socially to determine the dates of picnics with friends, which was probably where most of my use was directed. Probably the best example of how useful I found it was in writing a manual of procedures with about ten other volunteers who would ask questions. As we answered the questions, we folded the answers into the main part of the document. This was much more useful than writing a static document and then having a separate email list for discussion, or using track changes in Microsoft Word to email around a huge document that kept on growing. I have high hopes for the internet in terms of &lt;em&gt;ad hoc&lt;/em&gt; collaboration, particularly academic collaboration. I find &lt;a href=&#34;http://github.com/&#34;&gt;GitHub&lt;/a&gt; really exciting because it allows me to work on a private project and then add a collaborator when they come on board. Once a project is finished and the paper published, that private repository can be made public and anyone can fork it and do with it what they will. If they’re intrigued by what’s been done, they could contact me and discuss what they’ve done and we can build a new project based on their fork of my work. With so much of my work being based on R or MATLAB and written up in LaTeX, I find this potential way of working quite sensible. Add in the fact that GitHub gives you a wiki system for each project and you’ve got a great tool at your disposal. A somewhat &lt;em&gt;ad hoc&lt;/em&gt; collaborative tool that I organised is the &lt;a href=&#34;https://wiki.qut.edu.au/display/npbayes/Home&#34;&gt;wiki for QUT’s Bayesian Non-parametrics reading group&lt;/a&gt;. This is a repository for the collective work of the group, including Q&amp;amp;A on the papers we’re reading, notes from the meetings, a list of papers read, code chunks, links to videos explaining what we’re working on, etc. It’s been a really useful tool and I’d hope that others interested in the same work could use it as a resource for their own learning. There’s a lot of really cool stuff out there. It’s a matter of finding useful tools that don’t have particularly high barriers to entry and allow non-experts to view expertly produced material (like on RPubs). The longer it’s been since one was a student, the less likely one seems to be to adopt new workflow practices. I’ve suggested git to my supervisors as a good way for our groups to work but I have a feeling that none of them are interested enough in distributed version control to put the effort in to learning how to use them. So for now it’s annotated PDFs or printed pages with scribbles on them rather than making the edits to a LaTeX source file and committing and pushing their changes to a shared repository.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A few thoughts on the various software packages for stats</title>
      <link>/./2012/05/29/a-few-thoughts-on-the-various-software-packages-for-stats/</link>
      <pubDate>Tue, 29 May 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/05/29/a-few-thoughts-on-the-various-software-packages-for-stats/</guid>
      <description>&lt;p&gt;I had a drink with a friend who works in health statistics. She uses SAS at work and asked me what kind of software I use to do my statistics. R and MATLAB, I responded. MATLAB because it’s fast and good and R because it’s free and has &lt;a href=&#34;http://cran.r-project.org/web/packages/available_packages_by_name.html&#34;&gt;heaps of additional packages&lt;/a&gt; to extend its use. A few days later she asked me if I’d seen &lt;a href=&#34;http://www.r-bloggers.com/will-2015-be-the-beginning-of-the-end-for-sas-and-spss/&#34;&gt;an article on R-bloggers&lt;/a&gt; predicting the end of the use of SAS in academic circles, with R overtaking SAS some time in 2015/16. Even discounting the R package system, the fact remains that R is far less cheaper than SAS or SPSS. As the GFC continues to bite hard and governments, universities and other large institutions look to shed unnecessary costs, perhaps R’s price ($0) will lead to its adoption. Institutional licenses for SAS and SPSS (and let’s throw MINITAB in there, as it’s also used) can’t be cheap and cutting out expensive software when a mature, free statistics environment (R+RStudio) is available would be a very simple way to reduce some ongoing costs. Support is available through companies like &lt;a href=&#34;http://www.revolutionanalytics.com&#34;&gt;Revolution Analytics&lt;/a&gt; if the argument is that SAS support the software they sell. Yes, I’m a bit of an R evangelist, particularly in my research group where people don’t use SAS, Stata, SPSS, MINITAB or MATLAB but instead use Microsoft Excel (one of the worst pieces of software for statistical analysis). I would love to see R displace SAS, SPSS and other proprietary software packages in the next few years, but there’s another parallel objective; the quest to stop scientists using Excel for data analysis and modelling. It’s slow and based on an accounting spreadsheet. If we can get people off Excel and on to R (rather than SAS or SPSS, which would be attractive choices because they’ve heard of them somewhere and because they cost a lot of money they must be good) then academia as a whole will benefit immensely.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bioaerosols and lab reorganisation</title>
      <link>/./2012/05/22/bioaerosols-and-lab-reorganisation/</link>
      <pubDate>Tue, 22 May 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/05/22/bioaerosols-and-lab-reorganisation/</guid>
      <description>&lt;p&gt;Yesterday I had a meeting with my supervisor and two colleagues about looking at the microbiological measurements we’ve been taking as part of UPTECH. It was an interesting meeting if only for the chance to really see how the four of us are approaching this paper from the point of view of our backgrounds. Heidi Salonen, who has been working on microbiology with us since Caroline Duchaine went back to Canada, will be the lead author on the paper. She’s got a background in OH&amp;amp;S and this was really clear when she was talking about the need to quantify safe levels of microbiology. I was sitting there asking the questions that will shape the data analysis, like what sort of relationships we anticipate. Mandana Mazaheri was focussing on making use of the rest of the UPTECH data as best we can (we have a huge Access database) and Lidia Morawska was making sure none of us got carried away and that we could agree on an approach. I’m going to use this paper as an opportunity to push R as our statistical engine rather than Excel (which is all that scientists seem to use). I’m being brought on as the statistician and really don’t want to about learning how to use Excel and write VBA to do the things that can be done trivially in R. R’s plots are a lot better than what we tend to see in Excel and Mandana and I have been learning how polygon() works so we can shade the region between two particle diameter distributions in a plot for a report she’s working on. Mandana’s actually been quite good about learning R and seems to have stuck with it more than a lot of the other PhD students (except for Farhad Salimi, who is doing some really interesting work). I’d really like to use this paper as an opportunity to have people use git but Mandana and Heidi aren’t programmers, and I think even using R is going to stretch them. A &lt;a href=&#34;windows.github.com&#34;&gt;Windows GUI for git&lt;/a&gt; was released recently and I’m finding it better for committing and pushing than TortoiseGit or the command line git client as neither of these deal with my ssh keys particularly well. I don’t know what the deal is there. Perhaps it’s better under a more modern version of Windows. I also spent some time in the lab today helping reorganise it. Me being in the lab is a rarer event than the &lt;a href=&#34;http://www.bbc.co.uk/news/18141625&#34;&gt;recent annular eclipse&lt;/a&gt;. I don’t work in the lab and so am not responsible for the mess left over from experiments and not putting things away when I’m done, but I do rely on the lab’s monitoring data so it’s only fair that I help out from time to time. Just don’t ask me to help out with any of the science.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;a href=&#34;http://knowyourmeme.com/photos/234739-i-have-no-idea-what-im-doing&#34;&gt;&lt;img src=&#34;fa5.jpg&#34; /&gt;&lt;/a&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linux</title>
      <link>/./2012/05/16/linux/</link>
      <pubDate>Wed, 16 May 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/05/16/linux/</guid>
      <description>&lt;p&gt;I swear, when I finish this PhD and start my new position I’ll either be asking for a Mac or installing Linux on my current computer. A lot of my work at the moment revolves around remotely connecting to the university’s supercomputer and using git to do version control for documents stored locally. Both of these require decent ssh and X forwarding and I am sick to death of the way Windows XP deals with both of these .The faculty was going to upgrade us all to Windows 7 but then it got merged with another faculty and I guess it slipped down the list of priorities. I’ve used Mac OS X and/or Linux as my home operating system for quite a while now (must be at least ten years) and while I’m generally okay to use Windows and there are some nice implementations of the programs I need to use (LaTeX, MATLAB, RStudio), the fact is that these programs are available on other operating systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Collected wisdom of the R community</title>
      <link>/./2012/05/11/collected-wisdom-of-the-r-community/</link>
      <pubDate>Fri, 11 May 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/05/11/collected-wisdom-of-the-r-community/</guid>
      <description>&lt;p&gt;Just for a bit of fun, I thought I might share with you the &lt;a href=&#34;http://cran.r-project.org/web/packages/fortunes/vignettes/fortunes.pdf&#34;&gt;&lt;em&gt;fortunes&lt;/em&gt;&lt;/a&gt; package from CRAN, containing some of the collected wisdom of the R community. My favourite, as someone who’s attempted to use &lt;a href=&#34;http://cran.r-project.org/web/packages/BRugs/index.html&#34;&gt;&lt;em&gt;BRugs&lt;/em&gt;&lt;/a&gt; to fit Bayesian models in R using WinBUGS, is number 13 on the list.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Andrew Thomas: . . . and if something goes wrong here it is probably not WinBUGS since that has been running for more than 10 years. . . Peter Green (from the back): . . . and it still hasn’t converged!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Andrew Thomas and Peter Green (during the talk about ‘BRugs’) gR 2003, Aalborg (September 2003)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Don&#39;t forget to clean up</title>
      <link>/./2012/04/07/dont-forget-to-clean-up/</link>
      <pubDate>Sat, 07 Apr 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/04/07/dont-forget-to-clean-up/</guid>
      <description>&lt;p&gt;I just noticed I’m using 11GB of storage space on QUT’s HPC server. Almost all of that will be results from remotely running R-INLA over the last few weeks while writing my second paper. The INLA objects are usually tens of megabytes but sometimes hundreds depending on the model I fit (with ~50,000 rows of data) and I think I’d be pushing my relationship with the HPC team if I didn’t clean this up every so often. Thanks heaps to Håvard Rue and Dan Simpson for their help with getting me sorted out with remote R-INLA. Edit: if you’re at QUT and want to run R-INLA remotely, &lt;a href=&#34;https://docs.google.com/document/d/1gcZ-6mfFh0R1WuprSM3PWyRDjUZ7z048RS_bFXnFXU8/edit&#34;&gt;here are the instructions&lt;/a&gt; that have been worked out by me, Dan, Håvard and QUT HPC’s Ashley Wright.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using R</title>
      <link>/./2012/03/12/using-r/</link>
      <pubDate>Mon, 12 Mar 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/03/12/using-r/</guid>
      <description>&lt;p&gt;I am the only statistician in my aerosol science research group; everyone else is either a physicist, chemist, environmental scientist, engineer or some other physical scientist. Most of the people in the group do their data analysis and/or plotting in Excel or custom software like IGOR or Origin. One or two people in the group have used MATLAB in the past but there’s some sort of opportunity cost effect where it’s easier to use the Data Analysis plug-in in Excel than to load up MATLAB and remember how to fit a GLM or perform ANOVA. My background in computational mathematics and a bit of statistics means I’ve been exposed to other mathematics and statistics software and I’ve grown accustomed, through my stats research group, to seeing novel statistical methods and data analysis done in appropriate software packages (R, MATLAB, WinBUGS, and a Python package called &lt;a href=&#34;http://www.ccdamc.org.au/files/beachbayes/ChrisStrickland.pdf&#34;&gt;pyMCMC&lt;/a&gt; that some of &lt;a href=&#34;http://eprints.qut.edu.au/43469/&#34;&gt;my colleagues&lt;/a&gt; are working on). As such, it saddens me to see my fellow PhD students doing their stats and plotting in Excel (which has certainly come in leaps and bounds but is still not a good piece of statistical software). I’ve really been pushing &lt;a href=&#34;http://cran.r-project.org/&#34;&gt;R&lt;/a&gt; as a way of doing better data analysis and making better looking graphs (which can easily be exported as PDF or EPS files) over the last year or so and have been heartened to see that a) it’s recognised as a good tool and b) some people are using the scripts I have written for them to do things like randomly sample from a list of strings. One of the other PhD students has found an R package that will make our lives much easier, &lt;a href=&#34;http://www.openair-project.org/&#34;&gt;openair&lt;/a&gt;, and has even started investigating how to use R to do repeated calculations of summary statistics from SMPS data rather than writing the formula in a spreadsheet cell in Excel and choosing “fill down”. The risk here, of course, is that we will be asked to write all the statistics code for everyone else. In addition to this, one of the PhD students in the stats research group decided to run a short course in using R (with RStudio, which I’m now attempting to use instead of Notepad++ and NppToR) to manipulate data and do some simple plotting in ggplot2 and most of the (eight) students in my room went to the course. They all got various things out of going but all agreed that using R is the way to go. Some still find more advanced data analysis than t-tests, ANOVA and linear regression a bit daunting but will soldier on with the short courses that are being organised. There’s a bit of a feeling among the aerosol science PhD students that I should teach them data analysis in R, as I’m familiar with both what they’re working on (and thus have some idea of what they need to learn) and with them as people so I’ve got an idea of the sort of level of statistics and mathematics they’ve got. Now’s probably not the time for me to start teaching (PhD to finish) but it looks like I might be around for a little while so if I can help build the capacity of the group then I’d be more than happy to show everyone how to do their favourite data analysis in R and perhaps extend them a little further as scientists. While everyone brings their own strengths to the group in terms of skill sets and bodies of knowledge, I believe that every scientist should be able to do their own data analysis and that they should be able to do it in statistical software rather than something which has been developed for generating accounting reports and has later had scripting and statistical routines shoe-horned on to it (and while widely available isn’t free and “free”).&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
