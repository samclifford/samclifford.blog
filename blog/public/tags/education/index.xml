<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Education on Sam Clifford </title>
    <link>/./tags/education/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2016-03-10 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>Diagnostics for first year students</title>
      <link>/./2016/03/10/diagnostics-for-first-year-students/</link>
      <pubDate>Thu, 10 Mar 2016 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2016/03/10/diagnostics-for-first-year-students/</guid>
      <description>&lt;p&gt;The SEB113 teaching team last semester (me, Ruth Luscombe, Iwona Czaplinski, Brett Fyfield) wrote a paper for the HERDSA conference about the relationship between student engagement and success. We collected data on the timing of students’ use of the adaptive release tool we developed, where students confirm that they’ve seen some preparatory material before being given access to the lecture, computer lab and workshop material. We built a regression model that looked at the relationship between the number of weeks of material students gave themselves access to and their end of semester marks (out of 100%), and it showed that students who engaged more obtained better marks, where engagement also included active use of the Facebook group and attendance at workshop classes. I had assumed that we’d be able to get data on students’ maths backgrounds coming in, but with so many ways to enter university, we don’t have the background info on every student. QUT has set Queensland Senior Maths B as the assumed knowledge for SEB113 (and indeed the broader ST01 Bachelor of Science degree) and I’m interested in knowing whether or not the level of maths of students coming in has a bearing on how well they do over the course of the unit. This semester, we decided that it’d be good to not just get a sense of the students’ educational backgrounds but to assess what their level of mathematical and statistical skills are. We designed a diagnostic to run in the first lecture that would canvas students on their educational background, their attitudes towards mathematics and statistics, and how well they could answer a set of questions that a student passing Senior Maths B would be able to complete. The questions were taken from the PhD thesis of Dr Therese Wilson and research published by Dr Helen MacGillivray (both at QUT), so I’m fairly confident we’re asking the right questions. One thing I really liked about Dr MacGillivray’s diagnostic tool, a multiple choice test designed for engineering students, is that each incorrect choice is wrong for a very specific reason, such as not getting the order of operations right, not recognising something as a difference of squares, etc. I’m about to get the scanned and processed results back from the library and it turns out that a number of students didn’t put their name or student number on the answer sheet. Some put their names down but didn’t fill in the circles, so the machine that scans the answer sheet won’t be able to determine who the student is and it’ll take some manual data entry probably on my part to ensure that we can get as many students as possible the results of their diagnostic. So while I’ll have a good sense of the class overall, and how we need to support them, it’ll be harder than it should be to ensure that the people who need the help are able to be targetted for such help. Next semester I’ll try to run the same sort of thing, perhaps with a few modifications. We’ll need to be very clear about entering student numbers and names so that we can get everyone their own results. It’d be good to write a paper that follows on from our HERDSA paper and includes more information about educational background. It might also be interesting to check the relationship between students’ strength in particular topics (e.g. calculus, probability) and their marks on the corresponding items of assessment. Getting it right next semester and running it again in Semester 1 2017 would be a very useful way of gauging whether students who are weak in particular topics struggle to do well on certain pieces of assessment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ALP wants to teach kids how to program, and I agree</title>
      <link>/./2015/05/28/alp-wants-to-teach-kids-how-to-program-and-i-agree/</link>
      <pubDate>Thu, 28 May 2015 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2015/05/28/alp-wants-to-teach-kids-how-to-program-and-i-agree/</guid>
      <description>&lt;p&gt;I checked in on one of my workshop classes this morning to see how everyone was going in the final week, to remind them of the remaining help sessions and to check that they’re on track to complete their group assignments. There weren’t many students in the class, what with it being week 13, but of one of the students was very proud of the fact that she’d lifted her marks on the problem solving tasks from 1/10 to 8/10 over the course of the semester. She told me that going back over the last few workshops helped reinforce the coding that she needed to be able to do in order to complete the assessment. She plans on transferring into medicine, which is typically not a career that requires programming. At the end of the semester, with only one piece of assessment remaining and the decision made that she will change out of science, she is still putting a lot of effort into understanding the statistics and learning how to program is reinforcing this and allowing her to engage deeper than if we were restricted to the stats education I had in first year ten years ago where we spent a lot of time looking up the tails of distributions in a book of tables. Maths and statistics education (for students not studying maths/stats as a major) is no longer just about teaching students how to do long division in high school and calculus and point and click statistics methods at university. While some degree such as Electrical Engineering, Computer Science and IT have traditionally been associated with some amount of programming, it’s becoming more and more common for maths and stats service units to include MATLAB or R as a means of engaging deeper with the mathematical content and understanding solutions to linear systems and differential equations or performing data analysis and visualisation. Learning to program leads to better understanding of what you’re actually doing with the code. Computers are everywhere in our students’ lives and in their educational experiences. Due to their ubiquity, the relationship students have with computers is very different to what it was 10 years ago. Computers are great at enabling access to knowledge through library databases, Wikipedia and a bunch of other online repositories. But it’s not enough to be able to look up the answer, one also has to be able to calculate an answer when it hasn’t been determined by someone else. There is not yet a mathematics or statistics package that does all of the data analysis and all of the mathematical analysis that we might want to do in a classroom with a point and click, drag and drop interface. To this end, I teach my students how to use R to solve a problem. Computers can do nearly anything, but we have to be able to tell the computer how to do it. Learning simple coding skills in school prepares students to tackle more advanced coding in quantitative units in their university studies but it also teaches an understanding of how processes work based on inputs and outputs, and not just computational processes, it’s all about a literacy of processes and functions (inputs and outputs). Learning to code isn’t just about writing code as a profession no more than teaching students to read is done to prepare them in their profession of priest or newsreader. Coding provides another set of skills that are relevant to the future of learning and participation in society and the workforce, just as learning mathematics allows people to understand things like bank loans. Tony Abbott does not sound like he’s on board with the idea of giving kids the skills to get along in a world in which computers are part of our classroom the way books were when he was going through school. While reading, writing and basic mathematics skills will continue to be important skills, literacy is more than just reading comprehension. Information literacy, being able to handle data, and being able to reason out a process are even more important thanks to the changing technologies we are experiencing. Not every student is going to be a professional programmer, an app developer or big data analyst, but coding will be a skill which becomes more and more necessary as computers become more and more a part of our workplace not just as fancy typewriters or an instantaneous postal system but as a problem solving tool.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>That feeling when former students contact you</title>
      <link>/./2014/09/02/that-feeling-when-former-students-contact-you/</link>
      <pubDate>Tue, 02 Sep 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/09/02/that-feeling-when-former-students-contact-you/</guid>
      <description>&lt;p&gt;Last year I had a student in SEB113 who came in to the subject with a distaste for mathematics and statistics; they struggled with both the statistical concepts and the use of R throughout the semester and looked as though they would rather be anywhere else during the collaborative workshops. This student made it to every lecture and workshop though and came to enjoy the work of using R for statistical analysis of data; and earned a 7 in the unit.&lt;/p&gt;
&lt;p&gt;I just got an email from them asking for a reference for their VRES (Vacation Research Experience Scheme) project application. Not only am I proud of this student for working their butt off to get a 7 in a subject they disliked but came to find interesting, but I am over the moon to hear that they are interested in undertaking scientific field research. This student mentions how my “passion for teaching completely transformed my (their) view of statistics”, and their passion for the research topic is reflected in the email.&lt;/p&gt;
&lt;p&gt;This sort of stuff is probably the most rewarding aspect of lecturing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2014/07/02/posterior-samples/</link>
      <pubDate>Wed, 02 Jul 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/07/02/posterior-samples/</guid>
      <description>&lt;p&gt;ARC Discovery Projects have been returned to their authors, and we are putting our responses together for the rejoinders. Interesting to see that we got a comment suggesting that we use the less restrictive CC-by instead of CC-by-nc-sa as we’d suggested. We weren’t successful in our Linkage Project applications, which is disappointing as they were interesting projects (well, we thought so). Continuing to bring research funding in is an ongoing struggle for all research groups and I feel it’s only going to get harder as the new federal government’s research priorities appear to be more aligned to medical science that delivers treatments than to our group’s traditional strengths. SEB113 is pretty much completely over for the semester, with marks having been entered for almost every student. Overall I think the students did fairly well. We had some issues with the timetable this semester. Ideally, we’d like the Lecture, then all of the computer labs, then all of the workshops, so that we can introduce a statistical idea, show the code and then apply the idea and code in a group setting. Next semester, we have the lecture followed immediately by the workshops with the computer labs dotted throughout the remainder of the week. This has provided us with an opportunity to try some semi-flipped classroom ideas, where students are able/expected to do the computer lab at home at their own pace rather than watch a tutor explain it one line at a time at the front of a computer lab. I’m teaching part of a &lt;a href=&#34;http://www.eventbrite.com.au/e/r-statistical-language-for-air-pollution-epidemiology-tickets-12043581677&#34;&gt;two day course&lt;/a&gt; on the use of R in air pollution epidemiology. My part will introduce Bayesian statistics with a brief overview, a discussion about prior distributions as a means of encoding &lt;em&gt;a priori&lt;/em&gt; beliefs about model parameters, and discuss the use of Bayesian hierarchical modelling (as opposed to more traditional ANOVA techniques) as a way of making the most of the data that’s been collected. The other two presenters are &lt;a href=&#34;http://researchers.uq.edu.au/researcher/2181&#34;&gt;Dr Peter Baker&lt;/a&gt; and &lt;a href=&#34;http://researchers.uq.edu.au/researcher/2530&#34;&gt;Dr Yuming Guo&lt;/a&gt;. The course is being run by the CAR-CRE, who partially fund my postdoctoral fellowship. I had meant to post this back when they were doing the rounds, but there’s &lt;a href=&#34;http://www.tylervigen.com/&#34;&gt;a bunch of plots&lt;/a&gt; that attempt to show that correlation isn’t causation and that spurious correlations exist in large data sets. &lt;a href=&#34;https://tom-christie.github.io/articles/correlation/&#34;&gt;Tom Christie has responded&lt;/a&gt; to this by going over the fact that correlation in time series isn’t as simple as in the case of independent, identically distributed data. One should be careful that one’s criticism of bad statistics is itself founded on good statistics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2014/05/22/posterior-samples/</link>
      <pubDate>Thu, 22 May 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/05/22/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.compoundchem.com/2014/04/02/a-rough-guide-to-spotting-bad-science/&#34;&gt;A rough guide to spotting bad science&lt;/a&gt;. &lt;a href=&#34;http://simplystatistics.org/2014/05/07/why-big-data-is-in-trouble-they-forgot-about-applied-statistics/&#34;&gt;Why big data is in trouble: they forgot about applied statistics&lt;/a&gt;. Big data analytics are all well and good but you have to keep in mind that there are statistical properties that govern which inferences are valid. While I’m comfortable giving a lecture I really struggled to get through them in undergrad. &lt;a href=&#34;http://news.sciencemag.org/education/2014/05/lectures-arent-just-boring-theyre-ineffective-too-study-finds&#34;&gt;It turns out they may not be the most effective way to get information to students.&lt;/a&gt; My supervisors, Professor Lidia Morawska, is giving a public talk (free to register) at QUT soon, &lt;a href=&#34;https://www.qut.edu.au/institute-for-future-environments/about/news-events/news?news-id=71015&#34;&gt;“Air Quality Reports On Our Mobiles - Do We Care?”&lt;/a&gt; June 6 2014&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The ongoing crusade against Excel-based analysis</title>
      <link>/./2014/05/14/the-ongoing-crusade-against-excel-based-analysis/</link>
      <pubDate>Wed, 14 May 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/05/14/the-ongoing-crusade-against-excel-based-analysis/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;One of the things I catch myself saying quite often in SEB113 is “This is new. It’s hard. But remember, you weren’t born knowing how to walk. You learned it”, as my way of saying that it’s okay to not understand this straight away, it takes time, practice and determination. I often say this in response to students complaining about learning R to do their data analysis. It’s actually got to the point where t&lt;/span&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;he unit co-ordinator suggested I get a t-shirt printed with “You weren’t born knowing how to walk” on the front and “So learn R” on the back.&lt;/span&gt; One of the reasons I’m so keen to push new students into learning R is that while Excel can do some of the simpler calculations required in the first year of a science degree it is often completely inadequate for doing data analysis as a professional scientist, or even in an advanced level university course. I actually saw a senior researcher in a 3 day Bayesian statistics course try to avoid using R to code a Gibbs sampler by getting it up and running in Excel. They managed it, but it took minutes to run what the rest of us could compute in a second (and it was for a trivially simple problem). There are &lt;a href=&#34;http://www.asq904.org/StatisticalFlawsInExcel.pdf&#34;&gt;problems with Excel&lt;/a&gt;, such as its inability to deal with the standard deviation of a group of very large numbers due to its bizarre formulation. Apparently the secret to sane use of Excel is to &lt;a href=&#34;http://www.r-bloggers.com/excel-fanaticism-and-r/&#34;&gt;only use it for data storage&lt;/a&gt;. This guiding principle has meant that I no longer manipulate my data in Excel. Even with time stamp information I’ll fire up the &lt;a href=&#34;http://cran.r-project.org/web/packages/lubridate/index.html&#34;&gt;lubridate&lt;/a&gt; package to convert from one format to another. I’m slowly exploring the &lt;a href=&#34;http://blog.datascienceretreat.com/&#34;&gt;Hadleyverse&lt;/a&gt; and that sort of approach is filtering through into SEB113 where we’re teaching the use of ggplot2 and reshape2 within RStudio. These are all powerful tools that simplify data analysis and avoid the hackish feel that much Excel-based analysis has, where pivot tables are a thing and graphs are made by clicking and dragging a selection tool down the data (which can lead to &lt;a href=&#34;http://en.wikipedia.org/wiki/Growth_in_a_Time_of_Debt&#34;&gt;some nasty errors&lt;/a&gt;). The fact that these powerful tools that make data analysis simple are free is another reason to choose R over Excel. I’m not on the “Open Source Software and provision of all code is mandatory” bandwagon as others seem to be when it comes to analysis being replicable. I agree it’s a worthwhile goal but it’s not a priority for me. That said, though, I definitely support encouraging the use of free software (in both senses) in education on the grounds of equity of access. I had a chat with some students in SEB113 yesterday about why we’re teaching everything in R given that the SEB114 staff use a combination of Excel, MATLAB (and maybe even other packages I don’t know about). If we were to teach analysis the way that the SEB114 lecturers do it themselves, we’d have to teach multiple packages to multiple disciplines. Even discounting the fact that everything we teach is implemented in R, that R is free (unlike Excel and MATLAB), cross-platform (Excel on Linux? Try OpenOffice/OfficeLibre) and extensible (MATLAB has toolboxes, Excel has add-ins, R has a nice package manager) was a big plus for students who said that being able to work on assignments at home was valuable and so paying for software would make study difficult. Convincing students to use R can be difficult, especially if they have no programming background, but ultimately they seem to accept that R is powerful, can do more than Excel and that writing reusable code makes future analysis easier. Convincing SEB114 academics that teaching their students to use R is a good idea is probably a harder sell, given that they’ve got years of experience with other tools. It’s still only semester 3 of the new Bachelor of Science course so we’ll have to see how this plays out over the years to come.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Timetabling and the potential for alternative delivery in SEB113</title>
      <link>/./2014/04/07/timetabling-and-the-potential-for-alternative-delivery-in-seb113/</link>
      <pubDate>Mon, 07 Apr 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/04/07/timetabling-and-the-potential-for-alternative-delivery-in-seb113/</guid>
      <description>&lt;p&gt;I’ve been pretty busy writing the analysis plan for the main paper from the UPTECH project and reorganising SEB113 workshops. We’ve had some meetings recently with QUT timetabling people which has led to discussions about how we try to get students to enrol in a sensible pair of workshops and labs for both SEB113 and SEB114. One of the biggest concerns when it comes to these paired subjects is making sure that people attend the labs and workshops in the right order and are working with the same groups across both subjects so that we can structure the teaching material. In SEB113 the preferred order of classes is Lectorial, Computer Lab, Collaborative Workshop. The lecture introduces the topic, the lab shows you how it’s implemented in R and the workshop gets you working in a group with others to solve a problem based on the topic. The problem comes about with QUT’s timetabling software providing a timetable which contains no clashes for the core first year subjects (SEBs 101, 102, 113, 114). Timetabling the lectures/lectorials for these units so that they don’t clash is a task in and of itself and I’m impressed that the timetabling people have managed to make sure these subjects don’t clash (I remember taking two units for the applied physics co-major in the old B App Sc course where the lectures clashed). The non-clashing timetable doesn’t necessarily mean students can enrol in the class order that we would prefer. It’s also unlikely that we can automatically combine a lab-workshop pair as one thing to be enrolled in and it’s impractical to try to get a staff member to enrol students manually. It’s got me thinking a lot about flipped classrooms and other ways of overcoming the timetable difficulty. The benefit of the workshop for students is that they have a group to work with on a big task and they have two tutors to ask for help when they get stuck. I feel like this would be difficult to do outside a classroom without some sort of help-desk queueing system that is only open between certain times (and then you’ve still got the time restrictions). The computer labs can be done individually at any time, though, as they’re about exposure to code rather than solving a particular problem. In this instance, we could probably cut down on the number of computer labs required by encouraging students to do the lab in their own time before their workshop, which is in the spirit of flipped classrooms. The last labs are in week 7 (this week!) which means it’s not going to be an issue much longer this semester. Semester 2 has fewer SEB113 enrolments (SEB114 isn’t offered) so it’s not going to be as big an issue then. Whether we go with changing the timetabling system or we modify computer labs to become programming consults (where to get help you must have attempted the lab) is something we can deal with a bit later. With the use of Echo360 being made mandatory in all lectures at QUT the availability of recorded lectures makes it easier for students to go through the material at their own pace. With so many students in the subject, there’s a large number of person hours which go into content delivery. I’m not sure we’re using that resource (labour) as effectively as we can, and changing the way we deliver the subject may help that.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/12/17/posterior-samples/</link>
      <pubDate>Tue, 17 Dec 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/12/17/posterior-samples/</guid>
      <description>&lt;p&gt;Today’s collection of links have the loose theme of engagement between the public and science. Grist, an environmental news service I used to read every single day, have an &lt;a href=&#34;http://grist.org/climate-energy/reddits-science-forum-banned-climate-deniers-why-dont-all-newspapers-do-the-same/&#34;&gt;interesting article&lt;/a&gt; about how on reddit the science subreddit has banned out and out climate denial (more accurately: conspiracy theories) from their discussion. The author, a moderator of the subreddit, wonders why mainstream news services don’t just do the same given that the science is in. If you’re unconvinced about the science, or are convinced that it’s right but are unsure about the specifics, &lt;a href=&#34;http://www.npr.org/blogs/13.7/2013/12/16/251437395/global-warming-explained-in-about-a-minute&#34;&gt;NPR has a short article and animation&lt;/a&gt; about the mechanisms of global warming. The authors raise the point that the general public find science a bit daunting, particularly when it’s something as complex as global climate systems. Charlotte Pezaro, a PhD student at the University of Queensland, is &lt;a href=&#34;http://cpezaro.wordpress.com/2013/12/17/making-decisions-in-personally-relevant-contexts-of-science/&#34;&gt;doing research&lt;/a&gt; into how science education plays a role in preparing people to evaluate evidence rather than just preparing them to do science or to memorise scientific facts. Pezaro argues that more than this, a focus of science education should be to develop scientific literacy in the making of decisions in a personally relevant context of science, such as health, nutrition and the use of technologies.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>More on the use of R and ggplot in SEB113</title>
      <link>/./2013/11/20/more-on-the-use-of-r-and-ggplot-in-seb113/</link>
      <pubDate>Wed, 20 Nov 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/11/20/more-on-the-use-of-r-and-ggplot-in-seb113/</guid>
      <description>&lt;p&gt;One of the benefits of waking up stupidly early is that you can organise to have coffee and a chat with a friend before work and still get there on time. A friend who I haven’t seen in a while contacted me recently to ask a few questions about learning R for data analysis and visualisation. While they won’t need to formally learn statistics and visualisation for their work it certainly doesn’t hurt to be able to generate better analysis of data and make more informative and easy to interpret graphs. My friend hasn’t done any statistics since high school Maths B, approximately ten years ago, which makes them similar to many of my SEB113 students. They have done a bit of programming along the way as a hobby, which will of course be a huge help. Having downloaded R and had a crack at a ggplot2 tutorial, they were confident that they &lt;em&gt;could&lt;/em&gt; learn what was going on even though they didn’t really understand what was going on in the tutorial. We sat down with the tutorial and some avocado on toast and worked through what the arguments for each function represented and what the data frame was made of, how ggplot has a grammar of graphics and how we can continue to add elements to the code to change the plot. To an extent, the ability to work through but not explain what some code is doing is typical of an SEB113 student in the first half of the subject (where we provide the code and get them to run it). It’s not until later in the semester, when the computer labs stop, that we expect that they can turn their ideas into code (and they’re welcome to cannibalise the code we provide) to write their quantitative workbooks. I suggested the &lt;a href=&#34;http://samclifford.info/2013/11/11/coursera-courses-on-statistics/&#34; title=&#34;Coursera courses on statistics&#34;&gt;Coursera course that started yesterday&lt;/a&gt; as a way to get a bit more familiar with how R works and get recognition of the completion of the course (which isn’t a recognised qualification but is evidence of being interested enough to pursue it). These days I’m always on the lookout for better ways to introduce SEB113 students to R and ggplot2 and I found the following tutorials (and have passed them on to my friend and the SEB113 teaching team) via Matt Asher’s “&lt;a href=&#34;http://www.statisticsblog.com/2013/11/the-week-in-stats-nov-18th-edition/&#34;&gt;Statistics Blog&lt;/a&gt;” and I have copied and pasted the text directly:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Martin Johnsson wrote a series of five well-written tutorials called &lt;em&gt;A slightly different introduction to R,&lt;/em&gt; with tips for beginner R users. Here are the links to parts &lt;a href=&#34;http://bit.ly/1fzSWRq&#34;&gt;I&lt;/a&gt;, &lt;a href=&#34;http://bit.ly/18pRHiz&#34;&gt;II&lt;/a&gt;, &lt;a href=&#34;http://bit.ly/19if90E&#34;&gt;III&lt;/a&gt;, &lt;a href=&#34;http://bit.ly/183o9eb&#34;&gt;IV&lt;/a&gt; and &lt;a href=&#34;http://bit.ly/1i7x0j0&#34;&gt;V&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I had no idea that the &lt;a href=&#34;http://cran.r-project.org/web/packages/coefplot/index.html&#34;&gt;coefplot&lt;/a&gt; package existed! That’s going to make visualisation of fitted linear models much easier for our students, as we’ve previously had them using geom_segment to manually plot estimates and confidence intervals. This is part of what I love about R, compared to, say, SAS. There’s a huge community of people working out there to add extra functionality to an open source project by building on each others’ work. GGally and coefplot both require ggplot2 and have got a lot of really nice functions that extend the publication quality graphics of ggplot2. The community is quite active and if you can think of a question for R there’s probably an answer out there already.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using ggplot2 in SEB113</title>
      <link>/./2013/09/10/using-ggplot2-in-seb113/</link>
      <pubDate>Tue, 10 Sep 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/09/10/using-ggplot2-in-seb113/</guid>
      <description>&lt;p&gt;One of the big pieces of feedback we got during last semester’s SEB113 class was that the programming was difficult to understand and reproduce. While the subject is not a programming subject, we do use R quite heavily for all of the data analysis. Maths B isn’t a pre-requisite for SEB113 and I’d wager that even fewer of the students entering the ST01 Bachelor of Science program have taken senior IPT/ICT subjects at their high schools than have taken Maths B. This semester we introduced R in the very first lecture and gave it a bit of a context. This means that students are aware from the get-go that they will be learning statistics through data analysis on a computer. The lectorials introduce the concepts and provide code for the resulting plots and analysis, the computer labs show how to do that particular form of analysis in R and then the collaborative workshops reinforce the labs by getting groups to work through the analysis of some problem using the statistical concepts and code that they’ve learned that week. One of the biggest stumbling blocks last semester was the inconsistency in the way visualisation was done in R. We used a combination of base graphics, trellis graphics in the lattice package, heatmaps and dendrograms from other packages and had to turn to yet another package to get colorbars for the heatmaps. Part of the fine-tuning this semester has been employing someone (who also does the labs) to rewrite the graphics in the labs in terms of Hadley Wickham’s &lt;a href=&#34;http://ggplot2.org/&#34;&gt;ggplot2&lt;/a&gt; library. This brings consistency to the graphical aspect of the unit and the plot geometries are named explicitly so that it’s clear what style of plot you’ll be generating. I was quite sceptical of ggplot2 when I first saw it, as the only exposure I had to it was the default options for a scatterplot with points. Sure, that’s pretty boring, but the fact that you can make a faceted grid (or wrap it using facet_wrap instead of facet_grid) means that investigating the use of small multiples is so much easier. Small multiples is a visualisation technique developed by &lt;a href=&#34;http://www.edwardtufte.com/tufte/&#34;&gt;Edward Tufte&lt;/a&gt; to allow the reader to see how the relationship between two variables changes as you also vary one or two other (categorical) covariates. Doing this in lattice required specifying a formula, similar to the way you specify a model in lm, but lattice is so different from the base graphics that you lose consistency. I’m touching up this week’s workshop at the moment and I’m really noticing where the graphics code has been greatly simplified by access to a grammar of graphics for a powerful set of plotting routines. The &lt;a href=&#34;http://cran.r-project.org/web/packages/GGally/index.html&#34;&gt;GGally&lt;/a&gt; pacakge provides things like ggpairs, which does what pairs does in the base graphics but gives you the correlation above the diagonal and the scatterplots below the diagonal. This makes for more informative graphs with the beauty of the ggplot style. As far as I can tell we’re hearing fewer complaints about the programming and the visualisation is happening much quicker in the workshops this semester as ggplot2’s documentation is amazing and it’s often a choice of geometry (and changing one or two options) rather than a choice of library (and changing the entire approach to the coding). The use of ggplot2 has made teaching visualisation much simpler and we’re now getting through the workshops quite quickly because the visualisation is no longer a huge stumbling block.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Becoming a grown up (at least a grown-up academic)</title>
      <link>/./2013/08/30/becoming-a-grown-up-at-least-a-grown-up-academic/</link>
      <pubDate>Fri, 30 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/30/becoming-a-grown-up-at-least-a-grown-up-academic/</guid>
      <description>&lt;p&gt;There are only four months left in my current postdoctoral appointment and I’m discussing plans for next year with my supervisor. There’s still a lot of unfinished UPTECH work that needs to happen, including helping a handful of PhD students with the stats in their papers for their thesis, dealing with the clinical data and putting together some research plans for what to do next. The plan at the moment is to look for some funding both internally and externally to provide a research appointment. I’m also interested in continuing lecturing next year, whether in SEB113 or another mathematics/statistics unit. Most of ILAQH is away as of today or tomorrow, as they travel to Prague for the &lt;a href=&#34;http://eac2013.cz/&#34;&gt;2013 European Aerosol Conference&lt;/a&gt;. The work that I’ve been doing with some colleagues from ILAQH and Italy, on personal sampling, will be featured on a poster. The paper has been submitted to ES&amp;amp;T but hasn’t been accepted yet, so unfortunately I can’t put a preprint up to show off the cool statistics that I had to learn to do the modelling in the paper. As a result of everyone being away, I’ll be one of two academic staff members left here. It’s going to be quiet, with most of the staff and a few PhD students gone. Barring the Finnish paper that I’m still revising, this personal sampling paper has been the paper which has required the most creative and original programming as there have been many different steps along the way. I am particularly proud of this paper and the work that went into it. When I was first brought on board there didn’t appear to be much clarity regarding what we wanted to investigate; we had a lot of personal sampling data but didn’t quite know what to do with it. I think the paper we’ve developed does service to the amount of work that was put into collecting the data and is aligned with what the UPTECH project was set up to do. I’m grateful to all co-authors on the paper (and everyone who was out there in the schools) for the work that they put into bringing this to fruition. I’m still finishing the final corrections for my thesis, due in a few weeks time. After that’s handed in I’ll be taking another step in becoming a grown-up academic: supervising a PhD student. I’ll be the replacement associate supervisor for a student whose original associate supervisor has moved from QUT to another university. QUT requires an internal primary and associate supervisor and I’m the one most familiar with the modelling that this student is doing as part of their thesis. We’ve already set a meeting schedule for the time when his primary supervisor is overseas and have discussed what sort of things I’ll expect to see. It’s a strange responsibility to have for someone who’s only just finishing up their thesis. I wonder how long it will be until I’m the primary supervisor for a student. Two years? Five? Ten? Worrying about funding, writing grant applications, supervising students, lecturing (writing assessment!). It’s a strange place to find oneself.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>More on regression</title>
      <link>/./2013/08/29/more-on-regression/</link>
      <pubDate>Thu, 29 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/29/more-on-regression/</guid>
      <description>&lt;p&gt;This week we moved on to regression with a categorical covariate, which we’ve used in the context of estimating the mean dissolved oxygen across three spatial locations or the mean grain size of different types of rock. I think one of the biggest stumbling blocks to our learning with R is the way that lm() and associated functions deal with factor terms. Personally, I am not a fan of the way it chooses the first alphabetic level as the baseline. I know you can relevel the factor levels to choose another one but I’d be much happier with a sum to zero constraint so that the effects are all departures from the mean. This is the way that R-INLA does it and, to me, it makes a lot more sense. Factor terms (categorical variables) are essentially a random effects mean, especially in a Bayesian setting where everything can be treated as a random effect. That lm() makes us choose a baseline and treats the other effects as differences to that baseline means we end up with a coefficients table which is more difficult to interpret. One option is to omit the intercept term from the model, with a function call like lm(data=my.data, y ∼ factor(x) - 1) but that still doesn’t give you a sense of the overall mean. In any case, the mixture of new regression techniques and hypothesis testing for whether or not some parameter is equal to zero is proving difficult. The difference between the t and standard Normal distributions seems to not be particularly well understood and while I’ve tried to make the link between a 95% confidence interval and hypothesis testing at a 5% level of significance quite explicit, the fact remains that these are both new concepts which are being taught by a relatively inexperienced lecturer to students whose mathematical literacy is generally not at the level of those I’ve tutored in subjects where Maths B was explicitly a pre-requisite rather than assumed knowledge. I managed to explain the heavy tails issue in class with a little bit of pantomime, showing how one might paint the tails of the t and Normal distributions and run out of paint at different values of t (or x) based on how much paint you had to use at values far away from the mean. I think about a quarter of the class was struggling with the diagram of overlapping triangles which was meant to be a “zoomed in” version of the point where the density functions of the Normal and t cross over as they get further away from the mean. The lectorials are recorded so I’ll be interested to see how it translates to a radio play setting. The computer labs are apparently quite dense at the moment, with a lot of fairly new ideas being reinforced in a 50 minute block. We’re quite fortunate to have all the labs before all the workshops this semester, so the lab is basically “here’s the code to do what was shown in the lectorials” and then the workshops are designed to implement the code for some problem and generate a bit of discussion. I think this week was probably one of the hardest, conceptually, because it brings together regression for categorical explanatory variables (a straight line is easier to understand than mutually exclusive sets of points), hypothesis testing, the t distribution and confidence intervals. I have uploaded some of last semester’s slides on the central limit theorem for those who may need them, but I think it’s more a familiarity and practice thing than the material being inherently inaccessible. Next week we’ll be moving on to different families for Generalised Linear Models and the use of the nls() function to fit non-linear models such as asymptotic, compartment and bi-exponential. I’m not such a fan of nls (or even nlme) but we can hardly teach them how to use something like WinBUGS to define their own custom mean functions (because if you struggle with the t distribution you’re going to have kittens trying to deal with using the Beta distribution as a conjugate prior for the Binomial model) or even throw them into using gam() from mgcv. I wouldn’t be averse to teaching Generalised Additive Models in an advanced follow-up unit for this subject. If we did that, we could remove the GLMs from SEB113 (which we’ve only introduced this semester) and spend some time on random effects models. I think such a subject would require a much stronger background in mathematics, so students may need to take MAB120/125 and MAB121/126 before attempting such a unit. Still, food for thought as QUT continues to develop the new Bachelor of Science course.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/08/02/posterior-samples/</link>
      <pubDate>Fri, 02 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/02/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://lindsaybradford.wordpress.com/2013/07/25/the-database-design-goggles-they-do-nothing/&#34;&gt;Database design is important&lt;/a&gt;, especially if someone else has to work with your database. It’s not really something we teach in undergraduate science, perhaps the American model of requiring a certain number of credits from certain fields would help remedy this. Ever wanted a glimpse of &lt;a href=&#34;http://xianblog.wordpress.com/2013/07/22/bayes-notebook/&#34;&gt;Bayes’ notebook&lt;/a&gt;? Laura McInerney’s comments on the benefits of &lt;a href=&#34;http://thesiswhisperer.com/2013/07/31/in-praise-of-the-small-conference/&#34;&gt;small conferences&lt;/a&gt; are similar to my experience with &lt;a href=&#34;http://bayesian.org/node/1657&#34;&gt;8 BNP&lt;/a&gt; in Veracruz, Mexico. As long as you’re within the niche field this sort of conference is a great experience. I felt a little like an outsider at 8 BNP because while I was interested in non-parametrics and was working on smoothing, a lot of people were working on things that I had no experience with which are actually the central elements of the field. I got to learn about a lot of neat things, hear some great talks and meet lots of amazing people, but I don’t think I was steeped in NP Bayes enough to really get the most out of the conference. My research went a bit away from NP Bayes these last few years so I didn’t get to put anything together for 9 BNP in Amsterdam. Perhaps ISBA 2014 in Cancún, Mexico will provide a bit more of a chance to get back to that work. We’re teaching R in SEB113. Perhaps any students reading this might be interested in these &lt;a href=&#34;http://www.computerworld.com/s/article/9239799/60_R_resources_to_improve_your_data_skills&#34;&gt;60 R resources&lt;/a&gt;. I use multiple monitors at work but really enjoyed the virtual monitors setup in Gnome when I ran Ubuntu. &lt;a href=&#34;http://lifehacker.com/5616859/is-the-multiple+monitor-productivity-boost-a-myth&#34;&gt;It turns out&lt;/a&gt; that having a large canvas of pixels, rather than multiple monitors, is the key to workplace productivity. My work setup has two widescreen monitors side by side in portrait orientation. This doesn’t work particularly well with programs that assume you’re using a single landscape monitor (such as RStudio) or give you a single window with multiple documents inside that each have focus one at a time (Microsoft Office, why can’t I have a spreadsheet on each monitor?) but it means I don’t have to keep switching back and forth between TeXStudio and RStudio when I’m writing up my analysis. &lt;a href=&#34;http://chronicle.com/article/Introduction-to-Ancient/140475/&#34;&gt;Flipped classes&lt;/a&gt; are an interesting model for education. I remember taking an Honours level mathematical modelling course a few years ago where the three hours of lecture time allocated us were used to discuss concepts and do modelling. We would read a chapter from the textbook in the lead-up to the class and then have a talk about what it meant and then work out a model based on a case study. I don’t know how well a truly flipped class would translate to a group bigger than about 30 students, but Sue Savage (QUT) tells me that the new lecture theatres in P block are designed to facilitate small group discussions within lectures. Daina Taimiņa explains &lt;a href=&#34;https://www.youtube.com/watch?v=w1TBZhd-sN0&#34;&gt;hyperbolic geometry&lt;/a&gt; with crochet. Every once in a while something similar pops up and I can’t help but get excited. &lt;a href=&#34;http://longnow.org/essays/richard-feynman-connection-machine/&#34;&gt;Daniel Hills recalls his memories&lt;/a&gt; of working with Richard Feynman on developing a massive parallel computer in the 1980s.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples - July wrap</title>
      <link>/./2013/07/29/posterior-samples---july-wrap/</link>
      <pubDate>Mon, 29 Jul 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/07/29/posterior-samples---july-wrap/</guid>
      <description>&lt;p&gt;I had some Posterior Samples to share before going on leave but didn’t get around to posting them. Here’s what’s been on my mind this month: Maths and science units are popular with (Kentucky) students until they realise that &lt;a href=&#34;http://blogs.wsj.com/economics/2013/07/08/math-science-popular-until-students-realize-theyre-hard/&#34;&gt;they’re hard&lt;/a&gt;. While not directly relevant to the Australian university education model it’s probably an important thing for the Science and Engineering Faculty to keep in mind. &lt;a href=&#34;http://blogs.wsj.com/economics/2013/07/08/math-science-popular-until-students-realize-theyre-hard/&#34;&gt;&lt;/a&gt; I’m looking at ggplot2 more these days, so the idea of a “grammar of graphics” is beginning to resonate with me. &lt;a href=&#34;//www.youtube.com/watch?v=xyGggdg31mc&#34;&gt;Here’s a talk&lt;/a&gt; about building one for &lt;a href=&#34;http://clojure.org/&#34;&gt;clojure&lt;/a&gt; (which I don’t use). Something for me to keep in mind when delivering SEB113 slides this semester is &lt;a href=&#34;https://www.dropbox.com/s/p0sgdgo7mxkoj4h/What%20your%20math%20slides%20dont%20need.pdf&#34;&gt;what your maths slides don’t need&lt;/a&gt;. Probably also good pointers for any PhD students graduating soon. &lt;a href=&#34;http://well.blogs.nytimes.com/2013/07/22/the-kitchen-as-a-pollution-hazard/&#34;&gt;An interesting article in the New York Times&lt;/a&gt; about air pollution from cooking. This is something that ILAQH has a research interest in and our nanotracer paper contains a bit of analysis of inhaled surface area dose from particles that originate from cooking. &lt;a href=&#34;http://www.nytimes.com/2013/07/22/business/in-climbing-income-ladder-location-matters.html&#34;&gt;Another NYT article&lt;/a&gt;, this time with a delicious visualisation of the geographical trends in income disparity and social mobility. &lt;a href=&#34;http://www.slate.com/articles/health_and_science/science/2013/07/statistics_and_psychology_multiple_comparisons_give_spurious_results.html&#34;&gt;Andrew Gelman writes at Slate&lt;/a&gt; about some of the problems with scientific publishing and the publication of spurious findings (which isn’t always willingly dishonest). A special “Big Bayes Stories” issue of “Statistical Science” will be published soon, focussing on the real world application of Bayesian statistics where other methods were inapplicable. &lt;a href=&#34;http://xianblog.wordpress.com/2013/07/29/big-bayes-stories/&#34;&gt;Christian Robert has written the preface&lt;/a&gt;; the issue is being edited by Robert, Kerrie Mengersen (one of my PhD supervisors) and Sharon McGrayne, author of “&lt;a href=&#34;http://www.amazon.com/Theory-That-Would-Not-Die/dp/1452636850&#34;&gt;The Theory That Would Not Die&lt;/a&gt;”. Also I went to &lt;a href=&#34;http://www.questacon.edu.au/&#34;&gt;Questacon&lt;/a&gt; and it was awesome.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples - SEB113 edition</title>
      <link>/./2013/07/10/posterior-samples---seb113-edition/</link>
      <pubDate>Wed, 10 Jul 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/07/10/posterior-samples---seb113-edition/</guid>
      <description>&lt;p&gt;I’m going to be frank, a lot of this relates to SEB113 - Quantitative Methods for Science, a subject I tutored last semester. One of the students from SEB113 last semester, Daniel Franks, is &lt;a href=&#34;https://twitter.com/dpfscience&#34;&gt;live-tweeting his Bachelor of Science degree&lt;/a&gt;. Daniel was in my workshop group last semester and I recognise some of the events he talks about in his timeline. It’s interesting to see his perspective not just on SEB113 but on the other three units that form the first semester of QUT’s new Bachelor of Science course. SEB113 is getting a small makeover for Semester 2. One of the things we’re considering is the use of ggplot2 instead of a combination of the base graphics package, heatmaps from dendrograms with colorbars from yet another package, etc. Lattice doesn’t have the nicest interface and it’s nigh on impossible to add elements afterwards (I hate you, levelplot). It’s possible to do &lt;a href=&#34;http://gettinggeneticsdone.blogspot.com.au/2010/01/ggplot2-tutorial-scatterplots-in-series.html&#34;&gt;small multiples in ggplot2&lt;/a&gt; fairly easily. We ought to be sticking to the same steps in data analysis as we did last semester, and Daniel’s tweets refer to an experience in class last semester where we discussed drawing the analysis method out of exploratory plots of the data, rather than trying to pick the “best” model &lt;em&gt;a priori&lt;/em&gt; and making the data fit the model. &lt;a href=&#34;http://simplystatistics.org/2013/06/27/what-is-the-best-way-to-analyze-data/&#34;&gt;Roger Peng’s got a good five step&lt;/a&gt; technique for analysing data:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory analysis&lt;/li&gt;
&lt;li&gt;Model fitting&lt;/li&gt;
&lt;li&gt;Model building&lt;/li&gt;
&lt;li&gt;Sensitivity analysis&lt;/li&gt;
&lt;li&gt;Reporting&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Via &lt;a href=&#34;http://www.quantumforest.com/2013/07/flotsam-13-early-july-links/&#34;&gt;Luis Apiolaza at Quantum Forest&lt;/a&gt; I’ve stumbled across &lt;a href=&#34;http://www.statschat.org.nz/&#34;&gt;Thomas Lumley&lt;/a&gt;’s Tumblr, where he’s doing some personal blogging about statistics. An &lt;a href=&#34;http://notstatschat.tumblr.com/post/54011641155/where-is-bayesian-introductory-statistics-better&#34;&gt;interesting post&lt;/a&gt; of his is on the role of Bayesian stats in introductory classes. I would love to turn SEB113 into a Bayesian statistics based class but for the time being I will have to settle for it dealing with modelling over tests (which is still a big win, pedagogically). Teaching Bayesian statistics generally relies on a good grounding in calculus, otherwise writing down full conditionals is going to be quite difficult. When people tell me that statistics is so different to mathematics I like to point out that it’s just a combination of calculus, linear algebra and some discrete mathematics. &lt;a href=&#34;http://magazine.amstat.org/blog/2013/07/01/calculus-and-statistics/&#34;&gt;Daniel Kaplan writes at &lt;strong&gt;AMSTAT&lt;/strong&gt;News&lt;/a&gt; about ditching mathematical formalism to make statistics more accessible. The American undergraduate model is very different to what we have in Australia, but I take his point about a first year calculus class not being as relevant to graduates as a first year statistics course that teaches statistical thinking over statistical calculation. I really like the focus in SEB113 on modelling using R rather than statistical tests by hand with pages of tables (as MAB101 was when I did my Bachelor of Science). If people finish SEB113 knowing how to read their data in to R and perform a Generalised Linear Model I think we’ll have done our job. If they want to go on to further statistics from there, the statistics units in the School of Mathematical Sciences work from a calculus perspective and while they require a calculus pre-requisite (MAB121 or MAB122 for those QUT students reading) you could do a lot worse than taking MAB210 and MAB314. I hope a follow-up data analysis course will be offered to Bachelor of Science students that builds on SEB113 and covers some more advanced topics and introduces enough mathematics to make those topics worthwhile. We’ll have to see how it all unfolds as this first cohort make their way through.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/06/24/posterior-samples/</link>
      <pubDate>Mon, 24 Jun 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/06/24/posterior-samples/</guid>
      <description>&lt;p&gt;I was a bit disappointed that it wasn’t about machines doing automatic analysis for us, but this article, “&lt;a href=&#34;http://simplystatistics.org/2013/06/14/the-vast-majority-of-statistical-analysis-is-not-performed-by-statisticians/&#34;&gt;The vast majority of statistical analysis is not performed by statisticians&lt;/a&gt;”, is a bit of a wake-up call for those statisticians who haven’t realised that we need to improve the way we teach statistics and interact with non-statisticians. I don’t think we have enough statisticians in the world to do all the analysis that needs doing, so we need to focus on training scientists and others better so that we don’t leave them stuck in a culture of bad regression and t-tests in Excel. Gianluca Baio (UCL) has &lt;a href=&#34;http://www.statistica.it/gianluca/Talks/INLA.pdf&#34;&gt;a really nice introduction&lt;/a&gt; to &lt;a href=&#34;http://www.r-inla.org/&#34;&gt;INLA&lt;/a&gt; with a comparison to &lt;a href=&#34;http://mcmc-jags.sourceforge.net/&#34;&gt;JAGS&lt;/a&gt;. I started using JAGS when WinBUGS/OpenBUGS was becoming too slow for the analysis I was doing but the major paper of my thesis uses INLA for spatio-temporal analysis. I still use both programs and when faced with a new problem will usually start in JAGS as it’s quite flexible in the way you set up priors. INLA has its advantages as well, one of them being that it will fit a Poisson likelihood to non-integer data very well. There’s &lt;a href=&#34;http://blogs.plos.org/attheinterface/2013/06/19/why-art-and-science/&#34;&gt;a neat little article on the PLoS blog&lt;/a&gt; about linkages between art and science and how the involvement of art in research (beyond making prettier plots, which is really more an issue of design than art) can lead to better scientific outcomes. Radford Neal has just announced &lt;a href=&#34;http://radfordneal.wordpress.com/2013/06/22/announcing-pqr-a-faster-version-of-r/&#34;&gt;pqR&lt;/a&gt;, “pretty quick R”, which is designed to make use of multiple cores wherever possible and avoid unnecessarily onerous computation. It’s not available for Mac/Windows yet, so I won’t be able to look at it for the time being. I wonder if QUT’s HPC group would consider making it available on the supercomputer.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/05/15/posterior-samples/</link>
      <pubDate>Wed, 15 May 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/05/15/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://andrewgelman.com/2013/05/14/gpstuff-bayesian-modeling-with-gaussian-processes&#34;&gt;GPStuff 4.1 has been released recently&lt;/a&gt;. I’d like to work with Aki Vehtari’s group. I should really learn more about different inference approaches like EM, EP, VB. Some advice for my SEB113 students who may struggle with the workload of first semester university comes from the sagest of equines, &lt;span class=&#34;citation&#34;&gt;[@horse_ebooks]&lt;/span&gt;(&lt;a href=&#34;https://twitter.com/horse_ebooks&#34; class=&#34;uri&#34;&gt;https://twitter.com/horse_ebooks&lt;/a&gt;).&lt;a href=&#34;http://samcliffordinfo.files.wordpress.com/2013/05/tumblr_mmjxritaln1qfgro5o1_500h.jpg&#34;&gt;&lt;img src=&#34;tumblr_mmjxritaln1qfgro5o1_500h.jpg?w=450&#34; alt=&#34;horseebooks&#34; /&gt;&lt;/a&gt;&lt;a href=&#34;http://www.biology.uq.edu.au/staff/hugh-possingham&#34;&gt;Hugh Possingham&lt;/a&gt;’s &lt;a href=&#34;http://brisscience.wordpress.com/2013/04/20/monday-20-may-2013/&#34;&gt;talking on Monday&lt;/a&gt; about the mathematics and economics of conservation as part of the &lt;a href=&#34;http://brisscience.wordpress.com/&#34;&gt;BrisScience&lt;/a&gt; seminar series. I’ve been meaning to make it to a Possingham talk for a while.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/05/13/posterior-samples/</link>
      <pubDate>Mon, 13 May 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/05/13/posterior-samples/</guid>
      <description>&lt;p&gt;Something important to remember as a maths educator is that &lt;a href=&#34;http://www.slate.com/articles/health_and_science/science/2013/04/math_teacher_explains_math_anxiety_and_defensiveness_it_hurts_to_feel_stupid.single.html&#34;&gt;maths can make people feel stupid and people don’t like feeling stupid&lt;/a&gt;. &lt;a href=&#34;http://kottke.org/13/05/riding-an-icebreaker&#34;&gt;Ever wondered what it’s like to spend two months on an ice-breaker?&lt;/a&gt; The &lt;a href=&#34;http://harvarddatascience.com/2013/05/05/harvard-stat-221-statistical-computing-and-visualization-all-lectures-online/&#34;&gt;lecture slides from Harvard’s Stat 221 course&lt;/a&gt; are all online now. Commander Chris Hadfield’s &lt;a href=&#34;http://www.youtube.com/watch?v=KaOC9danxNo&#34;&gt;farewell from the ISS&lt;/a&gt; is, fittingly, a cover of David Bowie’s “Space Oddity”.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/01/10/posterior-samples/</link>
      <pubDate>Thu, 10 Jan 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/01/10/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://andrewgelman.com/2013/01/a-important-new-survey-of-bayesian-predictive-methods-for-model-assessment-selection-and-comparison/&#34;&gt;A very comprehensive paper by Vehtari and Ojanen discusses various aspects of model assessment and comparison&lt;/a&gt;. I was mentioning to a friend the other day that I don’t think model assessment is taught very well in first year statistics classes. I recognise that you can’t teach everything in a first year class, but I think we should be going beyond R&lt;sup&gt;2&lt;/sup&gt; as a summary of model performance. Even the AIC would be a good start, as it incorporates a trade-off between goodness of fit and model complexity. I am yet to fully read the paper but it’s something I should definitely get my head around. &lt;a href=&#34;http://dynamicecology.wordpress.com/2013/01/03/advice-how-to-review-a-manuscript-for-a-journal/&#34;&gt;Dynamic Ecology has some advice for how to review a journal article&lt;/a&gt;. This will become increasingly relevant for me as a junior researcher. No doubt I’ll be asked whether I could review some submitted articles for a journal in which I am publishing my papers. Having not done it before, the first one will no doubt be the toughest. Perhaps I should ask the more junior reviewer of my thesis panel for any recent experience they’ve had. &lt;a href=&#34;http://www.insidehighered.com/news/2013/01/09/jstor-offer-limited-free-access-content-1200-journals&#34;&gt;JSTOR’s “Register &amp;amp; Read”&lt;/a&gt; program is an attempt to open up access to academic publishing by allowing individuals not affiliated with an organisation with a JSTOR subscription to read three articles every two weeks for free. This will be good for when news articles discuss a study and people are interested enough to go and find the original paper but don’t want to drop $30 to read it. It’s not as far-reaching as Open Access but it’s a good step. &lt;a href=&#34;https://twitter.com/search/%23overlyhonestmethods&#34;&gt;#OverlyHonestMethods&lt;/a&gt; is a hilarious Twitter hashtag that is encouraging scientists to confess their transgressions and is &lt;a href=&#34;http://io9.com/5974256/overlyhonestmethods-is-the-postsecret-of-the-science-world-and-it-is-amazing&#34;&gt;being called&lt;/a&gt; the &lt;a href=&#34;http://www.postsecret.com/&#34;&gt;PostSecret&lt;/a&gt; of the science world.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2012/12/27/posterior-samples/</link>
      <pubDate>Thu, 27 Dec 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/12/27/posterior-samples/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;span style=&#34;line-height:12px;&#34;&gt;&lt;a href=&#34;http://andrewgelman.com/2012/12/textbook-for-data-visualization/&#34;&gt;Gelman recommends&lt;/a&gt; Cleveland’s “&lt;a href=&#34;http://books.google.com.au/books/about/The_elements_of_graphing_data.html?id=Y2pqAAAAMAAJ&#34;&gt;The Elements of Graphing Data&lt;/a&gt;” over Tufte’s work for someone looking to put together a data analysis and visualisation couse. I’ll see if I can grab a copy from the library, it sounds interesting.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.nytimes.com/video/2012/12/24/science/100000001947354/wrights-law.html&#34;&gt;Jeffrey Wright uses wacky experiments to teach children about the universe, but it is his own personal story that teaches them the true meaning of life.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.quora.com/Google/What-do-statisticians-do-at-Google&#34;&gt;Quora - What do statisticians do at Google?&lt;/a&gt; Some of the answer is deliberately vague, but the answer appears to be “Figure out whether the engineers have made the search algorithms better” and “Try to quantify whether changes to the way ads work have been positive”.&lt;/li&gt;
&lt;li&gt;I work in an aerosol science group which has studied tobacco smoke. Something I can’t find in our publication history is a study on marijuana smoke. NORML have &lt;a href=&#34;http://norml.org/library/health-reports/item/norml-s-marijuana-health-mythology&#34;&gt;a page&lt;/a&gt; dedicated to disputing some of the myths surrounding marijuana that come from both the pro- and anti-marijuana camps. The review was done in 1994 so I’d be interested to see how the epidemiological evidence has changed as our equipment has improved. I don’t know that the Queensland government (or QUT) would let us purchase any marijuana to do the study.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hmm. Are bullet points really the way to do this? Merry Christmas!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Git used to write high school maths textbook</title>
      <link>/./2012/10/02/git-used-to-write-high-school-maths-textbook/</link>
      <pubDate>Tue, 02 Oct 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/10/02/git-used-to-write-high-school-maths-textbook/</guid>
      <description>&lt;p&gt;As a mathematician who uses git and LaTeX to collaboratively write papers with Finnish researchers, the following story is really neat. &lt;a href=&#34;http://linja-aho.blogspot.fi/2012/09/a-group-of-finnish-math-teachers-write.html&#34;&gt;A group of Finnish mathematicians, students and teachers got together and hacked out a draft upper secondary school level mathematics textbook&lt;/a&gt;. There’s still some tidying up to be done but a snapshot’s available &lt;a href=&#34;http://cloud.github.com/downloads/linjaaho/oppikirjamaraton-maa1/version-0.9.pdf&#34;&gt;here&lt;/a&gt;. &lt;a href=&#34;https://github.com/linjaaho/oppikirjamaraton-maa1/&#34;&gt;The collaborators used git&lt;/a&gt; to weave together the various parts of the book, allowing them to work on separate sections of the book and feed them back to github. One of the benefits of something like git is that when two people work on the same section and both try to commit their changes it flags the conflict and forces you to resolve it. Using Google Docs to collaborate like this leads to people getting in each others’ way and overwriting each other. Using Dropbox results in a forked version of the conflicted file with no sensible way to resolve the conflict. If you read the comments in their blog post you’ll see stories of other researchers doing similar work to create books. That the Finns are releasing their book under a CC-BY license means that others can take their work, &lt;a href=&#34;https://github.com/linjaaho/oppikirjamaraton-maa1/network&#34;&gt;fork the git repository&lt;/a&gt; and derive and compile their own versions of the book. If you speak Finnish, &lt;a href=&#34;https://www.facebook.com/oppikirjamaraton&#34;&gt;their Facebook page&lt;/a&gt; might also be of interest.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Educational testing and statistical testing</title>
      <link>/./2012/09/24/educational-testing-and-statistical-testing/</link>
      <pubDate>Mon, 24 Sep 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/09/24/educational-testing-and-statistical-testing/</guid>
      <description>&lt;p&gt;When the Australian Government introduced &lt;a href=&#34;http://www.naplan.edu.au/&#34;&gt;NAPLAN&lt;/a&gt; and the &lt;a href=&#34;http://www.myschool.edu.au/&#34;&gt;MySchool&lt;/a&gt; website I was very worried about league tables being drawn up, the ranking of schools and the stigmatisation of poorer schools as being “bad” and parents opting to not send their kids there. I don’t have a problem with benchmarks for students, letting parents know how their kids are developing and ensuring that governments are able to target their resources where they’re needed. What I do have a problem with is this bizarre notion that “accountability” means the government throws good policy, teachers and kids under a bus to appease parents or the more neoliberal elements of the national media. If it’s not clear, I have grave concerns about the impact on our education system of publicly releasing nation-wide summary statistics of how well the students are doing at each school. I think the USA’s focus on standardised testing and the awful notion of “merit based pay” threaten the integrity of public education. Having said that, the collection of this data and its appropriate analysis provide governments with a very good tool for assessing their policies and determining where to spend the finite amount of money they have. &lt;a href=&#34;http://www.quantumforest.com/2012/09/new-zealand-school-performance-beyond-the-headlines/&#34;&gt;This post&lt;/a&gt; from Quantum Forest shows how a naive analysis of literacy versus socioeconomics in New Zealand can give a very misleading picture. To cut a long story short, there’s a lot of variability that plotting a trend line or some averages doesn’t take into account. The post is worth a read and doesn’t go into a huge amount of statistical detail but explains, with some well described R code, the use of boxplots for quick summaries of data that show the variability inherent in the data. The author also discusses how this exploration can lead to appropriate modelling which takes the variability into account. This is the sort of exploration that should form the basis of any academic analysis of any data and I’m grateful to the author for explaining it simply and providing R code and the publicly available data. To me, statistics is all about quantifying uncertainty; Bayesian statistics even moreso. Confidence/credible intervals are not just something we calculate to check that something’s significant at the 5% level, they’re how we represent how certain we are about our parameter estimates. Not stating the uncertainty in one’s analysis may as well be a cardinal sin and no one should get away with providing a plot or parameter value without an estimate of its variability. It doesn’t matter if you’re a first year student, research scientist, public servant or journalist, you need to include uncertainty or else you’re lying to your audience (a lie of omission, but still a lie). With a bit of luck, there’s a statistician working at a major news service who can make use of this when the next MySchool report comes out and some statisticians working in the Department of Education who are able to make use of good statistical modelling to make suggestions to the Minister regarding funding allocation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistics Done Wrong</title>
      <link>/./2012/08/01/statistics-done-wrong/</link>
      <pubDate>Wed, 01 Aug 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/08/01/statistics-done-wrong/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.refsmmat.com/&#34;&gt;Alex Reinhart&lt;/a&gt; has written a fantastic article entitled “&lt;a href=&#34;http://www.refsmmat.com/statistics/&#34;&gt;Statistics Done Wrong&lt;/a&gt;” which goes into a lot of the mistakes that scientists made when conducting experiments and performing data analysis. Reinhart is a 20 year old physics major who says that while science has embraced the use of statistics it hasn’t embraced statistics education, often allowing undergraduate students to complete a science degree without taking any statistics classes (it was possible to take a Bachelor of Applied Science at QUT without even taking the first year data analysis unit, you just had to be prepared to do first year chemistry, life sciences and introductory physical science). The article is very good and goes into problems of insufficient statistical power, misunderstandings as to what p values are and what significance is, pseudoreplication when you think you’re doing independent replication, and how big a sample to use. I am not as well versed in statistical design as I would like to be, particularly when working in a lab which does experimental science. One of my supervisors is very passionate about good design, and for good reason; if you design your experiment or observation campaign well you will have a more efficient data collection system, an easier job doing the data analysis and the power of your tests means that you can be more confident in your analysis. As dull as statistics seems to be for first year science students, I could see it being very valuable to run a course on how to set up an experiment, both from a statistical design point of view and what is required of them in their field, and perform data analysis. I don’t think we can expect experimentalists to become great statistical teachers, so it may require an amount of service teaching by statisticians within the science units, as an optional class on basic data analysis techniques doesn’t seem to cut it. I really enjoyed the article and think it’s a great follow up to &lt;a href=&#34;http://samclifford.info/2012/07/12/plenary-speech/&#34; title=&#34;Plenary speech&#34;&gt;what I was talking about at Healthy Buildings&lt;/a&gt;. The next step is to identify how we teach these things to science students in a way that they don’t switch off.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistics - not just for statisticians</title>
      <link>/./2012/06/01/statistics---not-just-for-statisticians/</link>
      <pubDate>Fri, 01 Jun 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/06/01/statistics---not-just-for-statisticians/</guid>
      <description>&lt;p&gt;An article in &lt;a href=&#34;http://theconversation.edu.au/top-cited-academics-honoured-but-wheres-the-humanity-7348&#34;&gt;The Conversation&lt;/a&gt; made the rounds in our office yesterday and prompted a discussion with one of my colleagues about the role of statistics in scientific papers. The article itself talks about Australians who received an award from Thomson Reuters on the basis of publishing frequently cited work and discusses the different culture in science and the humanities when it comes to publication and citation. The basic argument is that the awards are skewed to the sciences where publication is traditionally by journal article and citation in other journal articles is picked up more than in the humanities where monographs/books are the traditional means of disseminating information. One of the academics interviewed makes the point that science is much more collaborative, with researchers working in teams to achieve the goals of a project. One of the email discussants in our group offered the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Graham Farquhar from the Australian National University is Australia’s citation laureate. Graham’s belief is this: start with a really good hypothesis driven question that no-one has answered and answer it. Deliver the answer with robust science and, voila, you have highly cited work. Coming up with the questions is hard though. Thinking seven impossible questions before breakfast is probably a daily norm in Graham’s existence. Doing this, and maintaining a normal personality is difficult though, but he manages quite well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A colleague talked to me about this idea of Farquhar’s in terms of a paper of his that he thought did well to answer an unanswered question. The aim of the paper was to offer an explanation for a physical phenomenon that is known in aerosols but it’s not well known why this phenomenon occurs. We then got into an argument about the robustness of the science, not based on the science that had been done (it was a synthesis of previous work by other authors), but because the evidence hadn’t all been included in a model that drew all these reviews together in a statistical manner. Scientists love &lt;em&gt;t&lt;/em&gt; tests. They are simple tests which tell you if there’s a difference between two groups and are appropriate in some, but not all, instances, and are a good first step for exploratory data analysis. I was making the point that a Bayesian meta-analysis would have been far more appropriate as it’s a technique which is specifically designed to draw multiple sources of information together in a single model to provide a better estimate of an effect size. The Bayesian approach would have also helped here as a review of the literature could be used to determine priors such that even if no new data was collected for this paper, inferences could be drawn based on the current state of knowledge. I think every research group should hire a statistician, have them retrain the researchers in how to use statistics, including GLMs/GAMs, spatial analysis, time series methods, and Bayesian inference in order to build capacity within the group. The statistician can then work with the scientists to ensure that new papers include the best analysis of results possible and to also review old papers to see if there’s any low hanging fruit in terms of interesting experiments/observations which could be re-analysed with something more than ANOVA or linear regression. Statistics isn’t just for statisticians; we need to get away from the idea that doing more than the bare minimum for a paper is going too far. Apparently QUT is reviewing the way it teaches statistics to science students (something which is long overdue, my recollection is that MAB101 is an awful unit full of statistical techniques relevant to agricultural trials) so I’m hopeful that we can teach students statistical techniques that are relevant to them in an exciting way. I hope we don’t swing too far and just give them the exact tools they need and nothing else, because then we’re limiting graduates. If you’ve got the brain power to deal with atmospheric chemistry and/or physics, you should be able to handle statistics. Not everyone goes through to do advanced level statistical units but it shouldn’t be too much of a jump from collecting experimental data to analysing it in R with a package appropriate to your field. Edited to add: The point I’m trying to make here is that all researchers should have a basic understanding of exploratory/descriptive data analysis, simple GLMs (even if it’s just using glm() in R) and the ability to communicate the physical results in terms of the statistical modelling. I don’t expect that all scientists will go and become proficient in the use of Bayesian non-parametrics, but scientists should be able to start with some scatter plots, box plots and ANOVA to look for differences and then use regression to explain those differences.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
