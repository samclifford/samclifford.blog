<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Inla on Sam Clifford </title>
    <link>/./tags/inla/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2013-12-02 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>Posterior Samples</title>
      <link>/./2013/12/02/posterior-samples/</link>
      <pubDate>Mon, 02 Dec 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/12/02/posterior-samples/</guid>
      <description>&lt;p&gt;Thiago Martins has posted &lt;a href=&#34;http://tgmstat.wordpress.com/2013/11/28/computing-and-visualizing-pca-in-r/&#34;&gt;a neat little tutorial&lt;/a&gt; about using R to calculate and visualise Principle Components Analysis, using Fisher’s Iris data. PCA is something I’ve struggled with as I’ve gone further into statistics, as it comes across as being based on mathematics rather than statistics. I’d like to learn more about the Indian Buffet Process and associated non-parametric Bayesian methods but if I’m going to be looking at long and wide data sets (say, UPTECH questionnaire data) I’d like to have somewhere to start. It looks like this may provide that. Rasmus Bååth’s done &lt;a href=&#34;http://www.sumsar.net/blog/2013/11/easy-laplace-approximation/&#34;&gt;a tutorial on Laplace Approximations in R&lt;/a&gt; (hat tip to Matt Moores for this one). Laplace Approximations are an alternative to MCMC simulation that can provide good approximations to well-behaved posterior densities in a fraction of the time. The tutorial deals with the issue of reparameterisation for when you’ve got parameters which have bounded values (such as binomial proportions). As a piece of trivia, Thiago (above) is based at NTNU where &lt;a href=&#34;http://www.r-inla.org/&#34;&gt;R-INLA&lt;/a&gt; is developed. I’m at the &lt;a href=&#34;http://emac2013.com.au/&#34;&gt;emac2013&lt;/a&gt; conference this week. We’re about half way through day one of the talks (of three) and there’s already been some fascinating stuff. Professor Robert Mahony (ANU) gave a talk that shows that the development of more advanced unmanned aerial vehicles (UAVs, drones) involves some quite complex but elegant mathematics, involving Lie group symmetries, rather than just coming up with cooler robots. Hasitha Nayanajith Polwaththe Gallage (QUT) showed some really interesting particle method (mesh-free) modelling where forces and energies were used to determine the shape of a red blood cell that had just ejected its nucleus.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/06/24/posterior-samples/</link>
      <pubDate>Mon, 24 Jun 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/06/24/posterior-samples/</guid>
      <description>&lt;p&gt;I was a bit disappointed that it wasn’t about machines doing automatic analysis for us, but this article, “&lt;a href=&#34;http://simplystatistics.org/2013/06/14/the-vast-majority-of-statistical-analysis-is-not-performed-by-statisticians/&#34;&gt;The vast majority of statistical analysis is not performed by statisticians&lt;/a&gt;”, is a bit of a wake-up call for those statisticians who haven’t realised that we need to improve the way we teach statistics and interact with non-statisticians. I don’t think we have enough statisticians in the world to do all the analysis that needs doing, so we need to focus on training scientists and others better so that we don’t leave them stuck in a culture of bad regression and t-tests in Excel. Gianluca Baio (UCL) has &lt;a href=&#34;http://www.statistica.it/gianluca/Talks/INLA.pdf&#34;&gt;a really nice introduction&lt;/a&gt; to &lt;a href=&#34;http://www.r-inla.org/&#34;&gt;INLA&lt;/a&gt; with a comparison to &lt;a href=&#34;http://mcmc-jags.sourceforge.net/&#34;&gt;JAGS&lt;/a&gt;. I started using JAGS when WinBUGS/OpenBUGS was becoming too slow for the analysis I was doing but the major paper of my thesis uses INLA for spatio-temporal analysis. I still use both programs and when faced with a new problem will usually start in JAGS as it’s quite flexible in the way you set up priors. INLA has its advantages as well, one of them being that it will fit a Poisson likelihood to non-integer data very well. There’s &lt;a href=&#34;http://blogs.plos.org/attheinterface/2013/06/19/why-art-and-science/&#34;&gt;a neat little article on the PLoS blog&lt;/a&gt; about linkages between art and science and how the involvement of art in research (beyond making prettier plots, which is really more an issue of design than art) can lead to better scientific outcomes. Radford Neal has just announced &lt;a href=&#34;http://radfordneal.wordpress.com/2013/06/22/announcing-pqr-a-faster-version-of-r/&#34;&gt;pqR&lt;/a&gt;, “pretty quick R”, which is designed to make use of multiple cores wherever possible and avoid unnecessarily onerous computation. It’s not available for Mac/Windows yet, so I won’t be able to look at it for the time being. I wonder if QUT’s HPC group would consider making it available on the supercomputer.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New INLA stuff makes me happy</title>
      <link>/./2013/04/16/new-inla-stuff-makes-me-happy/</link>
      <pubDate>Tue, 16 Apr 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/04/16/new-inla-stuff-makes-me-happy/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;www.r-inla.org&#34;&gt;R-INLA&lt;/a&gt; is a really neat use of GMRFs for computing posteriors for quite complicated Bayesian Latent Gaussian Models. I used it for spatio-temporal modelling in my PhD and had to feel my way through a lot based on an old demo which was purely spatial. As I got further and further into my PhD I saw extensions for R-INLA being written thanks to a few visits from, and email correspondence with, &lt;a href=&#34;http://www.math.ntnu.no/~daniesi/&#34;&gt;Dr Daniel Simpson&lt;/a&gt;, and the help list on the R-INLA site where Dan, Håvard Rue and Finn Lindgren are very quick with a reply. A few days ago I got an email from Rue telling me he’d been made aware of &lt;a href=&#34;http://arxiv.org/abs/1206.3833&#34;&gt;one of my thesis papers&lt;/a&gt; and if I wouldn’t mind having a look at running it with the new testing distribution of R-INLA. It’s the first time I’ve looked at the code again since submitting the paper for publication and it seems that an awful lot of work has been put into internal optimisation. The code for running my model requires less manual tuning now and I’m excited about using it in follow-up papers where I’ll be looking at more of the UPTECH data. There’s also a &lt;a href=&#34;http://www.r-inla.org/examples/tutorials/spde-tutorial&#34;&gt;new tutorial for spatial modelling with INLA&lt;/a&gt;, written by &lt;a href=&#34;http://www.leg.ufpr.br/~elias/&#34;&gt;Elias T. Krainski&lt;/a&gt;, which covers a number of topics such as a simple spatial regression, a spatial model with misalignment and non-stationary spatial models (which I’ve seen talked about a few times but there’s very little documentation about them). I think R-INLA, particularly the spatial modelling, has really come a long way over the last few years and it’s encouraging to see it being taken up at QUT where students would probably have used WinBUGS in the past. While there are some limitations in terms of the flexibility of the classes of models that can be fit in BUGS versus R-INLA I’d much rather do any spatial, spatio-temporal or non-parametric smoothing in R-INLA.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayes on the Beach</title>
      <link>/./2012/11/08/bayes-on-the-beach/</link>
      <pubDate>Thu, 08 Nov 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/11/08/bayes-on-the-beach/</guid>
      <description>&lt;p&gt;I’ve spent the last three days at the Bayes on the Beach conference/workshop in &lt;a href=&#34;https://maps.google.com.au/maps?q=caloundra&amp;amp;hl=en&amp;amp;sll=-19.457034,145.879162&amp;amp;sspn=35.100111,39.506836&amp;amp;hnear=Caloundra+Queensland&amp;amp;t=m&amp;amp;z=14&amp;amp;iwloc=A&#34;&gt;Caloundra&lt;/a&gt;. The meeting is an annual event where Bayesian statisticians (usually Australians who know Kerrie Mengersen) get together for a very casual set of talks, workshops and other activities in a beachside town.&lt;/p&gt;
&lt;div id=&#34;design&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Design&lt;/h3&gt;
&lt;p&gt;We arrived in Caloundra on Tuesday morning and got down to business with a keynote talk from &lt;a href=&#34;http://www.southampton.ac.uk/maths/about/staff/davew.page&#34;&gt;Dave Woods&lt;/a&gt; (University of Southampton). Professor Woods’ talk opened a day in which the over-arching theme was experimental design and made the point that any design is at least implicitly Bayesian. I really agree with this point because any design is based on the prior experience of the experimenter or is pieced together from a review of what’s been done previously. I don’t know of any situation in which an experimental study was carried out totally at random without choosing appropriate covariate values. I don’t have much of a head for design but I think after seeing the talks on Tuesday (particularly Liz Ryan’s, which others in her session praised as giving a good review of utility) I’m a bit more aware of what it all means. What really helped was &lt;a href=&#34;http://pharmacy.otago.ac.nz/our-people/academic-staff/stephen-duffull&#34;&gt;Stephen Duffull&lt;/a&gt; (University of Otago) in his Thursday talk where he spelled out D-optimality (wanting to maximise the effect) and P-optimality (wanting to get the best estimate of parameters). D- and P-optimality work in opposite directions quite frequently (killing all your patients doesn’t tell you much about the parameters in your model, for example) but it’s possible to trade them off with a DP-optimal design.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;workshops&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Workshops&lt;/h3&gt;
&lt;p&gt;One of my favourite aspects of the Bayes on the Beach meetings is the workshops. At my first Bayes on the Beach (2009) we were shown how to do a Bayesian meta-analysis to combine the results of a bunch of disparate studies that dealt with the same topic. Ever since then, I’ve been looking forward to learning some more statistics or getting to grips with interesting data. &lt;a href=&#34;http://datasearch2.uts.edu.au/science/staff/maths/details.cfm?StaffID=11641&#34;&gt;Matt Wand&lt;/a&gt; (University of Technology Sydney), &lt;a href=&#34;http://data.aims.gov.au/staffcv/jsf/external/view.xhtml?partyId=900000428&#34;&gt;Julian Caley&lt;/a&gt; (Australian Institute of Marine Science) and &lt;a href=&#34;http://staff.qut.edu.au/staff/lowchoy/&#34;&gt;Sama Low Choy&lt;/a&gt; (Queensland University of Technology, my associate supervisor) each pitched a problem that people could have a look at, not necessarily solving but, working towards a solution. Wand presented some really interesting spatio-temporal data of extreme rainfall in NSW which I was very keen to sign up to but Kerrie suggested that it might be a bit more of a challenge to do something other than spatio-temporal modelling of environmental data. Sama presented some work that she’s been doing on combining elicited opinions into a subjective prior that represents the aggregate knowledge of experts. I ended up in Caley’s group, where we worked on some data that &lt;a href=&#34;http://scholar.google.com/citations?user=T-nbIuUAAAAJ&amp;amp;hl=en&#34;&gt;Julie Vercelloni&lt;/a&gt; is dealing with as part of her PhD.&lt;/p&gt;
&lt;div id=&#34;reef-workshop&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Reef workshop&lt;/h4&gt;
&lt;p&gt;Caley, Vercelloni and Mengersen (among others) are working on coral coverage in the Great Barrier Reef in six sectors that run up the length of the reef (2600km). Within each sector there are reef shelves and on each shelf there are multiple reefs with multiple measurement patches. The question we attempted to answer was “How different are the long term trends in coral coverage at these reefs?” Caley has data going back to about 1994, reported annually, which when pooled looked very boring but when plotted (very well by Vercelloni) grouped by reef shelf within sector indicated that there might be quite a lot of interesting variation which may not be so straight forward to model. Our group split up into a few subgroups with different approaches and I ended up working with James McKeone and a few others on a model inspired by &lt;a href=&#34;http://samclifford.info/2012/10/19/anova/&#34; title=&#34;ANOVA&#34;&gt;Cari Kaufmann’s functional ANOVA with GP priors&lt;/a&gt;. Over the next day or so McKeone took what we’d discussed and written down as a model and came up with quite a general Gibbs sampling scheme that is flexible enough to admit any linear predictor. I’m fairly certain James and I both had P-splines in mind but I did talk later to Matt Wand about O’Sullivan splines and I think it might be conceptually easier to use Wand’s low rank thin plate smoothers, particularly as Vercelloni’s quite new to Bayesian statistics and has a background in ecology rather than computational statistics. It was interesting to see how the other workshops went on the Thursday afternoon recap. Sama’s group had split into three groups, each tackling the issue of elicitation with a different topic and a different angle. I must say that my favourite was the group who did an elicitation of predictions of the outcome of the US Presidential Election. &lt;a href=&#34;http://conidialcoleopticide.wordpress.com/&#34;&gt;Luisa Hall&lt;/a&gt; even managed to elicit my opinion for her survey without me even realising it (we talk a lot about politics)! The other groups asked about how risky a life-saving operation would have to be for them to not take it and average completion time for PhD students (which Sama gave a talk about on Thursday).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;real-time-updates-for-mean-field-variational-bayes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Real time updates for Mean Field Variational Bayes&lt;/h3&gt;
&lt;p&gt;Matt Wand also gave a talk and tutorial about using Mean Field Variational Bayes (a name he attributes to Mike Jordan) to do live, real-time updates of posterior estimates with streamed data such as stock trading. Rather than going into the content of the talk, I suggest you read &lt;a href=&#34;http://www.uow.edu.au/~mwand/ovbpap.pdf&#34;&gt;the paper&lt;/a&gt; he’s written with Tamara Broderick (University of California, Berkeley) and his PhD student Jan Luts (University of Technology Sydney) and check out &lt;a href=&#34;http://realtime-semiparametric-regression.net/&#34;&gt;their website&lt;/a&gt; with neat examples, e.g. &lt;a href=&#34;http://realtime-semiparametric-regression.net/SydneyRealEstate/&#34;&gt;the Sydney rental market&lt;/a&gt; (which I think would be fascinating with the train lines superimposed).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;games-and-posters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Games and Posters&lt;/h3&gt;
&lt;p&gt;Tuesday and Wednesday evenings had Luisa and I running some games (&lt;a href=&#34;http://en.wikipedia.org/wiki/Fictionary&#34;&gt;Dictionary&lt;/a&gt; and &lt;a href=&#34;http://boardgamegeek.com/boardgame/38159/ultimate-werewolf-ultimate-edition&#34;&gt;Werewolf&lt;/a&gt;) before the poster sessions. It’s interesting running games like this with Bayesians because Dictionary is basically a problem of credibility of unknown experts and Werewolf is all about updating an initially uninformative prior with information based on peoples’ behaviour as they accuse others while trying to avoid being lynched. The poster sessions themselves were quite good, with a wide variety of applications and methodologies being presented. Everyone seemed quite keen to talk about their posters and they were generally of a high quality. On Thursday afternoon I gave my talk about spatio-temporal modelling of the UPTECH data as a case study for INLA. I got a few questions about the versatility of INLA and my choice of random walk and spline models instead of other bases as well as comments about the spatial modelling I’m doing. Definitely some things to think about for the next papers I write.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-success&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A success&lt;/h3&gt;
&lt;p&gt;The organisers of Bayes on the Beach (Nicole White, Matt Moores, Jannah Baker and Dow Jaemjamrat) all did a really good job and I think everyone had a good time but was also inspired. I had a few minor issues (timekeeping is a perpetual bugbear of mine and I’m yet to go to a conference that runs totally on schedule) but think that the meeting did a really good job of bringing together a disparate group of researchers and introducing them to each other and providing ideas for future work and employment opportunities (Kim-Anh Do did a really good job of selling the &lt;a href=&#34;http://www.mdanderson.org/&#34;&gt;MD Anderson Cancer Center&lt;/a&gt;). I think the challenge for future Bayes on the Beach meetings will be managing the growth in the number of attendees. I look forward to next year’s meeting; I might even have some time to go to the beach!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>New paper from Trondheim&#39;s INLA group (arXiv)</title>
      <link>/./2012/10/03/new-paper-from-trondheims-inla-group-arxiv/</link>
      <pubDate>Wed, 03 Oct 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/10/03/new-paper-from-trondheims-inla-group-arxiv/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/1210.0333&#34;&gt;Martins, Simpson, Lindgren and Rue&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The INLA approach for approximate Bayesian inference for latent Gaussian models has been shown to give fast and accurate estimates of posterior marginals and also to be a valuable tool in practice via the R-package R-INLA. In this paper we formalize new developments in the R-INLA package and show how these features greatly extend the scope of models that can be analyzed by this interface. We also discuss the current default method in R-INLA to approximate posterior marginals of the hyperparameters using only a modest number of evaluations of the joint posterior distribution of the hyperparameters, without any need for numerical integration.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I’m using R-INLA for two UPTECH papers at the moment and I’m really interested in the replication functionality in 3.2 and the Kronecker functionality in 3.6, which seems to be a more “official” version of the work I’ve been doing rolling my own precision matrices for use with the generic0 model class. R-INLA is definitely a worthwhile piece of software to learn to use and the stochastic PDE model for spatial inference is one of the coolest things I’ve seen in terms of spatial modelling. I wonder if the Kronecker stuff in 3.6 will let me define a separable product of a stochastic PDE model and a model for the time series component.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unfinnished business</title>
      <link>/./2012/08/30/unfinnished-business/</link>
      <pubDate>Thu, 30 Aug 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/08/30/unfinnished-business/</guid>
      <description>&lt;p&gt;I’ve just heard from Kerrie Mengersen that the Finnish paper got rejected by BA for not being novel enough to publish there. So now I’m in a situation where I’ve got a paper which is too methodological for an applied stats journal (and far too methodological for an atmospheric science journal) and not a novel enough methodology for a journal as theoretical as BA. If BA don’t think it’s novel enough and it’s not the first time this data’s been published we’ll struggle to get it into something like &lt;a href=&#34;http://www.rss.org.uk/site/cms/contentviewarticle.asp?article=876&#34;&gt;JRSS B&lt;/a&gt; (IF: 3.645), &lt;a href=&#34;http://rsif.royalsocietypublishing.org/&#34;&gt;JRS: Interface&lt;/a&gt; (IF: 4.402) or &lt;a href=&#34;http://www.plosone.org&#34;&gt;PLoS One&lt;/a&gt; (IF: 4.092). Our options, then, seem to be trying an Elsevier journal like &lt;a href=&#34;http://www.journals.elsevier.com/environmental-modelling-and-software/&#34;&gt;EMS&lt;/a&gt; (IF: 3.114) or &lt;a href=&#34;http://www.journals.elsevier.com/computational-statistics-and-data-analysis/&#34;&gt;CSDA&lt;/a&gt; (IF: 1.028), which I’m not keen to do, somewhere like &lt;a href=&#34;http://www.jstatsoft.org&#34;&gt;JSS&lt;/a&gt; (IF: 2.647) or a more applied journal like &lt;a href=&#34;http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1467-9876&#34;&gt;JRSS C&lt;/a&gt; (IF: 0.828, which is quite low). I’d like to put this in a statistics journal because I want to have a career as a statistician rather than just someone who can only work in aerosols. That’s one of the reasons I’m not keen to publish this somewhere like Atmospheric Environment (where both my article and Bjarke’s, the bases of this work, were published). I’m really kicking myself now for not submitting an abstract for this as a contributed talk at ISBA. I would’ve got a BA article out of it. In happier news, I gave a presentation with two other PhD students to BRAG this morning where we talked about INLA. It went well and I think we’ve convinced a few of the others that INLA is pretty cool and worth using. Edit: and I’m getting a lot of mileage out of the Finnish/finish pun.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open access and my next paper</title>
      <link>/./2012/04/19/open-access-and-my-next-paper/</link>
      <pubDate>Thu, 19 Apr 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/04/19/open-access-and-my-next-paper/</guid>
      <description>&lt;p&gt;I got some substantial feedback from one of my supervisors earlier in the week and I’ve been chopping and changing this paper to ensure that the results section is more than just “Look at these graphs. They are graphs.” and that the introduction actually introduces the problem before leaping into how we propose to solve it. Another one of my supervisors will always pick me up on my use of informal language. I tend to write as if I’m explaining how I went about solving a problem, which is very well suited to tutorials, blogging and the &lt;a href=&#34;https://wiki.qut.edu.au/display/npbayes/home&#34;&gt;NP Bayes Wiki&lt;/a&gt; (the reading group is currently on hiatus) but less well suited to academic papers which will be sent to peer reviewed journals. I need to keep working on my formal scientific writing. Part of that probably means reading more academic papers from start to finish rather than skimming them for the information that I need. It probably also doesn’t help that most of the methodological stuff that I’m writing is drawn from the documentation and FAQs of an R package as well as personal communication with the authors of said package (said package is &lt;a href=&#34;www.r-inla.org&#34;&gt;R-INLA&lt;/a&gt;). I’ve got three papers on the go at the moment, so it’s going to be important that I can pick the low-hanging fruit of avoiding informal language so that my supervisors can concentrate on the actual work when giving me feedback. I’ve recently been spending a fair amount of time thinking about, and discussing with some colleagues, the issues of the &lt;a href=&#34;http://thecostofknowledge.com/&#34;&gt;Elsevier boycott&lt;/a&gt;, open access publishing and the use of preprint repositories like &lt;a href=&#34;http://arxiv.org/&#34;&gt;arXiv&lt;/a&gt;. QUT supports open access to some extent by running an &lt;a href=&#34;http://eprints.qut.edu.au/&#34;&gt;ePrints&lt;/a&gt; repository, where university researchers are encouraged to deposit a preprint of their work (not the version published by the journal), and &lt;a href=&#34;http://libguides.library.qut.edu.au/content.php?pid=84068&amp;amp;sid=624559&#34;&gt;subscribing to, or taking out membership with&lt;/a&gt;, a few open access journals. Some of these are quite highly regarded, particularly publications by the Public Library of Science, such as &lt;a href=&#34;http://www.plosone.org/home.action&#34;&gt;PLoS ONE&lt;/a&gt;. The bulk of our papers at ILAQH end up in Elsevier’s &lt;a href=&#34;http://www.journals.elsevier.com/atmospheric-environment/&#34;&gt;Atmospheric Environment&lt;/a&gt;, quite a respectable journal, but I feel uncomfortable propping up a business with such a coercive business model that allows them to operate at a &lt;a href=&#34;http://www.nytimes.com/2012/02/14/science/researchers-boycott-elsevier-journal-publisher.html?_r=1&#34;&gt;profit level of 36&lt;/a&gt;%. Sure, Adam Smith tells us that the baker is a baker not out of a love of bread but out of a desire to earn money with which to live, but we don’t have to accept the terms of business of one large company. I’m particularly interested in journals where the authors retain the copyright (or in the case of QUT, &lt;a href=&#34;http://www.mopp.qut.edu.au/D/D_03_01.jsp#D_03_01.05.mdoc&#34;&gt;the authors’ institution&lt;/a&gt;). I’m also a fan of journals which are innovating in the modern publishing environment, such as the &lt;a href=&#34;http://www.amstat.org&#34;&gt;American Statistical Association&lt;/a&gt;’s &lt;a href=&#34;http://www.jstatsoft.org/&#34;&gt;Journal of Statistical Software&lt;/a&gt; which is not only open access but publishes each article as its own issue of the journal, increasing the frequency of publication to a point where when an article’s reviewed it’s published straight away. I don’t like Elsevier’s business model and would be happy to look at publishing in high quality open access journals where the authors don’t have to sign away their copyright to the publishers. After all, it’s not the journals funding or doing the work. I believe granting the publishers a time-limited exclusive license to publish would be a far better model. There are other journals with a similar level of credibility and respect that don’t require authors to sign over copyright, don’t coerce libraries into taking big packages with unwanted journals bundled in and don’t freeze the person in the street out of reading the results of publicly funded research (a lot of research in Australia is funded by the &lt;a href=&#34;http://www.arc.gov.au/&#34;&gt;Australian Research Council&lt;/a&gt;, a government funding body). I don’t think Elsevier are evil, but they have a terrible business model in terms of disseminating high quality research to the public in an affordable manner. There are others out there doing a better job and researchers should support them.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Don&#39;t forget to clean up</title>
      <link>/./2012/04/07/dont-forget-to-clean-up/</link>
      <pubDate>Sat, 07 Apr 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/04/07/dont-forget-to-clean-up/</guid>
      <description>&lt;p&gt;I just noticed I’m using 11GB of storage space on QUT’s HPC server. Almost all of that will be results from remotely running R-INLA over the last few weeks while writing my second paper. The INLA objects are usually tens of megabytes but sometimes hundreds depending on the model I fit (with ~50,000 rows of data) and I think I’d be pushing my relationship with the HPC team if I didn’t clean this up every so often. Thanks heaps to Håvard Rue and Dan Simpson for their help with getting me sorted out with remote R-INLA. Edit: if you’re at QUT and want to run R-INLA remotely, &lt;a href=&#34;https://docs.google.com/document/d/1gcZ-6mfFh0R1WuprSM3PWyRDjUZ7z048RS_bFXnFXU8/edit&#34;&gt;here are the instructions&lt;/a&gt; that have been worked out by me, Dan, Håvard and QUT HPC’s Ashley Wright.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
