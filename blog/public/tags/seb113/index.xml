<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Seb113 on Sam Clifford </title>
    <link>/./tags/seb113/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2016-03-10 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>Diagnostics for first year students</title>
      <link>/./2016/03/10/diagnostics-for-first-year-students/</link>
      <pubDate>Thu, 10 Mar 2016 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2016/03/10/diagnostics-for-first-year-students/</guid>
      <description>&lt;p&gt;The SEB113 teaching team last semester (me, Ruth Luscombe, Iwona Czaplinski, Brett Fyfield) wrote a paper for the HERDSA conference about the relationship between student engagement and success. We collected data on the timing of students’ use of the adaptive release tool we developed, where students confirm that they’ve seen some preparatory material before being given access to the lecture, computer lab and workshop material. We built a regression model that looked at the relationship between the number of weeks of material students gave themselves access to and their end of semester marks (out of 100%), and it showed that students who engaged more obtained better marks, where engagement also included active use of the Facebook group and attendance at workshop classes. I had assumed that we’d be able to get data on students’ maths backgrounds coming in, but with so many ways to enter university, we don’t have the background info on every student. QUT has set Queensland Senior Maths B as the assumed knowledge for SEB113 (and indeed the broader ST01 Bachelor of Science degree) and I’m interested in knowing whether or not the level of maths of students coming in has a bearing on how well they do over the course of the unit. This semester, we decided that it’d be good to not just get a sense of the students’ educational backgrounds but to assess what their level of mathematical and statistical skills are. We designed a diagnostic to run in the first lecture that would canvas students on their educational background, their attitudes towards mathematics and statistics, and how well they could answer a set of questions that a student passing Senior Maths B would be able to complete. The questions were taken from the PhD thesis of Dr Therese Wilson and research published by Dr Helen MacGillivray (both at QUT), so I’m fairly confident we’re asking the right questions. One thing I really liked about Dr MacGillivray’s diagnostic tool, a multiple choice test designed for engineering students, is that each incorrect choice is wrong for a very specific reason, such as not getting the order of operations right, not recognising something as a difference of squares, etc. I’m about to get the scanned and processed results back from the library and it turns out that a number of students didn’t put their name or student number on the answer sheet. Some put their names down but didn’t fill in the circles, so the machine that scans the answer sheet won’t be able to determine who the student is and it’ll take some manual data entry probably on my part to ensure that we can get as many students as possible the results of their diagnostic. So while I’ll have a good sense of the class overall, and how we need to support them, it’ll be harder than it should be to ensure that the people who need the help are able to be targetted for such help. Next semester I’ll try to run the same sort of thing, perhaps with a few modifications. We’ll need to be very clear about entering student numbers and names so that we can get everyone their own results. It’d be good to write a paper that follows on from our HERDSA paper and includes more information about educational background. It might also be interesting to check the relationship between students’ strength in particular topics (e.g. calculus, probability) and their marks on the corresponding items of assessment. Getting it right next semester and running it again in Semester 1 2017 would be a very useful way of gauging whether students who are weak in particular topics struggle to do well on certain pieces of assessment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R Markdown</title>
      <link>/./2015/12/21/r-markdown/</link>
      <pubDate>Mon, 21 Dec 2015 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2015/12/21/r-markdown/</guid>
      <description>&lt;p&gt;I’ve been spending a bit of time over the last few days making an R tutorial for the members of my air quality research group. Rather than being a very general introduction to the use of R, e.g. file input/output, loops, making objects, I’ve decided to show a very applied workflow that involves the actual data analysis and explaining ideas as we go along. Part of this philosophy is that I’m not going to write a statistics tutorial, opting instead to point readers to textbooks that deal with first year topics such as regression models and hypothesis tests. It’s been a very interesting experience, and it’s meant having to deal with challenges along the way such as PDF graphs that take up so much file space for how (un-)important they are to the overall guide and, thinking about how to structure the tutorial so that I can assume zero experience with R but some experience with self-directed learning. The current version can be seen &lt;a href=&#34;https://samcliffordinfo.files.wordpress.com/2015/12/tutorial-20151221.pdf&#34; title=&#34;tutorial-20151221&#34;&gt;here&lt;/a&gt;. One of the ideas that Sama Low Choy had for SEB113 when she was unit coordinator and lecturer and I was just a tutor, was to write a textbook for the unit because there wasn’t anything that really covered our approach. Since seeing computational stats classes in the USA being hosted as repositories on GitHub I think it might be possible to use R Markdown or GitBook to write an R Markdown project that could be compiled either as a textbook with exercises or as a set of slides.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ALP wants to teach kids how to program, and I agree</title>
      <link>/./2015/05/28/alp-wants-to-teach-kids-how-to-program-and-i-agree/</link>
      <pubDate>Thu, 28 May 2015 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2015/05/28/alp-wants-to-teach-kids-how-to-program-and-i-agree/</guid>
      <description>&lt;p&gt;I checked in on one of my workshop classes this morning to see how everyone was going in the final week, to remind them of the remaining help sessions and to check that they’re on track to complete their group assignments. There weren’t many students in the class, what with it being week 13, but of one of the students was very proud of the fact that she’d lifted her marks on the problem solving tasks from 1/10 to 8/10 over the course of the semester. She told me that going back over the last few workshops helped reinforce the coding that she needed to be able to do in order to complete the assessment. She plans on transferring into medicine, which is typically not a career that requires programming. At the end of the semester, with only one piece of assessment remaining and the decision made that she will change out of science, she is still putting a lot of effort into understanding the statistics and learning how to program is reinforcing this and allowing her to engage deeper than if we were restricted to the stats education I had in first year ten years ago where we spent a lot of time looking up the tails of distributions in a book of tables. Maths and statistics education (for students not studying maths/stats as a major) is no longer just about teaching students how to do long division in high school and calculus and point and click statistics methods at university. While some degree such as Electrical Engineering, Computer Science and IT have traditionally been associated with some amount of programming, it’s becoming more and more common for maths and stats service units to include MATLAB or R as a means of engaging deeper with the mathematical content and understanding solutions to linear systems and differential equations or performing data analysis and visualisation. Learning to program leads to better understanding of what you’re actually doing with the code. Computers are everywhere in our students’ lives and in their educational experiences. Due to their ubiquity, the relationship students have with computers is very different to what it was 10 years ago. Computers are great at enabling access to knowledge through library databases, Wikipedia and a bunch of other online repositories. But it’s not enough to be able to look up the answer, one also has to be able to calculate an answer when it hasn’t been determined by someone else. There is not yet a mathematics or statistics package that does all of the data analysis and all of the mathematical analysis that we might want to do in a classroom with a point and click, drag and drop interface. To this end, I teach my students how to use R to solve a problem. Computers can do nearly anything, but we have to be able to tell the computer how to do it. Learning simple coding skills in school prepares students to tackle more advanced coding in quantitative units in their university studies but it also teaches an understanding of how processes work based on inputs and outputs, and not just computational processes, it’s all about a literacy of processes and functions (inputs and outputs). Learning to code isn’t just about writing code as a profession no more than teaching students to read is done to prepare them in their profession of priest or newsreader. Coding provides another set of skills that are relevant to the future of learning and participation in society and the workforce, just as learning mathematics allows people to understand things like bank loans. Tony Abbott does not sound like he’s on board with the idea of giving kids the skills to get along in a world in which computers are part of our classroom the way books were when he was going through school. While reading, writing and basic mathematics skills will continue to be important skills, literacy is more than just reading comprehension. Information literacy, being able to handle data, and being able to reason out a process are even more important thanks to the changing technologies we are experiencing. Not every student is going to be a professional programmer, an app developer or big data analyst, but coding will be a skill which becomes more and more necessary as computers become more and more a part of our workplace not just as fancy typewriters or an instantaneous postal system but as a problem solving tool.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Marrying differential equations and regression</title>
      <link>/./2015/01/12/marrying-differential-equations-and-regression/</link>
      <pubDate>Mon, 12 Jan 2015 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2015/01/12/marrying-differential-equations-and-regression/</guid>
      <description>&lt;p&gt;Professor Fabrizio Ruggeri (Milan) visited the Institute for Future Environments for a little while in late 2013. He has been appointed as Adjunct Professor to the Institute and gave a public talk with a brief overview of a few of his research interests. Stochastic modelling of physical systems is something I was exposed to in undergrad when a good friend of mine, Matt Begun (&lt;a href=&#34;http://www.sphcm.med.unsw.edu.au/research/infectious-diseases/phd-candidates&#34;&gt;who it turns out is doing a PhD&lt;/a&gt; under Professor Guy Marks, with whom ILAQH collaborates), suggested we do a joint Honours project where we each tackled the same problem but from different points of view, me as a mathematical modeller, him as a Bayesian statistician. It didn’t eventuate but it had stuck in my mind as an interesting topic. In SEB113 we go through some non-linear regression models and the mathematical models that give rise to them. Regression typically features a fixed equation and variable parameters and the mathematical modelling I’ve been exposed to features fixed parameters (elicited from lab experiments, previous studies, etc.) and numerical simulation of a differential equation to solve the system (as analytic methods aren’t always easy to employ). I found myself thinking “I wonder if there’s a way of doing both at once” and then shelved the thought because there was no way I would have the time to go and thoroughly research it. Having spent a bit of time thinking about it, I’ve had a crack at solving an ODE within a Bayesian regression model (Euler’s method in JAGS) for logistic growth and the Lotka-Volterra equations. I’ve started having some discussions with other mathematicians about how we marry these two ideas and it looks like I’ll be able to start redeveloping my mathematical modelling knowledge. This is somewhere I think applied statistics has a huge role to play in applied mathematical modelling. Mathematicians shouldn’t be constraining themselves to iterating over a grid of point estimates of parameters, then choosing the one which minimises some L&lt;sup&gt;&lt;em&gt;p&lt;/em&gt;&lt;/sup&gt;-norm (at least not without something like Approximate Bayesian Computation). I mean, why explore regions of the parameter space that are unlikely to yield simulations that match up with the data? If you’re going to simulate a bunch of simulations, it should be done with the aim of not just finding the most probable values but characterising uncertainty in the parameters. A grid of values representing a very structured form of non-random prior won’t give you that. Finding the maximum with some sort of gradient-based method will give you the most probable values but, again, doesn’t characterise uncertainty. Sometimes we don’t care about that uncertainty, but when we do we’re far better off using statistics and using it properly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2014/09/24/posterior-samples/</link>
      <pubDate>Wed, 24 Sep 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/09/24/posterior-samples/</guid>
      <description>&lt;p&gt;SEB113 students really seemed to enjoy looking at mathematical modelling last week. &lt;a href=&#34;http://books.google.com.au/books?id=nM_X1PTccloC&amp;amp;lpg=PA111&amp;amp;ots=dCzeaDpIeb&amp;amp;dq=lotka%20volterra%20fulford%20and%20barnes&amp;amp;pg=PA106#v=onepage&amp;amp;q=lotka%20volterra%20fulford%20and%20barnes&amp;amp;f=false&#34;&gt;The Lotka-Volterra equations&lt;/a&gt; continue to be a good teaching tool. A student pointed out that when reviewing the limit idea for derivatives it’d be useful to show it with approximating the circumference of a circle using a polygon. So I knocked this up: &lt;a href=&#34;https://samcliffordinfo.files.wordpress.com/2014/09/approximations.png&#34;&gt;&lt;img src=&#34;approximations.png?w=300&#34; alt=&#34;approximations&#34; /&gt;&lt;/a&gt; Are you interested in big data and/or air quality? &lt;a href=&#34;https://www.qut.edu.au/research/our-research/student-topics/big-data-and-nanoparticles-modelling-complex-spatio-temporal-variation&#34;&gt;Consider doing a PhD with me&lt;/a&gt;. This week I showed in the workshop how Markov chains are a neat application of linear algebra for dealing with probability. We used &lt;a href=&#34;http://setosa.io/blog/2014/07/26/markov-chains/&#34;&gt;this interactive visualisation&lt;/a&gt; to investigate what happens as the transition probabilities change. Zoubin Ghahramani has written &lt;a href=&#34;http://rsta.royalsocietypublishing.org/content/371/1984/20110553.abstract&#34;&gt;a really nice review paper&lt;/a&gt; of Bayesian non-parametrics that I really recommend checking out if you’re interested in the new modelling techniques that have been coming out in the last few years for complex data sets. &lt;a href=&#34;http://www.wired.com/2014/09/exercism/&#34;&gt;Exercism.io&lt;/a&gt; is a new service for learning how to master programming by getting feedback on exercises.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>That feeling when former students contact you</title>
      <link>/./2014/09/02/that-feeling-when-former-students-contact-you/</link>
      <pubDate>Tue, 02 Sep 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/09/02/that-feeling-when-former-students-contact-you/</guid>
      <description>&lt;p&gt;Last year I had a student in SEB113 who came in to the subject with a distaste for mathematics and statistics; they struggled with both the statistical concepts and the use of R throughout the semester and looked as though they would rather be anywhere else during the collaborative workshops. This student made it to every lecture and workshop though and came to enjoy the work of using R for statistical analysis of data; and earned a 7 in the unit.&lt;/p&gt;
&lt;p&gt;I just got an email from them asking for a reference for their VRES (Vacation Research Experience Scheme) project application. Not only am I proud of this student for working their butt off to get a 7 in a subject they disliked but came to find interesting, but I am over the moon to hear that they are interested in undertaking scientific field research. This student mentions how my “passion for teaching completely transformed my (their) view of statistics”, and their passion for the research topic is reflected in the email.&lt;/p&gt;
&lt;p&gt;This sort of stuff is probably the most rewarding aspect of lecturing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lotka-Volterra and Bayesian statistics and teaching</title>
      <link>/./2014/08/27/lotka-volterra-and-bayesian-statistics-and-teaching/</link>
      <pubDate>Wed, 27 Aug 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/08/27/lotka-volterra-and-bayesian-statistics-and-teaching/</guid>
      <description>&lt;p&gt;One of the standard population dynamics models that I learned in my undergrad mathematical modelling units was the Lotka-Volterra equations. These represent a very simple set of assumptions about populations, and while they don’t necessarily give physically realistic population trajectories they’re an interesting introduction to the idea that differential equations systems don’t necessarily have an explicit solution. The assumptions are essentially: prey grow exponentially in the absence of predators, predation happens at a rate proportional to the product of the predator and prey populations, birth of predators is dependent on the product of predator and prey populations, predators die off exponentially in the absence of prey. In SEB113 we cover non-linear regressions, the mathematical models that lead to them, and then show that mathematical models don’t always yield a nice function. We look at equilibrium solutions and then show that we orbit around it rather than tending towards (or away from) it. We also look at what happens to the trajectories as we change the relative size of the rate parameters. Last time we did the topic, I posted about using the logistic growth model for our Problem Solving Task and it was pointed out to me that the model has a closed form solution, so we don’t explicitly need to use a numerical solution method. This time around I’ve been playing with using Euler’s method inside JAGS to fit the Lotka-Volterra system to some simulated data from sinusoidal functions (with the same period). I’ve put a bit more effort into the predictive side of the model, though. After obtaining posterior distributions for the parameters (and initial values) I generate simulations with lsode in R, where the parameter values are sampled from the posteriors. The figure below shows the median and 95% CI for the posterior predictive populations as well as points showing the simulated data. &lt;a href=&#34;https://samcliffordinfo.files.wordpress.com/2014/08/lv2.png&#34;&gt;&lt;img src=&#34;lv2.png?w=300&#34; alt=&#34;lv&#34; /&gt;&lt;/a&gt;The predictions get more variable as time goes on, as the uncertainty in the parameter values changes the period of the cycles that the Lotka-Volterra system exhibits. This reminds me of a chat I was having with a statistics PhD student earlier this week about sensitivity of models to data. The student’s context is clustering of data using overfitted mixtures, but I ended up digressing and talking about Edward Lorenz’s discovery of chaos theory through a meteorological model that was very sensitive to small changes in parameter values. The variability in the parameter values in the posterior give rise to the same behaviour, as both Lorenz’s work and my little example in JAGS involve variation in input values for deterministic modelling. Mine was deliberate, though, so isn’t as exciting or groundbreaking a discovery as Lorenz but we both come to the same conclusion: forecasting is of limited use when your model is sensitive to small variations in parameters. As time goes on, my credible intervals will likely end up being centred on the equilibrium solution and the uncertainty in the period of the solution (due to changing coefficient ratios) will result in very wide credible intervals. It’s been a fun little experiment again, and I’m getting more and more interested in combining statistics and differential equations, as it’s a blend of pretty much all of my prior study. The next step would be to use something like MATLAB with a custom Gibbs/Metropolis-Hastings scheme to bring in more of the computational mathematics I took. It’d be interesting to see if there’s space for this sort of modelling in the Mathematical Sciences School’s teaching programs as it combines some topics that aren’t typically taught together. I’ve heard murmurings of further computational statistics classes but haven’t been involved with any planning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper helicopters</title>
      <link>/./2014/08/15/paper-helicopters/</link>
      <pubDate>Fri, 15 Aug 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/08/15/paper-helicopters/</guid>
      <description>&lt;p&gt;There is no textbook for SEB113 - Quantitative Methods in Science. It’s not that we haven’t bothered to prescribe one, it’s that no one seemed to be taking the same approach we decided upon two years ago when the planning for the unit started. There are books on statistics for chemistry, statistics for ecology, statistics for physics, statistics for mathematics, etc. but trying to find a general “statistics for science” book that focuses on modelling rather than testing has been difficult. That said, there are some amazing resources out there if you know where to look, not just for learning statistics but for teaching statistics. One of the most useful that we’ve come across is “&lt;a href=&#34;http://ukcatalogue.oup.com/product/9780198572244.do&#34;&gt;Teaching Statistics&lt;/a&gt;”, by Andrew Gelman and Deborah Nolan. The book itself is full of advice for things like groupwork, topic order, structure of learning activities, etc. but my favourite thing so far is the paper helicopter experiment. &lt;a href=&#34;http://www.tandfonline.com/doi/pdf/10.1080/08982119208918925&#34;&gt;George Box&lt;/a&gt; introduced engineering students to statistical design with the paper helicopter. The experiment itself is quite simple and motivates the idea of using statistics to optimise some design by varying the dimensions of the helicopter. As an activity, it’s a fun way to collect some data that can be used in analysis. By dividing the class up into groups and getting each group to do one or two different designs it’s possible to collect quite a large amount of data, with replication used to identify any group-level effects that may be explaining the variation within the data. There’s a great &lt;a href=&#34;http://www.tandfonline.com/doi/abs/10.1198/000313005X70777#.U-1UePmSx8E&#34;&gt;paper by David Annis&lt;/a&gt; which simplifies the experiment by only varying the length and width of the helicopter’s “blade”, and explains that fitting polynomials and their interactions may yield a regression surface which explains the variation but ignores the physics of the helicopter. In a class teaching statistics to scientists, any chance to tie the statistics to some scientific context must be leapt upon. One of the biggest challenges in teaching statistics is making it relevant to students so that it doesn’t come across as a dry technique for crunching numbers but as a way of probing deeper into a scientific question. I was discussing the experiment with a colleague yesterday who mentioned that when she was learning mathematics at university as part of her science degree it was at the hands of mathematicians who were teaching the unit as if it were for other mathematicians. It was only at the end of the course that a lecturer said “Oh, and by the way, these methods can be used to analyse scientific data”. This is the message that needs to come at the start of the class. Statistics (and more generally, mathematics) gives the scientist a set of tools to ask questions of their data. Being able to ask the right question is therefore very valuable. Ignoring the science means you’re throwing out all the hard work that went into the experimental design and data collection. This is one reason we focus so much on modelling instead of testing. Stopping your analysis at ANOVA doesn’t do justice to your data. Annis’ paper shows the derivation of a mathematical model of the motion of the helicopter from force balance equations, terminal velocities and rotational inertia. This mathematical model is then converted to a non-linear regression model. In SEB113 we cover non-linear regression after linear regression and then show where the regression models come from with a week of mathematical modelling. Even though students enrolled in the unit may not have Senior Maths B (assumed knowledge, rather than a formal pre-requisite) many enjoy peeking behind the curtain to see where the models come from. More than that, they are learning that application-specific non-linear models include what is known about the particular application. We explain first order compartment models (pharmacokinetics), asymptotic growth (ecology), the biexponential model (biology) and logistic growth (ecology) models and show the mathematical models that lead to their existence. We’ve also shown the Lotka-Volterra equations (ecology) in the past as an example of emulating a system, which students seem to enjoy (some are even comforted by the idea that there’s no exact solution). This year we’ll be adding the paper helicopter model to the mix, performing the experiment in week 5’s workshops and analysing the data in the Problem Solving Tasks. I’ll try to get some feedback on whether the students enjoy the experiment and can understand and complete the outcomes; I think it’s neat, but does it appeal to them? I really like that in the past we’ve had students who feel ownership of their data by collecting it in SEB114 - Experimental Science and analysing it in SEB113. SEB114 doesn’t run in second semester, though, so we have had to figure out ways of collecting data for analysis and I think the helicopter’s the best one we’ve come up with yet. We’ve modified the design found in Annis’ paper and I’ve used Adobe InDesign to come up with a printable A4 design where students don’t have to do any measuring, just cutting, folding and paper clipping. We have 12 designs available to us, which gives us a lot of flexibility when it comes to parallelising the experiment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior Samples</title>
      <link>/./2014/07/30/posterior-samples/</link>
      <pubDate>Wed, 30 Jul 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/07/30/posterior-samples/</guid>
      <description>&lt;p&gt;Interested in collaborative use of R, MATLAB, etc. for analysis and visualisation within a webpage? &lt;a href=&#34;http://nbviewer.ipython.org/gist/msund/403910de45e282d658fa&#34;&gt;Combining plotly and iPython&lt;/a&gt; can help you with that. Cosmopolitan (yes, &lt;em&gt;that&lt;/em&gt; Cosmopolitan) has &lt;a href=&#34;http://www.cosmopolitan.com/career/news/a29534/get-that-life-emily-graslie-science/&#34;&gt;a great article&lt;/a&gt; interviewing Emily Graslie, Chief Curiosity Officer at the Field Museum in Chicago. She discusses being an artist and making the transition into science, science education and YouTube stardom. A few of the PhD students in my lab have asked if I could run an introduction to R session. I’d mentioned the CAR workshop that I’d be doing but the cost had put them off. Luckily, there are alternatives like &lt;a href=&#34;https://www.datacamp.com/courses/introduction-to-r&#34;&gt;Datacamp&lt;/a&gt;, &lt;a href=&#34;https://www.coursera.org/&#34;&gt;Coursera&lt;/a&gt; and &lt;a href=&#34;http://www.lynda.com/R-tutorials/R-Statistics-Essential-Training/142447-2.html&#34;&gt;Lynda&lt;/a&gt;. Coursera’s next round of “Data Science”, delivered by Johns Hopkins University, starts next Monday (Course 1 - &lt;a href=&#34;https://www.coursera.org/specialization/jhudatascience/1&#34;&gt;R Programming&lt;/a&gt;). So get in there and learn some R! I’m considering recommending some of these Coursera courses to my current SEB113 students who want to go a bit further with R, but the approach that they take in these online modules is quite different to what we do in SEB113 and I don’t want them to confuse themselves.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2014/07/02/posterior-samples/</link>
      <pubDate>Wed, 02 Jul 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/07/02/posterior-samples/</guid>
      <description>&lt;p&gt;ARC Discovery Projects have been returned to their authors, and we are putting our responses together for the rejoinders. Interesting to see that we got a comment suggesting that we use the less restrictive CC-by instead of CC-by-nc-sa as we’d suggested. We weren’t successful in our Linkage Project applications, which is disappointing as they were interesting projects (well, we thought so). Continuing to bring research funding in is an ongoing struggle for all research groups and I feel it’s only going to get harder as the new federal government’s research priorities appear to be more aligned to medical science that delivers treatments than to our group’s traditional strengths. SEB113 is pretty much completely over for the semester, with marks having been entered for almost every student. Overall I think the students did fairly well. We had some issues with the timetable this semester. Ideally, we’d like the Lecture, then all of the computer labs, then all of the workshops, so that we can introduce a statistical idea, show the code and then apply the idea and code in a group setting. Next semester, we have the lecture followed immediately by the workshops with the computer labs dotted throughout the remainder of the week. This has provided us with an opportunity to try some semi-flipped classroom ideas, where students are able/expected to do the computer lab at home at their own pace rather than watch a tutor explain it one line at a time at the front of a computer lab. I’m teaching part of a &lt;a href=&#34;http://www.eventbrite.com.au/e/r-statistical-language-for-air-pollution-epidemiology-tickets-12043581677&#34;&gt;two day course&lt;/a&gt; on the use of R in air pollution epidemiology. My part will introduce Bayesian statistics with a brief overview, a discussion about prior distributions as a means of encoding &lt;em&gt;a priori&lt;/em&gt; beliefs about model parameters, and discuss the use of Bayesian hierarchical modelling (as opposed to more traditional ANOVA techniques) as a way of making the most of the data that’s been collected. The other two presenters are &lt;a href=&#34;http://researchers.uq.edu.au/researcher/2181&#34;&gt;Dr Peter Baker&lt;/a&gt; and &lt;a href=&#34;http://researchers.uq.edu.au/researcher/2530&#34;&gt;Dr Yuming Guo&lt;/a&gt;. The course is being run by the CAR-CRE, who partially fund my postdoctoral fellowship. I had meant to post this back when they were doing the rounds, but there’s &lt;a href=&#34;http://www.tylervigen.com/&#34;&gt;a bunch of plots&lt;/a&gt; that attempt to show that correlation isn’t causation and that spurious correlations exist in large data sets. &lt;a href=&#34;https://tom-christie.github.io/articles/correlation/&#34;&gt;Tom Christie has responded&lt;/a&gt; to this by going over the fact that correlation in time series isn’t as simple as in the case of independent, identically distributed data. One should be careful that one’s criticism of bad statistics is itself founded on good statistics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The ongoing crusade against Excel-based analysis</title>
      <link>/./2014/05/14/the-ongoing-crusade-against-excel-based-analysis/</link>
      <pubDate>Wed, 14 May 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/05/14/the-ongoing-crusade-against-excel-based-analysis/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;One of the things I catch myself saying quite often in SEB113 is “This is new. It’s hard. But remember, you weren’t born knowing how to walk. You learned it”, as my way of saying that it’s okay to not understand this straight away, it takes time, practice and determination. I often say this in response to students complaining about learning R to do their data analysis. It’s actually got to the point where t&lt;/span&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;he unit co-ordinator suggested I get a t-shirt printed with “You weren’t born knowing how to walk” on the front and “So learn R” on the back.&lt;/span&gt; One of the reasons I’m so keen to push new students into learning R is that while Excel can do some of the simpler calculations required in the first year of a science degree it is often completely inadequate for doing data analysis as a professional scientist, or even in an advanced level university course. I actually saw a senior researcher in a 3 day Bayesian statistics course try to avoid using R to code a Gibbs sampler by getting it up and running in Excel. They managed it, but it took minutes to run what the rest of us could compute in a second (and it was for a trivially simple problem). There are &lt;a href=&#34;http://www.asq904.org/StatisticalFlawsInExcel.pdf&#34;&gt;problems with Excel&lt;/a&gt;, such as its inability to deal with the standard deviation of a group of very large numbers due to its bizarre formulation. Apparently the secret to sane use of Excel is to &lt;a href=&#34;http://www.r-bloggers.com/excel-fanaticism-and-r/&#34;&gt;only use it for data storage&lt;/a&gt;. This guiding principle has meant that I no longer manipulate my data in Excel. Even with time stamp information I’ll fire up the &lt;a href=&#34;http://cran.r-project.org/web/packages/lubridate/index.html&#34;&gt;lubridate&lt;/a&gt; package to convert from one format to another. I’m slowly exploring the &lt;a href=&#34;http://blog.datascienceretreat.com/&#34;&gt;Hadleyverse&lt;/a&gt; and that sort of approach is filtering through into SEB113 where we’re teaching the use of ggplot2 and reshape2 within RStudio. These are all powerful tools that simplify data analysis and avoid the hackish feel that much Excel-based analysis has, where pivot tables are a thing and graphs are made by clicking and dragging a selection tool down the data (which can lead to &lt;a href=&#34;http://en.wikipedia.org/wiki/Growth_in_a_Time_of_Debt&#34;&gt;some nasty errors&lt;/a&gt;). The fact that these powerful tools that make data analysis simple are free is another reason to choose R over Excel. I’m not on the “Open Source Software and provision of all code is mandatory” bandwagon as others seem to be when it comes to analysis being replicable. I agree it’s a worthwhile goal but it’s not a priority for me. That said, though, I definitely support encouraging the use of free software (in both senses) in education on the grounds of equity of access. I had a chat with some students in SEB113 yesterday about why we’re teaching everything in R given that the SEB114 staff use a combination of Excel, MATLAB (and maybe even other packages I don’t know about). If we were to teach analysis the way that the SEB114 lecturers do it themselves, we’d have to teach multiple packages to multiple disciplines. Even discounting the fact that everything we teach is implemented in R, that R is free (unlike Excel and MATLAB), cross-platform (Excel on Linux? Try OpenOffice/OfficeLibre) and extensible (MATLAB has toolboxes, Excel has add-ins, R has a nice package manager) was a big plus for students who said that being able to work on assignments at home was valuable and so paying for software would make study difficult. Convincing students to use R can be difficult, especially if they have no programming background, but ultimately they seem to accept that R is powerful, can do more than Excel and that writing reusable code makes future analysis easier. Convincing SEB114 academics that teaching their students to use R is a good idea is probably a harder sell, given that they’ve got years of experience with other tools. It’s still only semester 3 of the new Bachelor of Science course so we’ll have to see how this plays out over the years to come.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2014/04/26/posterior-samples/</link>
      <pubDate>Sat, 26 Apr 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/04/26/posterior-samples/</guid>
      <description>&lt;p&gt;I’m teaching science students how to do statistics. It would be great if we could turn them into Bayesians, especially seeing as we’ve just covered the Agresti-Coull correction for estimating proportions from small experiments. &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/research/published/teachingbayes.pdf&#34;&gt;Andrew Gelman has an interesting paper&lt;/a&gt; on teaching Bayesian statistics to non-statisticians that focuses on the delivery of skills rather than concepts. I would definitely agree with his approach, especially when you consider how he stresses that discussing the model is probably the most important part. NASA have done some work simulating global aerosols and it’s been compiled into &lt;a href=&#34;http://www.itsokaytobesmart.com/post/82630633966/one-of-my-favorite-gifs-of-one-of-my-favorite-nasa&#34;&gt;a neat video&lt;/a&gt; (via &lt;a href=&#34;http://www.itsokaytobesmart.com/&#34;&gt;It’s Okay To Be Smart’s Joe Hanson&lt;/a&gt;). CSIRO have been doing some interesting stuff looking at the production of organic aerosols as well, so this is something I’m paying a bit more attention to at the moment. &lt;a href=&#34;https://www.datacamp.com/&#34;&gt;Datacamp&lt;/a&gt; is a set of online labs for learning to use R, covering the basics of R, data analysis and statistical inference, and computational finance and econometrics. &lt;a href=&#34;https://medium.com/of-games-and-code/d90a50c5d58e&#34;&gt;Learn to be a better coder&lt;/a&gt; by improving your communication skills. The most practical (in terms of coding, at least) aspect of this includes using meaningful names and writing comments that describe what the code does when it’s not clear from the code itself.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Combining differential equations and regression</title>
      <link>/./2014/04/22/combining-differential-equations-and-regression/</link>
      <pubDate>Tue, 22 Apr 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/04/22/combining-differential-equations-and-regression/</guid>
      <description>&lt;p&gt;Last week I gave my first lecture for the semester to the SEB113 students. While they tend to not have a particularly strong mathematics background I got some very positive feedback on how much they enjoyed learning about mathematical modelling. We revised differentiation, what derivatives are and then jumped into a bit about formulating differential equations from words that represent the assumptions that the model makes. The bulk of that week’s lecture is showing where the non-linear regression models we used in the previous week (first order compartment, asymptotic, biexponential) come from. To do this we have a chat about exponential growth and decay models as some of the easiest differential equation models to deal with. I show them how we solve the exponential model exactly and then make reference to the fact that I don’t expect them to solve these equations in this subject. We show the solutions to the DE systems and make it very clear that the non-linear regression models are the solutions to differential equations that represent different assumptions. We finish the lecture off with a section on how we can’t always get a “pen and paper” solution to differential equations and so sometimes we either simplify the system to one we can solve (alluding to perturbation methods) or give it to a numerical solver (alluding to computational mathematics). Because it’s how I learned about numerical solutions to DEs I showed the students the Lotka-Volterra model and discussed why we can’t solve X(t) and Y(t) and so have to use numerical methods. For different parameter values we get variations on the same behaviour: cyclic patterns, prey population growth followed by predator population growth followed by overconsumption of prey leading to fewer predators being born to replace the dying. Many students seemed to enjoy investigating this model in the workshops, as it’s quite different to everything we’ve learned so far. Solution is via the deSolve package in R but we introduce the students to Euler’s method and discuss numerical instability and the accumulation of numerical error. I finish off the lecture with a chat about how regression tends to make assumptions about the form of the mean relationship between variables so we can do parameter estimation and that differential equations give us a system we can solve to obtain that mean relationship. I state that while we &lt;em&gt;can&lt;/em&gt; solve the DE numerically while simultaneously estimating the parameter it is way outside the scope of the course. I had a bit of time this morning to spend on next week’s lecture material (linear algebra) so decided to have a go at numerical estimation for the logistic growth model and some data based on the Orange tree circumference data set in R with JAGS/rjags. It’s the first time I’ve had a go at combining regression and numerical solutions to DEs in the same code, so I’ve only used Euler’s method. That said, I was very happy with the solution and the code is provided below the cut. [code language=“r”] # euler.bugs model{ y[1] ~ dnorm(mu[1], tau.y) mu[1] &amp;lt;- y0 + dt * exp(lr) * y0 * (1 - y0/K) for (i in 2:n){ y[i] ~ dnorm(mu[i], tau.y) mu[i] &amp;lt;- y[i-1] + dt * exp(lr) * y[i-1] * (1 - y[i-1]/K) } for (i in 1:n){ y.p[i] ~ dnorm(mu[i], tau.y) } lr ~ dnorm(0, 1e-6) K ~ dnorm(0, 1e-6) y0 ~ dunif(0.001, 1000) tau.y ~ dgamma(0.001, 0.001) } [/code] Which can be called appropriately with [code language=“r”] library(rjags) library(ggplot2) my.orange &amp;lt;- data.frame(age=seq(100, 1900, by=200), circumference = c(32, 47, 73, 101, 134, 162, 182, 194, 205, 214)) dt &amp;lt;- 10 orange.dat &amp;lt;- data.frame(age=seq(0, 3000, by=dt),circumference=NA) orange.dat[match(table=orange.dat&lt;span class=&#34;math inline&#34;&gt;\(age, x=my.orange\)&lt;/span&gt;age),“circumference”] &amp;lt;- my.orange[, “circumference”] orange.m &amp;lt;- jags.model(file=“euler.bugs”, data=list(y=orange.dat&lt;span class=&#34;math inline&#34;&gt;\(circumference, n = nrow(orange.dat), dt=dt),inits=list(y0=50, K=500, lr=-5)) orange.b &amp;lt;- jags.samples(model=orange.m, n.iter=10000, variable.names=c(&amp;quot;y.p&amp;quot;,&amp;quot;K&amp;quot;,&amp;quot;lr&amp;quot;,&amp;quot;y0&amp;quot;)) orange.pred &amp;lt;- coda.samples(model=orange.m, n.iter=1000, variable.names=c(&amp;quot;y.p&amp;quot;)) orange.sum &amp;lt;- summary(orange.pred, q=c(0.025, 0.5, 0.975)) orange.gg &amp;lt;- data.frame(orange.sum\)&lt;/span&gt;quantiles) orange.gg&lt;span class=&#34;math inline&#34;&gt;\(age &amp;lt;- orange.dat\)&lt;/span&gt;age windows(height=3.5) ggplot(data=orange.gg, aes(x=age, y=X50.)) + geom_line() + geom_line(aes(y=X2.5.), lty=2) + geom_line(aes(y=X97.5.), lty=2) + geom_point(data= my.orange, aes(y=circumference), alpha=0.5) + theme_bw() + xlab(“Time (days)”) + ylab(“Tree circumference (mm)”) [/code] The resulting picture can be seen below. [caption id=“attachment_1226” align=“aligncenter” width=“625”][&lt;img src=&#34;predicting.png?w=625&#34; alt=&#34;Prediction of tree circumference from logistic growth differential equation&#34; /&gt;](&lt;a href=&#34;http://samcliffordinfo.files.wordpress.com/2014/04/predicting.png&#34; class=&#34;uri&#34;&gt;http://samcliffordinfo.files.wordpress.com/2014/04/predicting.png&lt;/a&gt;) Prediction of tree circumference from logistic growth differential equation[/caption]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A few R things</title>
      <link>/./2014/04/10/a-few-r-things/</link>
      <pubDate>Thu, 10 Apr 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/04/10/a-few-r-things/</guid>
      <description>&lt;p&gt;“&lt;a href=&#34;http://hackerretreat.com/r-good-parts/&#34;&gt;R: The Good Parts&lt;/a&gt;” is an attempt to showcase the best way to do things in R. I’m not yet at the stage of dealing with absolutely massive data sets but things will be heading that way for me if aerosol samplers continue to measure at higher frequencies. Left out of the article is a discussion of &lt;a href=&#34;http://blog.rstudio.org/2014/01/17/introducing-dplyr/&#34;&gt;dplyr&lt;/a&gt;; I’m still using functions from the apply family! Maybe I should also get used to using &lt;a href=&#34;http://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.pdf&#34;&gt;data.table&lt;/a&gt;. (Update: I’m now using data.table and its syntax to apply functions across grouping levels that I’ve set as keys. This is amazing). While we’ve been incorporating a few of the mathematical needs of SEB114 into SEB113 it looks like we may need to go a bit further with incorporating the R needs. I hadn’t really thought about plotting a specific function (other than a line y = ax + b) in the workshops but it looks like a few earth sciences students need to plot the function π x / (1+x)&lt;sup&gt;2&lt;/sup&gt;. So we’ll have to take stock over the next six months of what the experimental science lecturers want to put in their units and how we can help support that (also how we can get the science lecturers to help reinforce statistical modelling over statistical testing).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Timetabling and the potential for alternative delivery in SEB113</title>
      <link>/./2014/04/07/timetabling-and-the-potential-for-alternative-delivery-in-seb113/</link>
      <pubDate>Mon, 07 Apr 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/04/07/timetabling-and-the-potential-for-alternative-delivery-in-seb113/</guid>
      <description>&lt;p&gt;I’ve been pretty busy writing the analysis plan for the main paper from the UPTECH project and reorganising SEB113 workshops. We’ve had some meetings recently with QUT timetabling people which has led to discussions about how we try to get students to enrol in a sensible pair of workshops and labs for both SEB113 and SEB114. One of the biggest concerns when it comes to these paired subjects is making sure that people attend the labs and workshops in the right order and are working with the same groups across both subjects so that we can structure the teaching material. In SEB113 the preferred order of classes is Lectorial, Computer Lab, Collaborative Workshop. The lecture introduces the topic, the lab shows you how it’s implemented in R and the workshop gets you working in a group with others to solve a problem based on the topic. The problem comes about with QUT’s timetabling software providing a timetable which contains no clashes for the core first year subjects (SEBs 101, 102, 113, 114). Timetabling the lectures/lectorials for these units so that they don’t clash is a task in and of itself and I’m impressed that the timetabling people have managed to make sure these subjects don’t clash (I remember taking two units for the applied physics co-major in the old B App Sc course where the lectures clashed). The non-clashing timetable doesn’t necessarily mean students can enrol in the class order that we would prefer. It’s also unlikely that we can automatically combine a lab-workshop pair as one thing to be enrolled in and it’s impractical to try to get a staff member to enrol students manually. It’s got me thinking a lot about flipped classrooms and other ways of overcoming the timetable difficulty. The benefit of the workshop for students is that they have a group to work with on a big task and they have two tutors to ask for help when they get stuck. I feel like this would be difficult to do outside a classroom without some sort of help-desk queueing system that is only open between certain times (and then you’ve still got the time restrictions). The computer labs can be done individually at any time, though, as they’re about exposure to code rather than solving a particular problem. In this instance, we could probably cut down on the number of computer labs required by encouraging students to do the lab in their own time before their workshop, which is in the spirit of flipped classrooms. The last labs are in week 7 (this week!) which means it’s not going to be an issue much longer this semester. Semester 2 has fewer SEB113 enrolments (SEB114 isn’t offered) so it’s not going to be as big an issue then. Whether we go with changing the timetabling system or we modify computer labs to become programming consults (where to get help you must have attempted the lab) is something we can deal with a bit later. With the use of Echo360 being made mandatory in all lectures at QUT the availability of recorded lectures makes it easier for students to go through the material at their own pace. With so many students in the subject, there’s a large number of person hours which go into content delivery. I’m not sure we’re using that resource (labour) as effectively as we can, and changing the way we deliver the subject may help that.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Science in context - my context</title>
      <link>/./2014/03/05/science-in-context---my-context/</link>
      <pubDate>Wed, 05 Mar 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/03/05/science-in-context---my-context/</guid>
      <description>&lt;p&gt;One of the first year units that QUT has introduced in the new Bachelor of Science program is &lt;a href=&#34;http://www.qut.edu.au/study/unit-search/unit?unitCode=SEB101&amp;amp;idunit=44497&#34;&gt;SEB101 - Science in Context&lt;/a&gt;. The subject aims to impress upon students the idea that science happens as part of a larger community and that how and why research is conducted relies on interactions with that community. I received an email last week from a former SEB113 student of mine, Kathryn Turner, asking if she could interview me for the SEB101 Portfolio about the work I do as research scientist. Kathryn and I organised to sit down and have a chat for this afternoon to discuss what I do, what relevance it has to the community and how the community sees the work we do. We spent most of our fifteen minutes talking about the UPTECH project and how my work, statistical analysis for the various papers, is part of a large, interdisciplinary project that allows me to work with many different sources of data and do different analyses. I mentioned that I initially studied mathematical modelling, focussing on computational fluid dynamics, and that I got involved in this research project because my primary supervisor (Professor Lidia Morawska) lectured an elective that I took in my undergrad (Global Energy Balance and Climate Change). I went to have a chat with her after I’d finished Honours about what sort of PhD projects she might have available (writing about this now, it feels like a lifetime ago; it was only 2008) and she was in the process of planning UPTECH and recruiting people. I was offered the chance to apply cool mathematical techniques to an interesting environmental health problem based that had links to transport planning. Sign me up! We also talked about the ethics side of the project, involving doing health diagnostic measurements with students, taking a health history and demographics survey home, etc. and how QUT makes sure we’re very careful with this sort of thing. I’m glad I didn’t have to do the ethics application for the project. Kathryn asked what the schools thought about having scientists come in and work with the kids. From what I understand, the schools were quite accepting and the kids were excited about the prospect of being involved with the personal sampling aspect; we also handed out badges that say “I’m doing SCIENCE” to the kids who were part of the study. On a bit of a tangent, and we didn’t discuss this, I think it’s good to have scientists seen as being regular people who have decided to pursue science and that the science isn’t just lab work. FermiLab did &lt;a href=&#34;http://ed.fnal.gov/projects/scientists/&#34;&gt;a really interesting project&lt;/a&gt; a few years ago about kids’ perceptions of scientists. They talked to some seventh graders and got them to describe and draw what they thought a scientist was before and after meeting a group of physicists who worked at the lab. The UPTECH members who went to the schools to do the measurements represent a very multicultural group, including (but not limited to) people of Iranian, Chinese, Egyptian, Malaysian, and Northern and Eastern European descent, and included both men and women. I hope that one of the outcomes of having such a diverse group involved with the field work for the project was that the students saw that scientists aren’t all old, white men with frizzy, greying hair, a lab coat and glasses. &lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;After we’d wrapped up the interview, Kathryn said it was interesting to learn a bit more about the research career of a lecturer and seemed quite interested in the various projects that I get to work on. For my part, I found it a really interesting interview because I don’t often get asked about the ethical and community implications of my work. While I do spend my days sitting in front of a computer running statistical analyses, I am actually a research scientist who relies on the support of the public both through my funding and through social acceptance that looking at the health impact of air quality is valuable.&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>More on the use of R and ggplot in SEB113</title>
      <link>/./2013/11/20/more-on-the-use-of-r-and-ggplot-in-seb113/</link>
      <pubDate>Wed, 20 Nov 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/11/20/more-on-the-use-of-r-and-ggplot-in-seb113/</guid>
      <description>&lt;p&gt;One of the benefits of waking up stupidly early is that you can organise to have coffee and a chat with a friend before work and still get there on time. A friend who I haven’t seen in a while contacted me recently to ask a few questions about learning R for data analysis and visualisation. While they won’t need to formally learn statistics and visualisation for their work it certainly doesn’t hurt to be able to generate better analysis of data and make more informative and easy to interpret graphs. My friend hasn’t done any statistics since high school Maths B, approximately ten years ago, which makes them similar to many of my SEB113 students. They have done a bit of programming along the way as a hobby, which will of course be a huge help. Having downloaded R and had a crack at a ggplot2 tutorial, they were confident that they &lt;em&gt;could&lt;/em&gt; learn what was going on even though they didn’t really understand what was going on in the tutorial. We sat down with the tutorial and some avocado on toast and worked through what the arguments for each function represented and what the data frame was made of, how ggplot has a grammar of graphics and how we can continue to add elements to the code to change the plot. To an extent, the ability to work through but not explain what some code is doing is typical of an SEB113 student in the first half of the subject (where we provide the code and get them to run it). It’s not until later in the semester, when the computer labs stop, that we expect that they can turn their ideas into code (and they’re welcome to cannibalise the code we provide) to write their quantitative workbooks. I suggested the &lt;a href=&#34;http://samclifford.info/2013/11/11/coursera-courses-on-statistics/&#34; title=&#34;Coursera courses on statistics&#34;&gt;Coursera course that started yesterday&lt;/a&gt; as a way to get a bit more familiar with how R works and get recognition of the completion of the course (which isn’t a recognised qualification but is evidence of being interested enough to pursue it). These days I’m always on the lookout for better ways to introduce SEB113 students to R and ggplot2 and I found the following tutorials (and have passed them on to my friend and the SEB113 teaching team) via Matt Asher’s “&lt;a href=&#34;http://www.statisticsblog.com/2013/11/the-week-in-stats-nov-18th-edition/&#34;&gt;Statistics Blog&lt;/a&gt;” and I have copied and pasted the text directly:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Martin Johnsson wrote a series of five well-written tutorials called &lt;em&gt;A slightly different introduction to R,&lt;/em&gt; with tips for beginner R users. Here are the links to parts &lt;a href=&#34;http://bit.ly/1fzSWRq&#34;&gt;I&lt;/a&gt;, &lt;a href=&#34;http://bit.ly/18pRHiz&#34;&gt;II&lt;/a&gt;, &lt;a href=&#34;http://bit.ly/19if90E&#34;&gt;III&lt;/a&gt;, &lt;a href=&#34;http://bit.ly/183o9eb&#34;&gt;IV&lt;/a&gt; and &lt;a href=&#34;http://bit.ly/1i7x0j0&#34;&gt;V&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I had no idea that the &lt;a href=&#34;http://cran.r-project.org/web/packages/coefplot/index.html&#34;&gt;coefplot&lt;/a&gt; package existed! That’s going to make visualisation of fitted linear models much easier for our students, as we’ve previously had them using geom_segment to manually plot estimates and confidence intervals. This is part of what I love about R, compared to, say, SAS. There’s a huge community of people working out there to add extra functionality to an open source project by building on each others’ work. GGally and coefplot both require ggplot2 and have got a lot of really nice functions that extend the publication quality graphics of ggplot2. The community is quite active and if you can think of a question for R there’s probably an answer out there already.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coursera courses on statistics</title>
      <link>/./2013/11/11/coursera-courses-on-statistics/</link>
      <pubDate>Mon, 11 Nov 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/11/11/coursera-courses-on-statistics/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;I was looking into Coursera courses for a colleague who wants their postgrad students to have a stronger background in statistics. These students are coming from a background of having done science but not necessarily taken any mathematics or statistics electives in a while. It’d be good for them to learn R for their data analysis, too, seeing as most other statistics packages cost money and Excel, while readily available within universities and other research laboratories, is utterly terrible for proper data analysis.&lt;/span&gt; I managed to find the following courses which look relevant to the students and perhaps might even be good for any SEB113 students seeking to consolidate their statistical knowledge. This may also be relevant for current researchers looking to refamiliarise themselves with statistics and learn a good software package while they’re at it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/course/biostats&#34;&gt;Mathematical Biostatistics Boot Camp 1&lt;/a&gt;. Starting November 18 2013 and running for seven weeks. This requires some knowledge of calculus but looks like a really good course. Uses R.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/course/compdata&#34;&gt;Computing for Data Analysis&lt;/a&gt;. Starting January 6 2014 and running for four weeks. Uses R.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/course/statreasoning&#34;&gt;Statistical Reasoning for Public Health: Estimation, Inference, &amp;amp; Interpretation&lt;/a&gt;. Starting January 21 2013 and running for eight weeks.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/course/statistics&#34;&gt;Data Analysis and Statistical Inference&lt;/a&gt;. Starting February 17 2013 and running for ten weeks. Uses R.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These courses are all free and some of them provide you with a statement of accomplishment upon successful completion, which could be a useful addition to your CV if you want to show prospective employers that you’re enthusiastic about learning and applying statistics. Edit: there’s also &lt;a href=&#34;https://novoed.com/numbers-life-quantway&#34;&gt;this&lt;/a&gt; “Numbers for Life” from Novo Ed. Johns Hopkins’ Open Courseware has the following neat courses:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://ocw.jhsph.edu/index.cfm/go/viewCourse/course/introbiostats/coursePage/index/&#34;&gt;Introduction to Biostatistics&lt;/a&gt;. This is a bit like if SEB113 focussed on tests rather than models.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ocw.jhsph.edu/index.cfm/go/viewCourse/course/BiostatisticsLectureSeries05/coursePage/index/&#34;&gt;Biostatistics Lecture Series&lt;/a&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;. A discussion not so much about statistics but the way statistics is practised.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;Methods in Biostatistics&lt;/span&gt; &lt;a href=&#34;http://ocw.jhsph.edu/index.cfm/go/viewCourse/course/MethodsInBiostatisticsI/coursePage/index/&#34;&gt;I&lt;/a&gt; &lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;and&lt;/span&gt; &lt;a href=&#34;http://ocw.jhsph.edu/index.cfm/go/viewCourse/course/methodsinbiostatisticsii/coursePage/index/&#34;&gt;II&lt;/a&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;Statistics for Laboratory Scientists&lt;/span&gt; &lt;a href=&#34;http://ocw.jhsph.edu/index.cfm/go/viewCourse/course/StatisticsLaboratoryScientistsI/coursePage/index/&#34;&gt;I&lt;/a&gt; &lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;and&lt;/span&gt; &lt;a href=&#34;http://ocw.jhsph.edu/index.cfm/go/viewCourse/course/statisticslaboratoryscientistsii/coursePage/index/&#34;&gt;II&lt;/a&gt;&lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ocw.jhsph.edu/index.cfm/go/viewCourse/course/EssentialsProbabilityStatisticalInference/coursePage/index/&#34;&gt;Essentials of Probability and Statistical Inference IV&lt;/a&gt;. GAMs, CARTs, neural networks. Good for postgrad level statisticians.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ben Fitzpatrick also introduced me to &lt;a href=&#34;http://www.lynda.com/&#34;&gt;lynda&lt;/a&gt;, which has a few short courses on statistics and R.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Science as storytelling</title>
      <link>/./2013/11/06/science-as-storytelling/</link>
      <pubDate>Wed, 06 Nov 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/11/06/science-as-storytelling/</guid>
      <description>&lt;p&gt;I used to not be a very confident public speaker. I remember getting up at a community meeting in 2007 and stammering some words out to a group of residents; it was a disaster. Motivated for the desire for some money to augment my Youth Allowance payments I applied to be a tutor with the School of Mathematics (QUT) during my final years of undergrad and found that I became a bit better at talking to people. My Honours seminar was still a nervous affair but it was much less disastrous than the community meeting. After Honours I had a job teaching mathematics to a group of video game programmers, developing the curriculum to suit their needs and interests and it’s here that I became far more comfortable with speaking. I was coming up with my own material and delivering it to people who I knew were interested in it. That’s a world away from teaching university students, where many may not see the point in learning what I’m teaching. This is especially the case in service mathematics and statistics units. During my PhD studies I got interested in improvised theatre as a creative alternative to the mathematics, statistics and science that was my day. My reputation as someone not afraid to get up in front of 100 people and perform lead to my being asked by one of my PhD supervisors if I’d like to be a tutor in the brand new SEB113 course. Teaching students how to use R for their data analysis? Of course I’m interested! After the end of a very enjoyable, if somewhat disjointed, semester I was asked if I’d consider lecturing the smaller second semester re-run. I jumped at the chance. Restructuring the subject from the way it was run in first semester meant we could focus on the way the material flowed and see if we could smooth out some of the jumps in style, making the unit more consistent and easier to understand. We had to do a lot of work rejigging the slides, writing new workshops and computer laboratory worksheets to accommodate the use of ggplot rather than a combination of base, lattice and MASS graphics. The result was a subject that introduces a diverse list of topics in a much more sensible manner:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Measurement and variation&lt;/li&gt;
&lt;li&gt;Visualisation&lt;/li&gt;
&lt;li&gt;Summary statistics and confidence intervals&lt;/li&gt;
&lt;li&gt;Inference and sample size, hypothesis tests&lt;/li&gt;
&lt;li&gt;Regression lines of best fit&lt;/li&gt;
&lt;li&gt;Regression with a categorical variable&lt;/li&gt;
&lt;li&gt;Non-linear regression based on process models&lt;/li&gt;
&lt;li&gt;Multivariate summary statistics and regression&lt;/li&gt;
&lt;li&gt;Mathematical modelling of process models&lt;/li&gt;
&lt;li&gt;Linear algebra, including the guts of how linear regression works&lt;/li&gt;
&lt;li&gt;Writing scientifically, revisiting the scientific method&lt;/li&gt;
&lt;li&gt;Writing about numbers, conditional probability for understanding hypothesis tests&lt;/li&gt;
&lt;li&gt;Revisiting visualisation&lt;/li&gt;
&lt;li&gt;An introduction to advanced quantitative methods&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It became apparent around week 8-9 that what we were doing was telling a story of how to get from calculating means to understanding how to develop a model to either model a process or emulate that process. The discussions about writing scientifically became about how the quantitative reports were telling a story. The first step, the introduction, is like meeting the characters for the first time and understanding their relationships with each other. As we move through the methods and analysis we see the action of the story unfold. The conclusion is the consequences of the action and by relating the analysis back to the motivating aim we can see the arc of the story and understand more about these characters. Now that the teaching is over and the marking of the quantitative workbooks is coming to a close, I’ve got a bit more space in my head to process my thoughts about improvisation and storytelling (we do a weekly show and I’m still doing workshops on the weekend). I’ve picked up a book on my Kindle by scientist-turned-filmmaker Randy Olson, entitled “&lt;a href=&#34;http://www.amazon.com/gp/product/B00FASMHP8/ref=oh_d__o00_details_o00__i00?ie=UTF8&amp;amp;psc=1&#34;&gt;Connection: Hollywood Storytelling meets Critical Thinking&lt;/a&gt;”. Olson’s book is all about how any intellectual topic can be made interesting and accessible by treating its presentation as telling a story. He states that we, as humans, engage with stories far more than we do with dry information as we feel stories it in our hearts, guts and sexual organs rather than just in our brains. Not only is the communication of scientific results storytelling but lecturing is storytelling. I’ve thought this semester that lecturing is definitely a style of performance, but the idea that the topics in a unit should follow an arc and tell a unified story means that part of the academic’s role is telling a story in the classroom. For me, that means that elements of comedy, pantomime and suspense make their way into my lectures. Storytelling in science is a very interesting topic and I look forward to making my way through the remainder of the book over the end of year break, ready to start a new semester with a revised narrative arc and better storyboarding, maybe even a few new characters.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/10/25/posterior-samples/</link>
      <pubDate>Fri, 25 Oct 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/10/25/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.upworthy.com/one-guy-with-a-marker-just-made-the-global-warming-debate-completely-obsolete-7&#34;&gt;Here’s an interesting but simple discussion&lt;/a&gt; of the risk analysis of action on climate change. What I like is that the author encourages the viewer to play around with the outcomes rather than using an appeal to authority that states “this is what the IPCC and other climate scientists say will happen”. it’s a very Bayesian way of asking people to consider risk, not just in the conditional statement sense but in the sense that one’s personal subjective belief is viewed as a welcome addition to the discussion. So much of the climate debate seems to be based on whether or not the IPCC is right; some people don’t like that the IPCC is an international body of elite scientists and so will ignore anything they say. Couching the exercise in terms of “Any reasonable person should be able to concede that they may be wrong about the way the physical world behaves” takes it away from “who is right and who is wrong?”. &lt;a href=&#34;http://www.usq.edu.au/scholarships/usq/australian-postgraduate-award&#34;&gt;USQ has some APA PhD scholarships available&lt;/a&gt;, including scholarships for computational mathematics and climate risk and resource management. So if you’re interested in $30,000 per year (tax free!) to work on some interesting topics check out the USQ site. Problems with the interpretation of p values are &lt;a href=&#34;http://bayesianbiologist.com/2011/08/21/p-value-fallacy-on-more-or-less/&#34;&gt;often due to people treating them as statements about their hypothesis&lt;/a&gt; given some data when they’re really statements about the data given some hypothesis. Any SEB113 students reading would do well to remember this. I’ve also &lt;a href=&#34;http://samclifford.info/2013/10/18/response-to-a-student-on-p-values/&#34; title=&#34;Response to a student on p values&#34;&gt;blogged my response&lt;/a&gt; to an SEB113 student about this. &lt;a href=&#34;http://simplystatistics.org/2013/10/15/teaching-least-squares-to-a-5th-grader-by-calibrating-a-programmable-robot/&#34;&gt;Here’s a cute little video&lt;/a&gt; about using least squares regression to figure out how many revolutions a Lego Mindstorms robot’s wheel needs to turn to make it travel 1m. It’s an interesting use of regression as a calibration technique, which is something we haven’t covered explicitly in SEB113 but I imagine it’d be pretty common in engineering classes. &lt;a href=&#34;http://thesiswhisperer.com/2013/10/23/why-is-grey-literature-not-open-access/&#34;&gt;Interesting post at the Thesis Whisperer&lt;/a&gt; asking why work that’s owned by someone (as in the intellectual property rights are assigned) but not copyrighted by a commercial publisher isn’t more freely available.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Response to a student on p values</title>
      <link>/./2013/10/18/response-to-a-student-on-p-values/</link>
      <pubDate>Fri, 18 Oct 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/10/18/response-to-a-student-on-p-values/</guid>
      <description>&lt;p&gt;My students are working on their 25% assessment pieces, the Quantitative Workbook. These are group assignments that require students do a quantitative analysis from start to finish on some ecology data we’ve given them. A few students are struggling with the p value concept, particularly what it means in the R summary.lm() output. I responded to the student with the following statement. It’s a bit more verbose than I might have liked but I think it’s important to try to step it through from start to finish. It took me ages to get this as an undergrad.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The hypothesis test that R does and gives you in the regression summary asks:&lt;/p&gt;
&lt;p&gt;What is the probability of seeing a test statistic (third column in the output) at least as extreme as what we have if the true value of the parameter were actually zero (this is our null hypothesis)?&lt;/p&gt;
&lt;p&gt;Our best estimates of the parameters given the data we are using with our model (first column in the output) are found by minimising the sum of squares of the errors between the observed values and the fitted values (see the Normal equations slides from the linear algebra week). Our uncertainty about those estimates is given to us with the standard error of the estimate (second column in the output) which is related to the size of the standard deviation of the residuals. More uncertainty in our fitted values reflects uncertainty in our parameter estimates. If the standard error is comparable in size to the estimate, then perhaps our uncertainty may mean we can’t reject the idea that the true value of the parameter is zero (i.e. we may not be able to detect that this variable has an effect).&lt;/p&gt;
&lt;p&gt;The test statistic (third column) is assumed to come from a t distribution whose degrees of freedom is the number of data points we started with minus the number of parameters we’ve observed. The idea of the test statistic coming from a t distribution reflects the notion that our data is a finite sample of all the data that could have been collected if the experiment were repeated an infinite number of times under the same conditions. If the test statistic is really far away from zero, then it’s very improbable that we would observe sampled data like this if the true value of this parameter were zero (i.e. the relevant variable plays no role in explaining the variation in the response variable).&lt;/p&gt;
&lt;p&gt;It’s traditional in science to use a cutoff for the p value of 0.05, corresponding to whether a 95% confidence interval covers zero. This is saying “we accept that in 1 out of every 20 identically conducted experiments we may see no observable effect, the rest of the time we see it”. If your p value, the probability of seeing a test statistic at least as extreme as this if the true value of the parameter is zero, is less than 0.05 then you’ve got evidence to reject the null hypothesis. Sometimes we want to be really confident and we choose a cutoff of 0.01, corresponding to whether a 99% CI covers zero. If the p value is less than 0.01 (where only at most 1 in 100 experiments show us a zero effect) then we have evidence to reject the null hypothesis at our 0.01 level. Sometimes we will accept a less confident cutoff of 0.1 (1 in 10 experiments). Whatever level we choose must be stated up front.&lt;/p&gt;
&lt;p&gt;So in summary the hypothesis we are testing is “The true value of the parameter is zero”, the p value is a probabilistic statement that says “If I assume the true value is zero, what’s the probability of seeing a test statistic (that represents how uncertain I am about my estimate) at least as big as this?”&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/10/10/posterior-samples/</link>
      <pubDate>Thu, 10 Oct 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/10/10/posterior-samples/</guid>
      <description>&lt;p&gt;Open Access publishing is an exciting new development but like any new industry there are those who would seek to unethically make a quick fortune by providing a sub-par service. &lt;em&gt;Science&lt;/em&gt; did some research into this by submitting a fake article with a number of glaring errors (such as non-existent institutions) in it to a number of journals around the world to see who’d accept it. &lt;a href=&#34;http://www.npr.org/blogs/health/2013/10/03/228859954/some-online-journals-will-publish-fake-science-for-a-fee?ft=1&amp;amp;f=1001&#34;&gt;The results are quite interesting&lt;/a&gt;. I know of a few instances of work published in a respectable journal being plagiarised and published in one of these scam journals and one case where the authors hastily retracted their submission once they found out the journal wasn’t anywhere near as prestigious as the editors made out. I support the goals of Open Access and web-based publication but be careful. I’ve been poking around the QUT Wiki looking for information on graduation. In the Higher Degree Research portal there’s a link to a brilliant article (three years old) on &lt;a href=&#34;http://www.timeshighereducation.co.uk/news/how-not-to-write-a-phd-thesis/410208.article&#34;&gt;how not to write a thesis&lt;/a&gt;. Probably good advice for any PhD student looking to graduate sooner rather than later. Elizabeth C. Matsui has been working with Roger Peng for several years; she has &lt;a href=&#34;http://simplystatistics.org/2013/10/08/the-care-and-feeding-of-the-biostatistician/&#34;&gt;some advice on how to cultivate a successful relationship with the biostatistician&lt;/a&gt; you’ve brought on to your medical science project. I think some of these could be adapted to be more general to cover dealing with statisticians. One of the biggest things I could add here is that you need to recognise that asking someone to “help with the statistics” means dedicating some serious time and effort to figuring out a conceptual model (which will be converted into a quantitative model) that addresses the scientific questions you wish to ask. Asking a statistician to “calculate the correlations” or even more broadly “do the statistics” is like asking a scientist to just “run an experiment” or “do the science”. These things all take time, energy and communication. Further to this, don’t just ask for p-values if you’re dealing with a statistician. P-values represent some probabilistic statement, such as “What is the probability of seeing a test statistic at least as extreme as this if the true value of the parameter was zero?”. Sure, you can do simple tests like ANOVA or Chi-Squared goodness of fit but the real value in working with a statistician is being able to develop models that represent assumptions about the data and then assessing whether those assumptions are justified and looking at the inferences that they provide. As my unit co-ordinator for SEB113 pointed out to me the other day, default hypothesis tests of H&lt;sub&gt;0&lt;/sub&gt;: β = 0 at a 5% significance level may not always be sensible and may not even answer the question you’re interested in answering. And finally here’s &lt;a href=&#34;http://www.youtube.com/watch?v=QPKKQnijnsM&#34;&gt;a video that’s almost a year old&lt;/a&gt; that shows a very interesting case of eliciting a distribution from the American public about what they perceive the distribution of wealth in their country is and what they think it is. Turns out the ideal is as far away from the perception as the perception is from the reality.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/10/04/posterior-samples/</link>
      <pubDate>Fri, 04 Oct 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/10/04/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://dynamicecology.wordpress.com/2013/09/26/wiwacs-vs-zombie-ideas/&#34;&gt;Zombie ideas in ecology&lt;/a&gt; make it difficult to publish results that go against them, but how prevalent are they? &lt;a href=&#34;http://mathesaurus.sourceforge.net/octave-r.html&#34;&gt;R for MATLAB users&lt;/a&gt;. This would have been handy in SEB113 if we were still using MATLAB. Perhaps we should give it as a general resource for stats classes in the mathematics program at QUT. The US government shutdown is having &lt;a href=&#34;http://edition.cnn.com/2013/10/03/opinion/urry-nasa-shutdown/index.html?hpt=hp_c2&#34;&gt;quite severe effects on science&lt;/a&gt;, such as threatening the launch of the next &lt;a href=&#34;http://edition.cnn.com/2013/10/03/us/shutdown-mars-mission/index.html&#34;&gt;Mission to Mars&lt;/a&gt;. This is pretty awful given it was &lt;a href=&#34;http://www.news.com.au/technology/sci-tech/happy-birthday-nasa-shut-it-down/story-fn5fsgyc-1226731242175&#34;&gt;NASA’s 50th birthday&lt;/a&gt; earlier this week.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Non-statistics topics in SEB113</title>
      <link>/./2013/09/27/non-statistics-topics-in-seb113/</link>
      <pubDate>Fri, 27 Sep 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/09/27/non-statistics-topics-in-seb113/</guid>
      <description>&lt;p&gt;While SEB113 is all about Quantitative Methods in Science it’s been quite notable that the last two weeks have been mathematical rather than statistical. We started the semester with visualisation and then headed into summary statistics, confidence intervals, hypothesis tests and linear regression. We extended what we knew about linear regression into dealing with non-linear regression (using the nls function) and using multiple predictors to explain variation. In week 7, the non-linear models we used were largely presented without comment as to where they came from, although we did discuss their use to model particular curves. Last semester we planned to do mathematical modelling but Dann Mallet was away and therefore couldn’t deliver a lecture. My undergrad was in mathematical modelling and computational mathematics, so I leapt at the chance to do some mathematical modelling. I figured that the best place to start would be the non-linear models that we dealt with in week 7. I had to do a bit of digging but I found the differential equations for the asymptotic model, first order compartment model and biexponential model as well as a nice little example of how they’re used. While many students haven’t done derivatives recently I do feel like by the end of the workshops there was a bit more of an understanding and appreciation of the modelling and the mathematics behind it. I figured that seeing as we weren’t actually doing a differential equations class and could only assume Maths B it would be a bit much to discuss the calculation of the exact answers for anything beyond the exponential growth model. We talked about how these differential equations have exact solutions but that we can’t always find an exact solution so we look to numerical solutions. We worked through how you can take the approximation for the derivative and ignore the limiting case and rearrange the definition to arrive at a very simple numerical technique. Only one of the workshop rooms properly installed the deSolve library but we did have a good discussion with the groups about how the Lotka-Volterra system has no exact solution and that the solution you obtain lies on an orbit in the phase plane (but I didn’t say that was what it was called). One of my favourite moments in that workshop was when one of a group of students asked about what would happen if the logistic model overshot the carrying capacity steady state. This led into a discussion about discrete time difference equations and the stability of the equilibrium point. The book we used, Barnes and Fulford, has a nice section on the chaotic behaviour of the logistic growth difference model and how as you increase the growth rate you move from smooth evolution that doesn’t overshoot to decaying oscillations around the carrying capacity to non-decaying oscillation between two population levels, to four, eight and then an infinite number of levels (you don’t return to the same population in finite time). That seemed to really tickle them and prompted a discussion about how this sort of behaviour has been observed in real world populations (elephants in a wildlife reserve?). This week we focussed on linear algebra, compressing three weeks of lectures from last semester into one single lecture. This may have been a little rushed in the delivery, because there were so many topics to deal with, but we basically split it up so the first half was showing how to turn a mathematical problem into a matrix system and visualising the solution. The second half was then a walk through of Gauss-Jordan elimination in order to get a matrix in reduced row echelon form (RREF). There were some very salient questions about why we care about matrices being in RREF and what each of the criteria for RREF actually mean. I thought at the time that it was a bit of a distraction from the main thrust of the class but thinking back to it now I am very glad that the students asked these questions as it showed that they were attempting to engage with and understand the material rather than just dismissing it as unimportant. I like to try to point out how the various topics in SEB113 are related, so we had a talk about how making a mesh from a continuous domain to arrive at a discrete set of nodes turns our mathematical model into a system of equations that we can solve numerically (week 9). We spent a bit of time in the week 9 lecture and workshops discussing how Euler’s method uses an approximation of a time derivative to approximate the evolution of a system. In week 10 we looked at the equilibrium temperature of a domain with known boundary conditions, which I pointed out was all about discretising the second derivative from our model. In a similar vein, the solution of an overdetermined system with the normal equations was a callback to the regression we’d done, where an overdetermined system has a non-zero number of degrees of freedom. We use the normal equations to go from an overdetermined system to an exactly solvable one that minimises the sum of squares. I didn’t want to go too deep into the linear algebra of how that works but the workshop exercises featured fitting a non-linear regression with the nls function in R (week 7) and doing it “manually” by solving the normal equations in R after converting it to a linear function. I think the groups that did this one got a bit deeper understanding of how regression works, which I’m definitely happy about. I told a student that I don’t mind if they can’t reproduce a linear regression by coding it manually, so long as they walk away at the end of the semester being able to load some data into R, look at the potential relationships, model them and interpret the model output. These last few weeks have probably been some of the mathematically most challenging and I think that while not all students necessarily understand the maths that’s going on behind the R functions they at least have a bit of an appreciation for where it comes from and why its useful. I’m quite pleased by that, as it’s helping get the message across that mathematics is a useful tool for solving problems rather than just some abstract thing that “I’m never going to use once I finish high school”. The same goes for statistics. Rather than just being this painful thing that requires manual calculation of margins of error for your experiment (forget that for a joke, no one learns anything useful that way), statistics helps you uncover the patterns in your data, model them and make statements about how meaningful they are. If we can get students seeing regression as more than just “What’s the line of best fit and its R&lt;sup&gt;2&lt;/sup&gt;?” then we’ll have done a good job.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using ggplot2 in SEB113</title>
      <link>/./2013/09/10/using-ggplot2-in-seb113/</link>
      <pubDate>Tue, 10 Sep 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/09/10/using-ggplot2-in-seb113/</guid>
      <description>&lt;p&gt;One of the big pieces of feedback we got during last semester’s SEB113 class was that the programming was difficult to understand and reproduce. While the subject is not a programming subject, we do use R quite heavily for all of the data analysis. Maths B isn’t a pre-requisite for SEB113 and I’d wager that even fewer of the students entering the ST01 Bachelor of Science program have taken senior IPT/ICT subjects at their high schools than have taken Maths B. This semester we introduced R in the very first lecture and gave it a bit of a context. This means that students are aware from the get-go that they will be learning statistics through data analysis on a computer. The lectorials introduce the concepts and provide code for the resulting plots and analysis, the computer labs show how to do that particular form of analysis in R and then the collaborative workshops reinforce the labs by getting groups to work through the analysis of some problem using the statistical concepts and code that they’ve learned that week. One of the biggest stumbling blocks last semester was the inconsistency in the way visualisation was done in R. We used a combination of base graphics, trellis graphics in the lattice package, heatmaps and dendrograms from other packages and had to turn to yet another package to get colorbars for the heatmaps. Part of the fine-tuning this semester has been employing someone (who also does the labs) to rewrite the graphics in the labs in terms of Hadley Wickham’s &lt;a href=&#34;http://ggplot2.org/&#34;&gt;ggplot2&lt;/a&gt; library. This brings consistency to the graphical aspect of the unit and the plot geometries are named explicitly so that it’s clear what style of plot you’ll be generating. I was quite sceptical of ggplot2 when I first saw it, as the only exposure I had to it was the default options for a scatterplot with points. Sure, that’s pretty boring, but the fact that you can make a faceted grid (or wrap it using facet_wrap instead of facet_grid) means that investigating the use of small multiples is so much easier. Small multiples is a visualisation technique developed by &lt;a href=&#34;http://www.edwardtufte.com/tufte/&#34;&gt;Edward Tufte&lt;/a&gt; to allow the reader to see how the relationship between two variables changes as you also vary one or two other (categorical) covariates. Doing this in lattice required specifying a formula, similar to the way you specify a model in lm, but lattice is so different from the base graphics that you lose consistency. I’m touching up this week’s workshop at the moment and I’m really noticing where the graphics code has been greatly simplified by access to a grammar of graphics for a powerful set of plotting routines. The &lt;a href=&#34;http://cran.r-project.org/web/packages/GGally/index.html&#34;&gt;GGally&lt;/a&gt; pacakge provides things like ggpairs, which does what pairs does in the base graphics but gives you the correlation above the diagonal and the scatterplots below the diagonal. This makes for more informative graphs with the beauty of the ggplot style. As far as I can tell we’re hearing fewer complaints about the programming and the visualisation is happening much quicker in the workshops this semester as ggplot2’s documentation is amazing and it’s often a choice of geometry (and changing one or two options) rather than a choice of library (and changing the entire approach to the coding). The use of ggplot2 has made teaching visualisation much simpler and we’re now getting through the workshops quite quickly because the visualisation is no longer a huge stumbling block.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Becoming a grown up (at least a grown-up academic)</title>
      <link>/./2013/08/30/becoming-a-grown-up-at-least-a-grown-up-academic/</link>
      <pubDate>Fri, 30 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/30/becoming-a-grown-up-at-least-a-grown-up-academic/</guid>
      <description>&lt;p&gt;There are only four months left in my current postdoctoral appointment and I’m discussing plans for next year with my supervisor. There’s still a lot of unfinished UPTECH work that needs to happen, including helping a handful of PhD students with the stats in their papers for their thesis, dealing with the clinical data and putting together some research plans for what to do next. The plan at the moment is to look for some funding both internally and externally to provide a research appointment. I’m also interested in continuing lecturing next year, whether in SEB113 or another mathematics/statistics unit. Most of ILAQH is away as of today or tomorrow, as they travel to Prague for the &lt;a href=&#34;http://eac2013.cz/&#34;&gt;2013 European Aerosol Conference&lt;/a&gt;. The work that I’ve been doing with some colleagues from ILAQH and Italy, on personal sampling, will be featured on a poster. The paper has been submitted to ES&amp;amp;T but hasn’t been accepted yet, so unfortunately I can’t put a preprint up to show off the cool statistics that I had to learn to do the modelling in the paper. As a result of everyone being away, I’ll be one of two academic staff members left here. It’s going to be quiet, with most of the staff and a few PhD students gone. Barring the Finnish paper that I’m still revising, this personal sampling paper has been the paper which has required the most creative and original programming as there have been many different steps along the way. I am particularly proud of this paper and the work that went into it. When I was first brought on board there didn’t appear to be much clarity regarding what we wanted to investigate; we had a lot of personal sampling data but didn’t quite know what to do with it. I think the paper we’ve developed does service to the amount of work that was put into collecting the data and is aligned with what the UPTECH project was set up to do. I’m grateful to all co-authors on the paper (and everyone who was out there in the schools) for the work that they put into bringing this to fruition. I’m still finishing the final corrections for my thesis, due in a few weeks time. After that’s handed in I’ll be taking another step in becoming a grown-up academic: supervising a PhD student. I’ll be the replacement associate supervisor for a student whose original associate supervisor has moved from QUT to another university. QUT requires an internal primary and associate supervisor and I’m the one most familiar with the modelling that this student is doing as part of their thesis. We’ve already set a meeting schedule for the time when his primary supervisor is overseas and have discussed what sort of things I’ll expect to see. It’s a strange responsibility to have for someone who’s only just finishing up their thesis. I wonder how long it will be until I’m the primary supervisor for a student. Two years? Five? Ten? Worrying about funding, writing grant applications, supervising students, lecturing (writing assessment!). It’s a strange place to find oneself.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>More on regression</title>
      <link>/./2013/08/29/more-on-regression/</link>
      <pubDate>Thu, 29 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/29/more-on-regression/</guid>
      <description>&lt;p&gt;This week we moved on to regression with a categorical covariate, which we’ve used in the context of estimating the mean dissolved oxygen across three spatial locations or the mean grain size of different types of rock. I think one of the biggest stumbling blocks to our learning with R is the way that lm() and associated functions deal with factor terms. Personally, I am not a fan of the way it chooses the first alphabetic level as the baseline. I know you can relevel the factor levels to choose another one but I’d be much happier with a sum to zero constraint so that the effects are all departures from the mean. This is the way that R-INLA does it and, to me, it makes a lot more sense. Factor terms (categorical variables) are essentially a random effects mean, especially in a Bayesian setting where everything can be treated as a random effect. That lm() makes us choose a baseline and treats the other effects as differences to that baseline means we end up with a coefficients table which is more difficult to interpret. One option is to omit the intercept term from the model, with a function call like lm(data=my.data, y ∼ factor(x) - 1) but that still doesn’t give you a sense of the overall mean. In any case, the mixture of new regression techniques and hypothesis testing for whether or not some parameter is equal to zero is proving difficult. The difference between the t and standard Normal distributions seems to not be particularly well understood and while I’ve tried to make the link between a 95% confidence interval and hypothesis testing at a 5% level of significance quite explicit, the fact remains that these are both new concepts which are being taught by a relatively inexperienced lecturer to students whose mathematical literacy is generally not at the level of those I’ve tutored in subjects where Maths B was explicitly a pre-requisite rather than assumed knowledge. I managed to explain the heavy tails issue in class with a little bit of pantomime, showing how one might paint the tails of the t and Normal distributions and run out of paint at different values of t (or x) based on how much paint you had to use at values far away from the mean. I think about a quarter of the class was struggling with the diagram of overlapping triangles which was meant to be a “zoomed in” version of the point where the density functions of the Normal and t cross over as they get further away from the mean. The lectorials are recorded so I’ll be interested to see how it translates to a radio play setting. The computer labs are apparently quite dense at the moment, with a lot of fairly new ideas being reinforced in a 50 minute block. We’re quite fortunate to have all the labs before all the workshops this semester, so the lab is basically “here’s the code to do what was shown in the lectorials” and then the workshops are designed to implement the code for some problem and generate a bit of discussion. I think this week was probably one of the hardest, conceptually, because it brings together regression for categorical explanatory variables (a straight line is easier to understand than mutually exclusive sets of points), hypothesis testing, the t distribution and confidence intervals. I have uploaded some of last semester’s slides on the central limit theorem for those who may need them, but I think it’s more a familiarity and practice thing than the material being inherently inaccessible. Next week we’ll be moving on to different families for Generalised Linear Models and the use of the nls() function to fit non-linear models such as asymptotic, compartment and bi-exponential. I’m not such a fan of nls (or even nlme) but we can hardly teach them how to use something like WinBUGS to define their own custom mean functions (because if you struggle with the t distribution you’re going to have kittens trying to deal with using the Beta distribution as a conjugate prior for the Binomial model) or even throw them into using gam() from mgcv. I wouldn’t be averse to teaching Generalised Additive Models in an advanced follow-up unit for this subject. If we did that, we could remove the GLMs from SEB113 (which we’ve only introduced this semester) and spend some time on random effects models. I think such a subject would require a much stronger background in mathematics, so students may need to take MAB120/125 and MAB121/126 before attempting such a unit. Still, food for thought as QUT continues to develop the new Bachelor of Science course.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Test statistics and regression parameters</title>
      <link>/./2013/08/23/test-statistics-and-regression-parameters/</link>
      <pubDate>Fri, 23 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/23/test-statistics-and-regression-parameters/</guid>
      <description>&lt;p&gt;This week in SEB113 we’ve started on regression with some simple linear models with one explanatory variable. As not everyone has a particularly strong statistics background (high school Maths B) there are definitely some challenges.&lt;/p&gt;
&lt;p&gt;The big one seems to be moving from the Normal distribution, which everyone seems to get, for estimating the confidence interval of the mean, towards the t distribution for calculating confidence intervals for regression parameters. Putting the t distribution in the context of estimating quality between batches of Guiness helps a little with the question “Where did this even come from?” but doesn’t address the mathematics of it. Plotting a few different t distributions with varying degrees of freedom helps make the point that the t approaches the Normal when the degrees of freedom goes to infinity but does nothing to explain what the degrees of freedom actually are.&lt;/p&gt;
&lt;p&gt;I’ve found that explaining the data as a resource for fitting the regression model can be handy. For a data set with &lt;em&gt;n&lt;/em&gt; points you have a maximum of &lt;em&gt;n&lt;/em&gt; degrees of freedom. Each time you add a parameter to your regression model you consume a degree of freedom because you’re imposing a constraint, such as “there is a straight line”. If we had one data point in our data set and wanted to know the mean of the data we would know it exactly, there would be no uncertainty left in our estimate (and therefore zero degrees of freedom). If we had two data points and wanted the mean there would be some amount of uncertainty left because there’s now some variation in our data (we would have one degree of freedom left). If we had two points and wanted a line of best fit we would be back to zero degrees of freedom because we have completely characterised the trend in the data set by joining the two points.&lt;/p&gt;
&lt;p&gt;If we fit a regression with a total of &lt;em&gt;k&lt;/em&gt; parameters on the right hand side of &lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt; = &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;0&lt;/sub&gt; + &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt; &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;1_i_&lt;/sub&gt; + &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt; &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;2_i_&lt;/sub&gt; + … + &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;k-1&lt;/sub&gt; &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;-1_i _&lt;/sub&gt; + ϵ&lt;sub&gt;i&lt;/sub&gt; (a mean and some effects of explanatory variables) we would have &lt;em&gt;n&lt;/em&gt;-&lt;em&gt;k&lt;/em&gt; degrees of freedom. As you have fewer degrees of freedom left your t distribution ends up with more mass further from the mean. This means that you’re more uncertain about the value of the parameter because you’re using your data to estimate other parameters.&lt;/p&gt;
&lt;p&gt;Coming from a Bayesian perspective and looking at this sort of mathematics it’s easy to conclude that the model states that the parameters have a t distribution. This is, of course, completely incorrect because the parameters are fixed constants with unknown values. This idea totally throws me as I’ve been working with Bayesian analysis for the last few years (with the exception of perhaps one paper) and I’m used to thinking of all parameters as being random variables.&lt;/p&gt;
&lt;p&gt;This is related to the hypothesis testing and confidence interval issues that I have with the way first year statistics is taught. Confidence intervals, as I’ve mentioned previously, are counter-intuitive. They are the things that are random in our estimates of the true values of parameters. I like the approach that we’re taking where we look at whether the 95% confidence interval covers zero in order to make statements about whether or not the parameter plays a role in explaining the variation that we are modelling. I don’t like that we then calculate p values for testing the hypothesis that that parameter is equal to zero. These tests are statements about the probability of seeing the test statistic or more extreme given the model that we’re working with. It’s all backwards and leads inexperienced students to make statements such as “We accept the null hypothesis” and “The variable is statistically insignificant”, both of which are nails on a chalkboard to my ears.&lt;/p&gt;
&lt;p&gt;I know that we can’t teach statistics the way I would like to teach it, as these are science students who will be entering a field where ANOVA and t tests are still commonly used not as exploratory data analysis but as the basis for inference. I am very thankful for the fact that we are moving away from testing and towards modelling, and I’ve been trying to make the point in lectures that modelling allows us to do prediction, whereas testing allows us to only talk about what we’ve seen. If we can make sure the students can fit a model in R and use it to predict and/or make inferences I think we’ll have done our jobs because that is far more than I was able to do when I finished MAB101 ten years ago, when everything required we look through page after page of statistical table, hunting the right p value.&lt;/p&gt;
&lt;p&gt;Edit: 150 posts!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conference posters and an accepted paper</title>
      <link>/./2013/08/09/conference-posters-and-an-accepted-paper/</link>
      <pubDate>Fri, 09 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/09/conference-posters-and-an-accepted-paper/</guid>
      <description>&lt;p&gt;Thomas Lumley &lt;a href=&#34;http://notstatschat.tumblr.com/post/57747270796/speed-sessions-at-jsm-2013&#34;&gt;has some thoughts&lt;/a&gt; on poster sessions at JSM 2013. Healthy Buildings 2012 had something I hadn’t seen before - poster presenters were given two minutes at the end of the technical session most relevant to their poster to describe the work they are presenting in the poster session immediately after the talk slot. This gave poster presenters a small taste of presenting at a conference without them needing to prepare a full talk. QUT’s Nano and Molecular Sciences discipline had a poster session during its one day symposium a few months ago and the posters were run off &lt;a href=&#34;http://www.thecube.qut.edu.au/&#34;&gt;The Cube&lt;/a&gt;, which allowed people to zoom and rotate a static image of their poster (PDF preferred). Our group will have a few posters at the &lt;a href=&#34;http://eac2013.cz/&#34;&gt;European Aerosol Conference&lt;/a&gt; in a few weeks. Mandana Mazaheri and I have been discussing the issues of transporting posters back and forth internationally, including whether or not to print on cloth and the unwieldy nature of poster tubes. I am a big fan of mailing your poster home once it’s presented but a cloth poster can be folded up and put in your luggage and you can just give it a quick iron before presenting it. I’ve also seen way too many posters that are too busy and have gradient backgrounds. Hopefully by teaching SEB113 students about the principles of good visualisation of data QUT can produce graduates who know not to make ugly posters. &lt;a href=&#34;http://pubs.acs.org/doi/abs/10.1021/es4023706&#34;&gt;Our endotoxin paper got accepted in Environmental Science &amp;amp; Technology&lt;/a&gt; after a frantic couple of days of finalising amendments and responses to reviewer comments. This paper gave me a much better understanding of Bayesian hierarchical linear models and I’m very happy with how the paper turned out. The next step is to resubmit our fungus paper, which includes similar modelling but also uses a Multinomial model with Dirichlet prior to look at the proportions of different fungal genera across the UPTECH schools. There’s yet another paper looking at chemicals in floor dust which we’re also finalising that uses a similar methodology to the fungus paper but has its own subtleties due to some chemicals not being present across all schools.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Teaching confidence intervals</title>
      <link>/./2013/08/07/teaching-confidence-intervals/</link>
      <pubDate>Wed, 07 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/07/teaching-confidence-intervals/</guid>
      <description>&lt;p&gt;I was having a chat with two colleagues from the School of Maths this morning, as we all stared at our coffees to start the day, about how teaching in SEB113 is going. I mentioned the challenge of teaching confidence intervals to first year science students. The main things that make this difficult are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I do not use confidence intervals in my research&lt;/li&gt;
&lt;li&gt;confidence intervals are new to these students&lt;/li&gt;
&lt;li&gt;confidence intervals are counter-intuitive&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While most of my undergraduate statistics classes were 100% frequentist, my postgrad has been almost entirely Bayesian. The credible interval is how Bayesians summarise likely values that the parameter of interest might take. It is based on the quantiles of the posterior distribution, often obtained by sampling from the posterior and finding the 2.5th and 97.5th percentiles of the samples. It is explicitly a combination of the data and the prior and says “these are the most likely values of the parameter”.&lt;/p&gt;
&lt;p&gt;The confidence interval, by contrast, is based on the idea repeating the experiment an infinite number of times and that the true value of the parameter is covered by these intervals, say, 95% of the time. The idea of infinity is difficult enough without asking students to imagine partitioning an infinitely large set. We can’t say that with 95% probability the true value of the quantity of interest lies in the 95% confidence interval calculated from a particular sample but we can say that we are 95% confident that the 95% confidence interval calculated from a particular sample contains the true value.&lt;/p&gt;
&lt;p&gt;In frequentist statistics the parameter is fixed and the confidence interval is random, based on the sample, and the probability that the true value is contained within a particular confidence interval is either 0 or 1, as the parameter is non-random. In Bayesian statistics the parameter has a distribution based on the fixed data and the prior and interval summarises the range of the most likely values.&lt;/p&gt;
&lt;p&gt;The meaning of a 95% confidence interval is &lt;strong&gt;not&lt;/strong&gt; that there is a 95% chance that the true value lies in the interval.&lt;/p&gt;
&lt;p&gt;That the parameter is fixed and the intervals random can be quite a confusing concept, and the subtleties of the probabilistic statements are not readily understood by those who are only now taking their first steps into statistical data analysis. Maths B is not a pre-requisite for this subject, so some students are entering with only Maths A and may not have been exposed to the Normal distribution, hypothesis testing or any of the other ideas that are intrinsically linked to confidence intervals.&lt;/p&gt;
&lt;p&gt;Below are two plots showing the results of our experiment in this week’s workshops, where we each flipped a coin ten times and recorded the number of heads observed. We expect to see 95% of these intervals covering p=0.5. On Tuesday we saw 20 of 22 covering 0.5 and Wednesday was 12 out of 13 (but I had replotted Ruth’s and mine from Tuesday). All up, 91% of our intervals covered p=0.5, which is pretty close to what we would expect to see. For a few students, visualising the confidence intervals like this helped with the idea of the sampling variability and the confidence interval being based on the data observed rather than the theoretical model (p=0.5).&lt;/p&gt;
&lt;p&gt;[caption id=“attachment_974” align=“aligncenter” width=“450”][&lt;img src=&#34;tuesdaycis.jpg?w=450&#34; alt=&#34;95% confidence intervals for p(heads) from a Binomial(10, p) distribution. Tuesday’s class.&#34; /&gt;](&lt;a href=&#34;http://samcliffordinfo.files.wordpress.com/2013/08/tuesdaycis.jpg&#34; class=&#34;uri&#34;&gt;http://samcliffordinfo.files.wordpress.com/2013/08/tuesdaycis.jpg&lt;/a&gt;) 95% confidence intervals for p(heads) from a Binomial(10, p) distribution. Tuesday’s class.[/caption][caption id=“attachment_975” align=“aligncenter” width=“450”]&lt;a href=&#34;http://samcliffordinfo.files.wordpress.com/2013/08/wednesdaycis.jpg&#34;&gt;&lt;img src=&#34;wednesdaycis.jpg?w=450&#34; alt=&#34;95% confidence intervals for p(heads) from a Binomial(10, p) distribution. Wednesday’s class.&#34; /&gt;&lt;/a&gt; 95% confidence intervals for p(heads) from a Binomial(10, p) distribution. Wednesday’s class.[/caption]&lt;/p&gt;
&lt;p&gt;Next week we move on to the Normal distribution and the ideas of taking a large enough sample that you can observe the effect you are interested in. There’s no workshop due to the Ekka holiday, so I need to meet with Ben tomorrow to discuss how we incorporate what we need to in the computing lab.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/08/02/posterior-samples/</link>
      <pubDate>Fri, 02 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/02/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://lindsaybradford.wordpress.com/2013/07/25/the-database-design-goggles-they-do-nothing/&#34;&gt;Database design is important&lt;/a&gt;, especially if someone else has to work with your database. It’s not really something we teach in undergraduate science, perhaps the American model of requiring a certain number of credits from certain fields would help remedy this. Ever wanted a glimpse of &lt;a href=&#34;http://xianblog.wordpress.com/2013/07/22/bayes-notebook/&#34;&gt;Bayes’ notebook&lt;/a&gt;? Laura McInerney’s comments on the benefits of &lt;a href=&#34;http://thesiswhisperer.com/2013/07/31/in-praise-of-the-small-conference/&#34;&gt;small conferences&lt;/a&gt; are similar to my experience with &lt;a href=&#34;http://bayesian.org/node/1657&#34;&gt;8 BNP&lt;/a&gt; in Veracruz, Mexico. As long as you’re within the niche field this sort of conference is a great experience. I felt a little like an outsider at 8 BNP because while I was interested in non-parametrics and was working on smoothing, a lot of people were working on things that I had no experience with which are actually the central elements of the field. I got to learn about a lot of neat things, hear some great talks and meet lots of amazing people, but I don’t think I was steeped in NP Bayes enough to really get the most out of the conference. My research went a bit away from NP Bayes these last few years so I didn’t get to put anything together for 9 BNP in Amsterdam. Perhaps ISBA 2014 in Cancún, Mexico will provide a bit more of a chance to get back to that work. We’re teaching R in SEB113. Perhaps any students reading this might be interested in these &lt;a href=&#34;http://www.computerworld.com/s/article/9239799/60_R_resources_to_improve_your_data_skills&#34;&gt;60 R resources&lt;/a&gt;. I use multiple monitors at work but really enjoyed the virtual monitors setup in Gnome when I ran Ubuntu. &lt;a href=&#34;http://lifehacker.com/5616859/is-the-multiple+monitor-productivity-boost-a-myth&#34;&gt;It turns out&lt;/a&gt; that having a large canvas of pixels, rather than multiple monitors, is the key to workplace productivity. My work setup has two widescreen monitors side by side in portrait orientation. This doesn’t work particularly well with programs that assume you’re using a single landscape monitor (such as RStudio) or give you a single window with multiple documents inside that each have focus one at a time (Microsoft Office, why can’t I have a spreadsheet on each monitor?) but it means I don’t have to keep switching back and forth between TeXStudio and RStudio when I’m writing up my analysis. &lt;a href=&#34;http://chronicle.com/article/Introduction-to-Ancient/140475/&#34;&gt;Flipped classes&lt;/a&gt; are an interesting model for education. I remember taking an Honours level mathematical modelling course a few years ago where the three hours of lecture time allocated us were used to discuss concepts and do modelling. We would read a chapter from the textbook in the lead-up to the class and then have a talk about what it meant and then work out a model based on a case study. I don’t know how well a truly flipped class would translate to a group bigger than about 30 students, but Sue Savage (QUT) tells me that the new lecture theatres in P block are designed to facilitate small group discussions within lectures. Daina Taimiņa explains &lt;a href=&#34;https://www.youtube.com/watch?v=w1TBZhd-sN0&#34;&gt;hyperbolic geometry&lt;/a&gt; with crochet. Every once in a while something similar pops up and I can’t help but get excited. &lt;a href=&#34;http://longnow.org/essays/richard-feynman-connection-machine/&#34;&gt;Daniel Hills recalls his memories&lt;/a&gt; of working with Richard Feynman on developing a massive parallel computer in the 1980s.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples - July wrap</title>
      <link>/./2013/07/29/posterior-samples---july-wrap/</link>
      <pubDate>Mon, 29 Jul 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/07/29/posterior-samples---july-wrap/</guid>
      <description>&lt;p&gt;I had some Posterior Samples to share before going on leave but didn’t get around to posting them. Here’s what’s been on my mind this month: Maths and science units are popular with (Kentucky) students until they realise that &lt;a href=&#34;http://blogs.wsj.com/economics/2013/07/08/math-science-popular-until-students-realize-theyre-hard/&#34;&gt;they’re hard&lt;/a&gt;. While not directly relevant to the Australian university education model it’s probably an important thing for the Science and Engineering Faculty to keep in mind. &lt;a href=&#34;http://blogs.wsj.com/economics/2013/07/08/math-science-popular-until-students-realize-theyre-hard/&#34;&gt;&lt;/a&gt; I’m looking at ggplot2 more these days, so the idea of a “grammar of graphics” is beginning to resonate with me. &lt;a href=&#34;//www.youtube.com/watch?v=xyGggdg31mc&#34;&gt;Here’s a talk&lt;/a&gt; about building one for &lt;a href=&#34;http://clojure.org/&#34;&gt;clojure&lt;/a&gt; (which I don’t use). Something for me to keep in mind when delivering SEB113 slides this semester is &lt;a href=&#34;https://www.dropbox.com/s/p0sgdgo7mxkoj4h/What%20your%20math%20slides%20dont%20need.pdf&#34;&gt;what your maths slides don’t need&lt;/a&gt;. Probably also good pointers for any PhD students graduating soon. &lt;a href=&#34;http://well.blogs.nytimes.com/2013/07/22/the-kitchen-as-a-pollution-hazard/&#34;&gt;An interesting article in the New York Times&lt;/a&gt; about air pollution from cooking. This is something that ILAQH has a research interest in and our nanotracer paper contains a bit of analysis of inhaled surface area dose from particles that originate from cooking. &lt;a href=&#34;http://www.nytimes.com/2013/07/22/business/in-climbing-income-ladder-location-matters.html&#34;&gt;Another NYT article&lt;/a&gt;, this time with a delicious visualisation of the geographical trends in income disparity and social mobility. &lt;a href=&#34;http://www.slate.com/articles/health_and_science/science/2013/07/statistics_and_psychology_multiple_comparisons_give_spurious_results.html&#34;&gt;Andrew Gelman writes at Slate&lt;/a&gt; about some of the problems with scientific publishing and the publication of spurious findings (which isn’t always willingly dishonest). A special “Big Bayes Stories” issue of “Statistical Science” will be published soon, focussing on the real world application of Bayesian statistics where other methods were inapplicable. &lt;a href=&#34;http://xianblog.wordpress.com/2013/07/29/big-bayes-stories/&#34;&gt;Christian Robert has written the preface&lt;/a&gt;; the issue is being edited by Robert, Kerrie Mengersen (one of my PhD supervisors) and Sharon McGrayne, author of “&lt;a href=&#34;http://www.amazon.com/Theory-That-Would-Not-Die/dp/1452636850&#34;&gt;The Theory That Would Not Die&lt;/a&gt;”. Also I went to &lt;a href=&#34;http://www.questacon.edu.au/&#34;&gt;Questacon&lt;/a&gt; and it was awesome.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On leave next week</title>
      <link>/./2013/07/18/on-leave-next-week/</link>
      <pubDate>Thu, 18 Jul 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/07/18/on-leave-next-week/</guid>
      <description>&lt;p&gt;I’m on leave next week so am attempting to spend today and tomorrow getting ready some things finished before I disappear. This means finishing my contribution to two papers by one of the UPTECH PhD students, finding a new home for our personal sampling paper (Environmental Health Perspectives didn’t want it), getting SEB113 material ready for our 67 students next semester, contacting collaborators interstate about data, getting some Bayesian Network elicitation typed up and helping various PhD students with their data anlysis. It has been quite full on despite my supervisor being overseas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples - SEB113 edition</title>
      <link>/./2013/07/10/posterior-samples---seb113-edition/</link>
      <pubDate>Wed, 10 Jul 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/07/10/posterior-samples---seb113-edition/</guid>
      <description>&lt;p&gt;I’m going to be frank, a lot of this relates to SEB113 - Quantitative Methods for Science, a subject I tutored last semester. One of the students from SEB113 last semester, Daniel Franks, is &lt;a href=&#34;https://twitter.com/dpfscience&#34;&gt;live-tweeting his Bachelor of Science degree&lt;/a&gt;. Daniel was in my workshop group last semester and I recognise some of the events he talks about in his timeline. It’s interesting to see his perspective not just on SEB113 but on the other three units that form the first semester of QUT’s new Bachelor of Science course. SEB113 is getting a small makeover for Semester 2. One of the things we’re considering is the use of ggplot2 instead of a combination of the base graphics package, heatmaps from dendrograms with colorbars from yet another package, etc. Lattice doesn’t have the nicest interface and it’s nigh on impossible to add elements afterwards (I hate you, levelplot). It’s possible to do &lt;a href=&#34;http://gettinggeneticsdone.blogspot.com.au/2010/01/ggplot2-tutorial-scatterplots-in-series.html&#34;&gt;small multiples in ggplot2&lt;/a&gt; fairly easily. We ought to be sticking to the same steps in data analysis as we did last semester, and Daniel’s tweets refer to an experience in class last semester where we discussed drawing the analysis method out of exploratory plots of the data, rather than trying to pick the “best” model &lt;em&gt;a priori&lt;/em&gt; and making the data fit the model. &lt;a href=&#34;http://simplystatistics.org/2013/06/27/what-is-the-best-way-to-analyze-data/&#34;&gt;Roger Peng’s got a good five step&lt;/a&gt; technique for analysing data:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory analysis&lt;/li&gt;
&lt;li&gt;Model fitting&lt;/li&gt;
&lt;li&gt;Model building&lt;/li&gt;
&lt;li&gt;Sensitivity analysis&lt;/li&gt;
&lt;li&gt;Reporting&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Via &lt;a href=&#34;http://www.quantumforest.com/2013/07/flotsam-13-early-july-links/&#34;&gt;Luis Apiolaza at Quantum Forest&lt;/a&gt; I’ve stumbled across &lt;a href=&#34;http://www.statschat.org.nz/&#34;&gt;Thomas Lumley&lt;/a&gt;’s Tumblr, where he’s doing some personal blogging about statistics. An &lt;a href=&#34;http://notstatschat.tumblr.com/post/54011641155/where-is-bayesian-introductory-statistics-better&#34;&gt;interesting post&lt;/a&gt; of his is on the role of Bayesian stats in introductory classes. I would love to turn SEB113 into a Bayesian statistics based class but for the time being I will have to settle for it dealing with modelling over tests (which is still a big win, pedagogically). Teaching Bayesian statistics generally relies on a good grounding in calculus, otherwise writing down full conditionals is going to be quite difficult. When people tell me that statistics is so different to mathematics I like to point out that it’s just a combination of calculus, linear algebra and some discrete mathematics. &lt;a href=&#34;http://magazine.amstat.org/blog/2013/07/01/calculus-and-statistics/&#34;&gt;Daniel Kaplan writes at &lt;strong&gt;AMSTAT&lt;/strong&gt;News&lt;/a&gt; about ditching mathematical formalism to make statistics more accessible. The American undergraduate model is very different to what we have in Australia, but I take his point about a first year calculus class not being as relevant to graduates as a first year statistics course that teaches statistical thinking over statistical calculation. I really like the focus in SEB113 on modelling using R rather than statistical tests by hand with pages of tables (as MAB101 was when I did my Bachelor of Science). If people finish SEB113 knowing how to read their data in to R and perform a Generalised Linear Model I think we’ll have done our job. If they want to go on to further statistics from there, the statistics units in the School of Mathematical Sciences work from a calculus perspective and while they require a calculus pre-requisite (MAB121 or MAB122 for those QUT students reading) you could do a lot worse than taking MAB210 and MAB314. I hope a follow-up data analysis course will be offered to Bachelor of Science students that builds on SEB113 and covers some more advanced topics and introduces enough mathematics to make those topics worthwhile. We’ll have to see how it all unfolds as this first cohort make their way through.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
