<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Code on Sam Clifford </title>
    <link>/./tags/code/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2014-04-22 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>Combining differential equations and regression</title>
      <link>/./2014/04/22/combining-differential-equations-and-regression/</link>
      <pubDate>Tue, 22 Apr 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/04/22/combining-differential-equations-and-regression/</guid>
      <description>&lt;p&gt;Last week I gave my first lecture for the semester to the SEB113 students. While they tend to not have a particularly strong mathematics background I got some very positive feedback on how much they enjoyed learning about mathematical modelling. We revised differentiation, what derivatives are and then jumped into a bit about formulating differential equations from words that represent the assumptions that the model makes. The bulk of that week’s lecture is showing where the non-linear regression models we used in the previous week (first order compartment, asymptotic, biexponential) come from. To do this we have a chat about exponential growth and decay models as some of the easiest differential equation models to deal with. I show them how we solve the exponential model exactly and then make reference to the fact that I don’t expect them to solve these equations in this subject. We show the solutions to the DE systems and make it very clear that the non-linear regression models are the solutions to differential equations that represent different assumptions. We finish the lecture off with a section on how we can’t always get a “pen and paper” solution to differential equations and so sometimes we either simplify the system to one we can solve (alluding to perturbation methods) or give it to a numerical solver (alluding to computational mathematics). Because it’s how I learned about numerical solutions to DEs I showed the students the Lotka-Volterra model and discussed why we can’t solve X(t) and Y(t) and so have to use numerical methods. For different parameter values we get variations on the same behaviour: cyclic patterns, prey population growth followed by predator population growth followed by overconsumption of prey leading to fewer predators being born to replace the dying. Many students seemed to enjoy investigating this model in the workshops, as it’s quite different to everything we’ve learned so far. Solution is via the deSolve package in R but we introduce the students to Euler’s method and discuss numerical instability and the accumulation of numerical error. I finish off the lecture with a chat about how regression tends to make assumptions about the form of the mean relationship between variables so we can do parameter estimation and that differential equations give us a system we can solve to obtain that mean relationship. I state that while we &lt;em&gt;can&lt;/em&gt; solve the DE numerically while simultaneously estimating the parameter it is way outside the scope of the course. I had a bit of time this morning to spend on next week’s lecture material (linear algebra) so decided to have a go at numerical estimation for the logistic growth model and some data based on the Orange tree circumference data set in R with JAGS/rjags. It’s the first time I’ve had a go at combining regression and numerical solutions to DEs in the same code, so I’ve only used Euler’s method. That said, I was very happy with the solution and the code is provided below the cut. [code language=“r”] # euler.bugs model{ y[1] ~ dnorm(mu[1], tau.y) mu[1] &amp;lt;- y0 + dt * exp(lr) * y0 * (1 - y0/K) for (i in 2:n){ y[i] ~ dnorm(mu[i], tau.y) mu[i] &amp;lt;- y[i-1] + dt * exp(lr) * y[i-1] * (1 - y[i-1]/K) } for (i in 1:n){ y.p[i] ~ dnorm(mu[i], tau.y) } lr ~ dnorm(0, 1e-6) K ~ dnorm(0, 1e-6) y0 ~ dunif(0.001, 1000) tau.y ~ dgamma(0.001, 0.001) } [/code] Which can be called appropriately with [code language=“r”] library(rjags) library(ggplot2) my.orange &amp;lt;- data.frame(age=seq(100, 1900, by=200), circumference = c(32, 47, 73, 101, 134, 162, 182, 194, 205, 214)) dt &amp;lt;- 10 orange.dat &amp;lt;- data.frame(age=seq(0, 3000, by=dt),circumference=NA) orange.dat[match(table=orange.dat&lt;span class=&#34;math inline&#34;&gt;\(age, x=my.orange\)&lt;/span&gt;age),“circumference”] &amp;lt;- my.orange[, “circumference”] orange.m &amp;lt;- jags.model(file=“euler.bugs”, data=list(y=orange.dat&lt;span class=&#34;math inline&#34;&gt;\(circumference, n = nrow(orange.dat), dt=dt),inits=list(y0=50, K=500, lr=-5)) orange.b &amp;lt;- jags.samples(model=orange.m, n.iter=10000, variable.names=c(&amp;quot;y.p&amp;quot;,&amp;quot;K&amp;quot;,&amp;quot;lr&amp;quot;,&amp;quot;y0&amp;quot;)) orange.pred &amp;lt;- coda.samples(model=orange.m, n.iter=1000, variable.names=c(&amp;quot;y.p&amp;quot;)) orange.sum &amp;lt;- summary(orange.pred, q=c(0.025, 0.5, 0.975)) orange.gg &amp;lt;- data.frame(orange.sum\)&lt;/span&gt;quantiles) orange.gg&lt;span class=&#34;math inline&#34;&gt;\(age &amp;lt;- orange.dat\)&lt;/span&gt;age windows(height=3.5) ggplot(data=orange.gg, aes(x=age, y=X50.)) + geom_line() + geom_line(aes(y=X2.5.), lty=2) + geom_line(aes(y=X97.5.), lty=2) + geom_point(data= my.orange, aes(y=circumference), alpha=0.5) + theme_bw() + xlab(“Time (days)”) + ylab(“Tree circumference (mm)”) [/code] The resulting picture can be seen below. [caption id=“attachment_1226” align=“aligncenter” width=“625”][&lt;img src=&#34;predicting.png?w=625&#34; alt=&#34;Prediction of tree circumference from logistic growth differential equation&#34; /&gt;](&lt;a href=&#34;http://samcliffordinfo.files.wordpress.com/2014/04/predicting.png&#34; class=&#34;uri&#34;&gt;http://samcliffordinfo.files.wordpress.com/2014/04/predicting.png&lt;/a&gt;) Prediction of tree circumference from logistic growth differential equation[/caption]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My take on the Bayesian updating video</title>
      <link>/./2012/08/27/my-take-on-the-bayesian-updating-video/</link>
      <pubDate>Mon, 27 Aug 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/08/27/my-take-on-the-bayesian-updating-video/</guid>
      <description>&lt;p&gt;I’m referring here to my &lt;a href=&#34;http://samclifford.info/2012/08/25/438/&#34;&gt;last post&lt;/a&gt; where I reblogged something that &lt;a href=&#34;http://bayesianbiologist.com/&#34;&gt;Bayesian Biologist&lt;/a&gt; &lt;a href=&#34;http://bayesianbiologist.com/2012/08/17/an-update-on-visualizing-bayesian-updating/&#34;&gt;had written&lt;/a&gt;. I’ve modified the code (and included it below) to tweak it to my tastes and show the different behaviour of the posterior under a uniform Beta(1,1) prior and an informative Beta(20,20) prior. Rather than flipping a fair coin I’ve assumed that the coin is unfair (p=0.25) and the Beta(20,20) prior is chosen by the naive observer who believes, with a high degree of certainty, that the coin is fair. This is, to me, an illustration of the power of uninformative priors (something Laplace seemed keen on in Binomial experiments). It’s also a good demonstration of how given enough data the posteriors will converge to the same result. A persistent criticism of Bayesian analysis is that the priors are so subjective that anyone can come up with their own prior and get a different result to another observer. As more data is collected, the influence of the prior in the posterior is diminished. We see (in the 1.08MB animation below the cut) that the Beta(1,1) prior is more sensitive to each success and failure and the posterior mean approximates p much quicker than the more “certain” Beta(20,20) prior. The take home message? Be more vague than you think is warranted just in case your prior is not diffuse enough. To generate the gif I’ve used the instructions in the video in the last lines of the code. &lt;a href=&#34;updating.gif&#34;&gt;&lt;img src=&#34;http://samcliffordinfo.files.wordpress.com/2012/08/updating.gif&#34; title=&#34;Bayesian updating&#34; /&gt;&lt;/a&gt; [sourcecode language=“r”] ## Corey Chivers, 2012 ## ## modifications by ## ## Sam Clifford, 2012 ## sim_bayes &amp;lt;- function(p=0.5,N=100,y_lim=20,a_a=2,a_b=10,b_a=8,b_b=3) { ## Simulate outcomes in advance outcomes&amp;lt;-sample(1:0,N,prob=c(p,1-p),replace=TRUE) success&amp;lt;-cumsum(outcomes) for(frame in 1:N) { png(paste(“plots/”,1000+frame,“.png”,sep=“”)) curve(dbeta(x,a_a,a_b),xlim=c(0,1),ylim=c(0,y_lim),col=‘green’,xlab=‘p’,ylab=‘Posterior Density’,lty=2) curve(dbeta(x,b_a,b_b),col=‘blue’,lty=2,add=TRUE) lines(x=c(p,p),y=c(0,y_lim),lty=2,lwd=1,col=“grey60”) i &amp;lt;- frame # i don’t like having the leftovers on the screen – Sam #for(i in 1:frame) #{ # curve(dbeta(x,a_a+success[i]+1,a_b+(i-success[i])+1),add=TRUE,col=rgb(0,100,0,(1-(frame-i)/frame) * 100,maxColorValue=255)) # curve(dbeta(x,b_a+success[i]+1,b_b+(i-success[i])+1),add=TRUE,col=rgb(0,0,100,(1-(frame-i)/frame) * 100,maxColorValue=255)) #} curve(dbeta(x,a_a+success[i]+1,a_b+(i-success[i])+1),add=TRUE,col=rgb(0,100,0,255,maxColorValue=255),lwd=2) curve(dbeta(x,b_a+success[i]+1,b_b+(i-success[i])+1),add=TRUE,col=rgb(0,0,100,255,maxColorValue=255),lwd=2) # modifications to make prior explicit in legend legend(‘topleft’,legend=c( paste(‘Observer A - Beta(’, a_a , ‘,’ , a_b , ‘)’,sep=“”),paste(‘Observer B - Beta(’, b_a , ‘,’ , b_b , ‘)’,sep=“”)),lty=1,col=c(‘green’,‘blue’)) text(0.75,17,label=paste(success[i],“successes,”,i-success[i],“failures”)) dev.off() } # Sam # system(’mencoder mf://plots/*.png -mf fps=15:type=png -ovc copy -oac copy -o plots/output.avi’) # we’ll use GIMP rather than mencoder # &lt;a href=&#34;http://www.youtube.com/watch?v=u5_3MGP2Lj4&amp;amp;feature=player_detailpage#t=265s&#34; class=&#34;uri&#34;&gt;http://www.youtube.com/watch?v=u5_3MGP2Lj4&amp;amp;feature=player_detailpage#t=265s&lt;/a&gt; } sim_bayes(a_a=1,a_b=1,b_a=20,b_b=20,p=0.25,N=250) [/sourcecode]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/./2012/08/25/</link>
      <pubDate>Sat, 25 Aug 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/08/25/</guid>
      <description>&lt;p&gt;I came across this via reddit’s r/statistics community and thought I might share it as a nice way of visualising posteriors. Specifically, it’s a very good demonstration of the convergence of the posterior beliefs of two observers with separate priors but the same data (which is sequentially collected, but the order of successes/failures are irrelevant). So next time someone’s telling you that Bayesian statistics is inherently wrong because of the subjectivity of the prior belief, you can point them to something like this to demonstrate that as data is collected the posteriors become quite close. I suggest having a play with the R code to understand how the diffuseness of the priors affects the concentration of posterior belief. While the opposite beliefs of the observers in the attached video are a nice example of convergence to the same posterior, I think two priors with the same mean and different variance would be a more interesting visualisation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dear Mr Supercomputer</title>
      <link>/./2012/04/17/dear-mr-supercomputer/</link>
      <pubDate>Tue, 17 Apr 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/04/17/dear-mr-supercomputer/</guid>
      <description>&lt;p&gt;After ensuring that my colleagues were pushing their results to our git repository it’s now all steam ahead on this Finnish paper. I’ve got my MATLAB code running on the QUT supercomputer right now. It’s all very exciting watching it go and not run out of memory. Hopefully I can get these results sorted, do some model comparison, plotting and then send the document around for some final drafting within the next two or three weeks (the bulk of the theory section of the paper is finished). It would’ve been nice to be able to present this work at ISBA but I think I’d rather talk about the applied paper that discusses the spatio-temporal trends from the UPTECH project.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On to the next paper</title>
      <link>/./2012/04/11/on-to-the-next-paper/</link>
      <pubDate>Wed, 11 Apr 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/04/11/on-to-the-next-paper/</guid>
      <description>&lt;p&gt;I’ve just submitted the second paper of my PhD to my supervisors for what I hope to be a final round of feedback before we submit it. It comes in at just under twenty pages, which is what Annals of Applied Statistics sets as their usual maximum page limit. If there’s more to add I can probably turn some of the more theoretical stuff into supplementary material. Once we submit I’ll put the paper up on arXiv and link to it here so you can all have a look at how I think you should do hierarchical modelling in INLA for panel design data. Now that this paper’s more or less out of the way it’s time to go back to working on the paper that I’ve been working on with the Finnish team. It’s been a huge coding project but now the code is at a stage where, more or less, Bjarke and I are happy with it and it’s time to finalise the model we present as an example and write the analysis (the theory, discussion, etc. are mostly done). We still have to finalise where we’re going to send it but my hope is that we don’t have to pare away all the theory and end up sending it to an atmospheric science journal as an applied paper, because there has been an awful lot of work go into the modelling approach. I’m really looking forward to this one being done because it’s been going for about nine to ten months and it’s been on the backburner while I focussed on the INLA papers. It’s a very neat method and I hope the community agrees. I’ll also arXiv it and hope to release the code once the paper’s published.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Indian Buffet process</title>
      <link>/./2012/03/28/indian-buffet-process/</link>
      <pubDate>Wed, 28 Mar 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/03/28/indian-buffet-process/</guid>
      <description>&lt;p&gt;The NP Bayes reading group that I started is currently working through Griffiths and Ghahramani’s paper on the Indian Buffet process [1], an infinite latent feature model which is used in topic modelling, document clustering, pattern recognition, etc. It’s been pretty hard going because QUT doesn’t teach a lot of combinatorics and we’ve had some patchy attendance recently so we’ve had to go over things a few times (especially the Chinese Restaurant process, the left-ordering function). It’s been a very interesting paper and I am looking forward to the possibility of using it with some survey data. No doubt I’ll get a better understanding of it when I head to ISBA later in the year (&lt;a href=&#34;http://www2.e.u-tokyo.ac.jp/~isba2012/stsessions.html&#34;&gt;Nils Hjort has organised a session&lt;/a&gt; on Beta processes that will include talks from Yongdai Kim, Tamara Broderick and Sinead Williamson). If you’re interested in reading about it, we’ve been making notes on our Wiki. You can start from &lt;a href=&#34;https://wiki.qut.edu.au/display/npbayes/2012/03/&#34;&gt;here&lt;/a&gt; as it’s taken us all of this month so far. I’ve written some code snippets to do some of the things the paper mentions. [1] [Griffiths, T. L., and Ghahramani, Z. “Infinite Latent Feature Models and the Indian Buffet Process”, &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, 2005, &lt;strong&gt;18&lt;/strong&gt;, 475-82](&lt;a href=&#34;http://cocosci.berkeley.edu/tom/papers/ibptr.pdf&#34; class=&#34;uri&#34;&gt;http://cocosci.berkeley.edu/tom/papers/ibptr.pdf&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>git</title>
      <link>/./2012/03/14/git/</link>
      <pubDate>Wed, 14 Mar 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/03/14/git/</guid>
      <description>&lt;p&gt;A while ago I decided that it’d be worth ensuring that my PhD work was backed up on something other than my work computer. This ended up involving copying my work to my folder on a network drive, a Dropbox account and a portable hard drive, none of which were automated. I could have automated my use of Dropbox by working in my Dropbox folder, but that would be an awful lot of writing files over the internet whenever my LaTeXed papers were changed, whenever I saved an R data frame, etc. I eventually settled on &lt;a href=&#34;http://git-scm.com/&#34;&gt;git&lt;/a&gt;. While it’s not automated, it relies on committing changes rather than copying files. I find this a much more intuitive way of backing up code. When it’s worth keeping, commit it and push the changes. I’d previously worked with &lt;a href=&#34;http://subversion.tigris.org/&#34;&gt;SVN&lt;/a&gt; in undergrad and so git wasn’t a huge jump, and I really love the idea of version control as part of a backup strategy, because you will break your code. Probably the best part of using git is the &lt;a href=&#34;https://github.com/&#34;&gt;github&lt;/a&gt; website, where people can list open projects that you can fork and develop (like &lt;a href=&#34;https://github.com/rdenham/pymcmc&#34;&gt;pyMCMC&lt;/a&gt;, a python module for Bayesian inference, except this hasn’t been pushed publicly in about a year and I know they’re still working on it). I use git for two reasons&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;backing up and controlling versions of all the R, MATLAB and LaTeX files that are part of my PhD work&lt;/li&gt;
&lt;li&gt;collaborating on a project with someone who lives in Finland (MATLAB and LaTeX again)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So all my work is backed up online and I can share a project with my collaborator without fiddling around with permissions on my PhD project. I’m considering paying for the next level of account so that I can make a git project for each paper that I’m working on. If I go down this route, I would consider making such a project public once I have published the paper(s) that come out of that code. I really like the idea of social coding and think it would be a fascinating tool within a university context. I don’t know how much QUT does it at the moment, but if there was an internal github-type server where QUT researchers/students could host their code it would reduce the amount of code going back and forth over email. and make it much easier to bring people on to code-based projects. I have a feeling the Computer Science and Information Systems could well be using something like this (perhaps on a smaller scale and not for facilitating collaboration) but I think the mathematics, statistics and physics (they do computer simulations) academics could make use of these sorts of tools to increase the research capacity of the university.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
