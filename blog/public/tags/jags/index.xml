<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Jags on Sam Clifford </title>
    <link>/./tags/jags/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2015-01-12 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>Marrying differential equations and regression</title>
      <link>/./2015/01/12/marrying-differential-equations-and-regression/</link>
      <pubDate>Mon, 12 Jan 2015 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2015/01/12/marrying-differential-equations-and-regression/</guid>
      <description>&lt;p&gt;Professor Fabrizio Ruggeri (Milan) visited the Institute for Future Environments for a little while in late 2013. He has been appointed as Adjunct Professor to the Institute and gave a public talk with a brief overview of a few of his research interests. Stochastic modelling of physical systems is something I was exposed to in undergrad when a good friend of mine, Matt Begun (&lt;a href=&#34;http://www.sphcm.med.unsw.edu.au/research/infectious-diseases/phd-candidates&#34;&gt;who it turns out is doing a PhD&lt;/a&gt; under Professor Guy Marks, with whom ILAQH collaborates), suggested we do a joint Honours project where we each tackled the same problem but from different points of view, me as a mathematical modeller, him as a Bayesian statistician. It didn’t eventuate but it had stuck in my mind as an interesting topic. In SEB113 we go through some non-linear regression models and the mathematical models that give rise to them. Regression typically features a fixed equation and variable parameters and the mathematical modelling I’ve been exposed to features fixed parameters (elicited from lab experiments, previous studies, etc.) and numerical simulation of a differential equation to solve the system (as analytic methods aren’t always easy to employ). I found myself thinking “I wonder if there’s a way of doing both at once” and then shelved the thought because there was no way I would have the time to go and thoroughly research it. Having spent a bit of time thinking about it, I’ve had a crack at solving an ODE within a Bayesian regression model (Euler’s method in JAGS) for logistic growth and the Lotka-Volterra equations. I’ve started having some discussions with other mathematicians about how we marry these two ideas and it looks like I’ll be able to start redeveloping my mathematical modelling knowledge. This is somewhere I think applied statistics has a huge role to play in applied mathematical modelling. Mathematicians shouldn’t be constraining themselves to iterating over a grid of point estimates of parameters, then choosing the one which minimises some L&lt;sup&gt;&lt;em&gt;p&lt;/em&gt;&lt;/sup&gt;-norm (at least not without something like Approximate Bayesian Computation). I mean, why explore regions of the parameter space that are unlikely to yield simulations that match up with the data? If you’re going to simulate a bunch of simulations, it should be done with the aim of not just finding the most probable values but characterising uncertainty in the parameters. A grid of values representing a very structured form of non-random prior won’t give you that. Finding the maximum with some sort of gradient-based method will give you the most probable values but, again, doesn’t characterise uncertainty. Sometimes we don’t care about that uncertainty, but when we do we’re far better off using statistics and using it properly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The problem with p values</title>
      <link>/./2014/09/18/the-problem-with-p-values/</link>
      <pubDate>Thu, 18 Sep 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/09/18/the-problem-with-p-values/</guid>
      <description>&lt;p&gt;A coworker sent me &lt;a href=&#34;http://theconversation.com/the-problem-with-p-values-how-significant-are-they-really-20029?utm_medium=email&amp;amp;utm_campaign=Latest+from+The+Conversation+for+12+November+2013&amp;amp;utm_content=Latest+from+The+Conversation+for+12+November+2013+CID_036de2eab3f9b92457ea3d5b919247bc&amp;amp;utm_source=campaign_monitor&amp;amp;utm_term=The%20problem%20with%20p%20values%20how%20significant%20are%20they%20really&#34;&gt;this article&lt;/a&gt; about alternatives to the default 0.05 p value in hypothesis testing as a way to improve the corpus of published articles so that we can actually expect reproducability and have a bit more faith that these results are meaningful. The article is based on a paper published in the &lt;a href=&#34;http://www.pnas.org/content/early/2013/10/28/1313476110.abstract&#34;&gt;Proceedings of the National Academy of Sciences&lt;/a&gt; which talks about mapping Bayes Factors to p values for hypothesis tests so that there’s a way to think about the strength of the evidence. The more I do and teach statistics the more I detest frequentist hypothesis testing (including whether a regression coefficient is zero) as a means of describing whether or not something plays a “significant” role in explaining some physical phenomenon. In fact, the entire idea of statistical significance sits ill with me because the way we tend to view it is that 0.051 is not significant and 0.049 is significant, even though there’s only a very small difference between the two. I guess if you’re dealing with cutoffs you’ve got to put the cutoff somewhere, but turning something which by its very nature deals with uncertainty into a set of rigid rules about what’s significant and what’s not seems pretty stupid. &lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;My distaste for frequentist methods means that even for simple linear regressions I’ll fire up JAGS in R and fit a Bayesian model because I fundamentally disagree with the idea of an unknown but fixed true parameter. Further to this, the nuances of p values being distributed uniformly under the Null hypothesis means that we can very quickly make incorrect statements.&lt;/span&gt; I agree with the author of the article that shifting hypothesis testing p value goal posts won’t achieve what we want and I’ll have a bit closer a read of the paper. For the time being, I’ll continue to just mull this over and grumble when people say “statistically significant” without any reference to a significance level. NB: this post has been in an unfinished state since last November, when the paper started getting media coverage.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lotka-Volterra and Bayesian statistics and teaching</title>
      <link>/./2014/08/27/lotka-volterra-and-bayesian-statistics-and-teaching/</link>
      <pubDate>Wed, 27 Aug 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/08/27/lotka-volterra-and-bayesian-statistics-and-teaching/</guid>
      <description>&lt;p&gt;One of the standard population dynamics models that I learned in my undergrad mathematical modelling units was the Lotka-Volterra equations. These represent a very simple set of assumptions about populations, and while they don’t necessarily give physically realistic population trajectories they’re an interesting introduction to the idea that differential equations systems don’t necessarily have an explicit solution. The assumptions are essentially: prey grow exponentially in the absence of predators, predation happens at a rate proportional to the product of the predator and prey populations, birth of predators is dependent on the product of predator and prey populations, predators die off exponentially in the absence of prey. In SEB113 we cover non-linear regressions, the mathematical models that lead to them, and then show that mathematical models don’t always yield a nice function. We look at equilibrium solutions and then show that we orbit around it rather than tending towards (or away from) it. We also look at what happens to the trajectories as we change the relative size of the rate parameters. Last time we did the topic, I posted about using the logistic growth model for our Problem Solving Task and it was pointed out to me that the model has a closed form solution, so we don’t explicitly need to use a numerical solution method. This time around I’ve been playing with using Euler’s method inside JAGS to fit the Lotka-Volterra system to some simulated data from sinusoidal functions (with the same period). I’ve put a bit more effort into the predictive side of the model, though. After obtaining posterior distributions for the parameters (and initial values) I generate simulations with lsode in R, where the parameter values are sampled from the posteriors. The figure below shows the median and 95% CI for the posterior predictive populations as well as points showing the simulated data. &lt;a href=&#34;https://samcliffordinfo.files.wordpress.com/2014/08/lv2.png&#34;&gt;&lt;img src=&#34;lv2.png?w=300&#34; alt=&#34;lv&#34; /&gt;&lt;/a&gt;The predictions get more variable as time goes on, as the uncertainty in the parameter values changes the period of the cycles that the Lotka-Volterra system exhibits. This reminds me of a chat I was having with a statistics PhD student earlier this week about sensitivity of models to data. The student’s context is clustering of data using overfitted mixtures, but I ended up digressing and talking about Edward Lorenz’s discovery of chaos theory through a meteorological model that was very sensitive to small changes in parameter values. The variability in the parameter values in the posterior give rise to the same behaviour, as both Lorenz’s work and my little example in JAGS involve variation in input values for deterministic modelling. Mine was deliberate, though, so isn’t as exciting or groundbreaking a discovery as Lorenz but we both come to the same conclusion: forecasting is of limited use when your model is sensitive to small variations in parameters. As time goes on, my credible intervals will likely end up being centred on the equilibrium solution and the uncertainty in the period of the solution (due to changing coefficient ratios) will result in very wide credible intervals. It’s been a fun little experiment again, and I’m getting more and more interested in combining statistics and differential equations, as it’s a blend of pretty much all of my prior study. The next step would be to use something like MATLAB with a custom Gibbs/Metropolis-Hastings scheme to bring in more of the computational mathematics I took. It’d be interesting to see if there’s space for this sort of modelling in the Mathematical Sciences School’s teaching programs as it combines some topics that aren’t typically taught together. I’ve heard murmurings of further computational statistics classes but haven’t been involved with any planning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/06/24/posterior-samples/</link>
      <pubDate>Mon, 24 Jun 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/06/24/posterior-samples/</guid>
      <description>&lt;p&gt;I was a bit disappointed that it wasn’t about machines doing automatic analysis for us, but this article, “&lt;a href=&#34;http://simplystatistics.org/2013/06/14/the-vast-majority-of-statistical-analysis-is-not-performed-by-statisticians/&#34;&gt;The vast majority of statistical analysis is not performed by statisticians&lt;/a&gt;”, is a bit of a wake-up call for those statisticians who haven’t realised that we need to improve the way we teach statistics and interact with non-statisticians. I don’t think we have enough statisticians in the world to do all the analysis that needs doing, so we need to focus on training scientists and others better so that we don’t leave them stuck in a culture of bad regression and t-tests in Excel. Gianluca Baio (UCL) has &lt;a href=&#34;http://www.statistica.it/gianluca/Talks/INLA.pdf&#34;&gt;a really nice introduction&lt;/a&gt; to &lt;a href=&#34;http://www.r-inla.org/&#34;&gt;INLA&lt;/a&gt; with a comparison to &lt;a href=&#34;http://mcmc-jags.sourceforge.net/&#34;&gt;JAGS&lt;/a&gt;. I started using JAGS when WinBUGS/OpenBUGS was becoming too slow for the analysis I was doing but the major paper of my thesis uses INLA for spatio-temporal analysis. I still use both programs and when faced with a new problem will usually start in JAGS as it’s quite flexible in the way you set up priors. INLA has its advantages as well, one of them being that it will fit a Poisson likelihood to non-integer data very well. There’s &lt;a href=&#34;http://blogs.plos.org/attheinterface/2013/06/19/why-art-and-science/&#34;&gt;a neat little article on the PLoS blog&lt;/a&gt; about linkages between art and science and how the involvement of art in research (beyond making prettier plots, which is really more an issue of design than art) can lead to better scientific outcomes. Radford Neal has just announced &lt;a href=&#34;http://radfordneal.wordpress.com/2013/06/22/announcing-pqr-a-faster-version-of-r/&#34;&gt;pqR&lt;/a&gt;, “pretty quick R”, which is designed to make use of multiple cores wherever possible and avoid unnecessarily onerous computation. It’s not available for Mac/Windows yet, so I won’t be able to look at it for the time being. I wonder if QUT’s HPC group would consider making it available on the supercomputer.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Something I&#39;ve learned for BUGS/JAGS</title>
      <link>/./2013/03/19/something-ive-learned-for-bugs/jags/</link>
      <pubDate>Tue, 19 Mar 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/03/19/something-ive-learned-for-bugs/jags/</guid>
      <description>&lt;p&gt;Just quickly, I’m using JAGS with rjags to run some models for this fungal concentration paper I’m writing with our Finnish visitor (who leaves in a few months!). There are three levels of hierarchy in the experiment&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span style=&#34;line-height:12px;&#34;&gt;school (i = 1 to 25)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;measurement site within school (j = 1 to 3)&lt;/li&gt;
&lt;li&gt;sample number at measurement site (k = 1 to K&lt;sub&gt;j&lt;/sub&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;which mean that I was constructing my data frame as a three dimensional array, y[i,j,k] where there were uneven numbers of samples at each site within each school. Furthermore, modelling y[i,j,k] ~ dnorm(mu[i,j,k],tau.y) was giving me problems because I couldn’t tell JAGS to monitor a three dimensional parameter. I figured out that it’d just be easier to treat the levels of measurement site and sample number as covariates that I could use as index counters in JAGS. I’ve got much simpler code now and JAGS is monitoring mu[i] (i = 1 to 261). Edit: It’s been a busy month. I hope to finally publish that blog post about SEB113 soon.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
