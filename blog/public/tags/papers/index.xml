<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Papers on Sam Clifford </title>
    <link>/./tags/papers/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2014-11-05 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>Two pieces of good news this week</title>
      <link>/./2014/11/05/two-pieces-of-good-news-this-week/</link>
      <pubDate>Wed, 05 Nov 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/11/05/two-pieces-of-good-news-this-week/</guid>
      <description>&lt;p&gt;The full paper from the EMAC2013 conference last year is now available online. If you’re interested in the statistical methodology we used for estimating the inhaled dose of particles by students in the UPTECH project, you should check out our paper at the &lt;a href=&#34;http://journal.austms.org.au/ojs/index.php/ANZIAMJ/article/view/7819&#34;&gt;ANZIAM Journal&lt;/a&gt; (click the link that says “PDF” down the bottom under Full Text). More importantly, though, we were successful in applying for an ARC Discovery Project! This project will run for three years and combines spatio-temporal statistical modelling, sensor miniaturisation and mobile phone technologies to allow people to minimise their exposure to air pollution. Our summary of the project, from &lt;a href=&#34;http://www.arc.gov.au/pdf/DP15/DP15_Listing_by_State_and_Org.pdf&#34;&gt;the list of successful projects&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This interdisciplinary project aims to develop a personalised air pollution exposure monitoring system, leveraging the ubiquitousness and advancements in mobile phone technology and state of the art miniaturisation of monitoring sensors, data transmission and analysis. Airborne pollution is one of the top contemporary risks faced by humans; however, at present individuals have no way to recognise that they are at risk or need to protect themselves. It is expected that the outcome will empower individuals to control and minimise their own exposures. This is expected to lead to significant national socioeconomic benefits and bring global advancement in acquiring and utilising environmental information.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Other people at ILAQH were also successful in getting a Discovery Project grant looking at new particle formation and cloud formation in the Great Barrier Reef. I won’t be involved in that project but it sounds fascinating.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Revising another paper</title>
      <link>/./2014/09/11/revising-another-paper/</link>
      <pubDate>Thu, 11 Sep 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/09/11/revising-another-paper/</guid>
      <description>&lt;p&gt;We got a paper back from the reviewers a few days ago and there are some comments requesting revisions to the explanation of the statistical methods and the analysis. It’s interesting coming back to this paper, about a year after I last saw it (it’s been sent around to a few different journals to try to find a home for it). The PhD student who is the main author got into R and ggplot2 last year and has done some good work with linear mixed effects models and visualisation but some of the plots are the same sort of thing one might do in Excel (lots of boxplots next to each other rather than making use of small multiples). So now I get to delve back into some data and analysis that’s about a year old with fresh eyes. Having done more with ggplot2 over the last 12 months, there are some things that I will definitely change about this. The student and I had a chat this morning about how to tackle it, and we’re trying to choose the best way to split up our data for visualisation: 6 treatments, 4 measurement blocks, two different measures (PM2.5 mass concentration and PNC), a total of 48 boxplots, density plots or histograms. A paper with another PhD student has had its open discussion finalised now, which means more writing is probably going to happen. I find &lt;a href=&#34;http://www.atmos-chem-phys.net/&#34;&gt;ACP&lt;/a&gt;’s model quite interesting. The articles are peer reviewed, published for discussion, and then revised by the authors for final publication. I guess it spreads the review work out a bit and allows for multiple voices to be heard before final publication, each with a different approach and background.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Some good news</title>
      <link>/./2013/11/29/some-good-news/</link>
      <pubDate>Fri, 29 Nov 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/11/29/some-good-news/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://pubs.acs.org/doi/abs/10.1021/es403721w&#34;&gt;Our personal sampling paper got accepted for publication&lt;/a&gt; &lt;span style=&#34;line-height:1.714285714;font-size:1rem;&#34;&gt;quite quickly once we’d made the changes the reviewers required. This paper came from almost nothing about a year and a half ago to a really nice piece of mathematical and statistical modelling that will give us a lot to work with in future UPTECH papers, where we’ll be looking at the relationship between inhaled dose and child health. I’m presenting this work at the EMAC2013 conference next week (gotta write slides!) so it’s good to have the paper online now.&lt;/span&gt; My current post-doc is drawing to a close at the end of the year but I’ve got the contract in the email for my position for the next few years. After that, who knows? We got a grant under an international exchange program which means I may be off overseas for a few weeks/months during my next appointment. I’m looking forward to being able to spend some time outside the country at another university, having got a bit more experience now than when I went to Finland about two years in to my PhD. And it looks like I’m about to get my second PhD student to co-supervise.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Use of credible intervals in scientific papers</title>
      <link>/./2013/11/04/use-of-credible-intervals-in-scientific-papers/</link>
      <pubDate>Mon, 04 Nov 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/11/04/use-of-credible-intervals-in-scientific-papers/</guid>
      <description>&lt;p&gt;I just got comments back from reviewers for two papers that our group has submitted. I’ve done the statistics on both papers and given that Bayesian statistical models have been used in both papers (and we’ve been pretty up front about it) we’ve used credible intervals to discuss the uncertainty in our parameter estimates. A comment I’ve received from a few reviewers (and indeed co-authors) is something along the lines of&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;please replace all instances of ‘credible interval’ with ‘confidence interval’.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While I don’t expect every scientist to be intimately familiar with Bayesian statistics, its philosophy and the fundamental differences between confidence and credible intervals, the repeated use of a term would tend to indicate that it’s not a one-off typo. If repeatedly confronted in a paper with a term that one is not familiar with, surely the prudent course of action is to do a quick Google search to see what that term means in order to better understand the paper that one is reviewing. While the war within statistics over whether the Bayesian approach is actually statistically valid is over, awareness of it in the sciences seems to be lagging far behind. Air quality journals do publish papers that have a Bayesian flavour to the statistics and I have a feeling that just about every author that submits a paper with the correct way of summarising parameter uncertainty in a Bayesian model will face the same comment, “please replace all instances of ‘credible interval’ with ‘confidence interval’”.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Becoming a grown up (at least a grown-up academic)</title>
      <link>/./2013/08/30/becoming-a-grown-up-at-least-a-grown-up-academic/</link>
      <pubDate>Fri, 30 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/30/becoming-a-grown-up-at-least-a-grown-up-academic/</guid>
      <description>&lt;p&gt;There are only four months left in my current postdoctoral appointment and I’m discussing plans for next year with my supervisor. There’s still a lot of unfinished UPTECH work that needs to happen, including helping a handful of PhD students with the stats in their papers for their thesis, dealing with the clinical data and putting together some research plans for what to do next. The plan at the moment is to look for some funding both internally and externally to provide a research appointment. I’m also interested in continuing lecturing next year, whether in SEB113 or another mathematics/statistics unit. Most of ILAQH is away as of today or tomorrow, as they travel to Prague for the &lt;a href=&#34;http://eac2013.cz/&#34;&gt;2013 European Aerosol Conference&lt;/a&gt;. The work that I’ve been doing with some colleagues from ILAQH and Italy, on personal sampling, will be featured on a poster. The paper has been submitted to ES&amp;amp;T but hasn’t been accepted yet, so unfortunately I can’t put a preprint up to show off the cool statistics that I had to learn to do the modelling in the paper. As a result of everyone being away, I’ll be one of two academic staff members left here. It’s going to be quiet, with most of the staff and a few PhD students gone. Barring the Finnish paper that I’m still revising, this personal sampling paper has been the paper which has required the most creative and original programming as there have been many different steps along the way. I am particularly proud of this paper and the work that went into it. When I was first brought on board there didn’t appear to be much clarity regarding what we wanted to investigate; we had a lot of personal sampling data but didn’t quite know what to do with it. I think the paper we’ve developed does service to the amount of work that was put into collecting the data and is aligned with what the UPTECH project was set up to do. I’m grateful to all co-authors on the paper (and everyone who was out there in the schools) for the work that they put into bringing this to fruition. I’m still finishing the final corrections for my thesis, due in a few weeks time. After that’s handed in I’ll be taking another step in becoming a grown-up academic: supervising a PhD student. I’ll be the replacement associate supervisor for a student whose original associate supervisor has moved from QUT to another university. QUT requires an internal primary and associate supervisor and I’m the one most familiar with the modelling that this student is doing as part of their thesis. We’ve already set a meeting schedule for the time when his primary supervisor is overseas and have discussed what sort of things I’ll expect to see. It’s a strange responsibility to have for someone who’s only just finishing up their thesis. I wonder how long it will be until I’m the primary supervisor for a student. Two years? Five? Ten? Worrying about funding, writing grant applications, supervising students, lecturing (writing assessment!). It’s a strange place to find oneself.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New papers on the way</title>
      <link>/./2013/08/17/new-papers-on-the-way/</link>
      <pubDate>Sat, 17 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/17/new-papers-on-the-way/</guid>
      <description>&lt;p&gt;In addition to the Endotoxin paper which was accepted after revisions earlier this week, we are very close to submitting papers on the following topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;trends in and ratios of indoor and outdoor PNC in the UPTECH schools&lt;/li&gt;
&lt;li&gt;24 hour personal sampling and dose metrics for children in the UPTECH schools&lt;/li&gt;
&lt;li&gt;modern Bayesian spatial statistical techniques&lt;/li&gt;
&lt;li&gt;particle losses by size in diffusion dryers and thermodenuders&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to these, a few papers are nearing completion but are not yet ready for submission. These include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;fungus concentrations in the UPTECH schools&lt;/li&gt;
&lt;li&gt;flame retardants in dust in the UPTECH schools&lt;/li&gt;
&lt;li&gt;particle emission and deposition rates for indoor sources in the UPTECH schools&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, yes, my plate has been rather full recently with all the various research activities in which I’ve been enlisted. These are all fascinating examples of the interaction of statistics and science and have all required different approaches to quantify what’s going on. Some of these have been fairly obvious modifications to things I’ve already done and some have required me to look quite deeply into new approaches and figure out both what the model is and how we go about implementing it. I am particularly happy with coming to terms with the combination of a Poisson regression model for total counts and a multinomial for proportions within that count, particularly when some of the classes in the multinomial may have zero counts. That model was a challenge but and I think the paper that it gets used in is much stronger than similar papers which have just taken a very naive approach to analysing the data. One of the biggest challenges with this particular paper has been communicating with the scientists I’m working with. These people are definitely not Bayesian statisticians and don’t come from a medical science background, which tends to be more accepting of complex models that replace ANOVA. I’ve had to write a few paragraphs for the flame retardants in dust paper that describe the model we’re using. Hopefully that helps make the analysis more accessible to the co-authors but also those reading the paper who haven’t got postgraduate qualifications in statistics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conference posters and an accepted paper</title>
      <link>/./2013/08/09/conference-posters-and-an-accepted-paper/</link>
      <pubDate>Fri, 09 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/09/conference-posters-and-an-accepted-paper/</guid>
      <description>&lt;p&gt;Thomas Lumley &lt;a href=&#34;http://notstatschat.tumblr.com/post/57747270796/speed-sessions-at-jsm-2013&#34;&gt;has some thoughts&lt;/a&gt; on poster sessions at JSM 2013. Healthy Buildings 2012 had something I hadn’t seen before - poster presenters were given two minutes at the end of the technical session most relevant to their poster to describe the work they are presenting in the poster session immediately after the talk slot. This gave poster presenters a small taste of presenting at a conference without them needing to prepare a full talk. QUT’s Nano and Molecular Sciences discipline had a poster session during its one day symposium a few months ago and the posters were run off &lt;a href=&#34;http://www.thecube.qut.edu.au/&#34;&gt;The Cube&lt;/a&gt;, which allowed people to zoom and rotate a static image of their poster (PDF preferred). Our group will have a few posters at the &lt;a href=&#34;http://eac2013.cz/&#34;&gt;European Aerosol Conference&lt;/a&gt; in a few weeks. Mandana Mazaheri and I have been discussing the issues of transporting posters back and forth internationally, including whether or not to print on cloth and the unwieldy nature of poster tubes. I am a big fan of mailing your poster home once it’s presented but a cloth poster can be folded up and put in your luggage and you can just give it a quick iron before presenting it. I’ve also seen way too many posters that are too busy and have gradient backgrounds. Hopefully by teaching SEB113 students about the principles of good visualisation of data QUT can produce graduates who know not to make ugly posters. &lt;a href=&#34;http://pubs.acs.org/doi/abs/10.1021/es4023706&#34;&gt;Our endotoxin paper got accepted in Environmental Science &amp;amp; Technology&lt;/a&gt; after a frantic couple of days of finalising amendments and responses to reviewer comments. This paper gave me a much better understanding of Bayesian hierarchical linear models and I’m very happy with how the paper turned out. The next step is to resubmit our fungus paper, which includes similar modelling but also uses a Multinomial model with Dirichlet prior to look at the proportions of different fungal genera across the UPTECH schools. There’s yet another paper looking at chemicals in floor dust which we’re also finalising that uses a similar methodology to the fungus paper but has its own subtleties due to some chemicals not being present across all schools.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistics and microbiology</title>
      <link>/./2013/05/21/statistics-and-microbiology/</link>
      <pubDate>Tue, 21 May 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/05/21/statistics-and-microbiology/</guid>
      <description>&lt;p&gt;I’ve picked up a hobby over the last few months that is paying delicious dividends: homebrewing. It’s something I’d been wanting to try since about this time last year and I finally dropped the money (a cooking store voucher) on a cider homebrewing kit in February. My first batch was an apple cider that came with the kit and it’s been improving with age since the first bottle was opened in late February/early March. The second batch was a pear cider that a friend asked me to make for her; it was divided into two batches after primary fermentation so that I could try something different with the “excess”. The resulting pear and berry cider will make its debut quite soon, as it’s been patiently settling and aging over the last three weeks or so. While I haven’t been keeping time series of the specific gravity, temperature and colour of the cider as it brews, there is certainly grounds to do so. Brewing and statistics have a history which goes back at least as far as William Sealy Gosset, who developed the t-distribution (and test) under the name “Student” while working at the Guinness brewery in 1908. Brewing involves balancing complex ecosystems of a whole lot of different things (depending on what you’re making) and is essentially a giant biochemical experiment. To get properly into brewing requires an understanding of botany, chemistry, microbiology, physics and statistics as you attempt to turn your basic ingredients into something which is tasty, non-toxic and perhaps even effervescent. I would like to start brewing beer at home soon, which will no doubt lead to me reading a lot more about hops, malt, wort, grains and yeasts and taking more fastidious notes. So my exposure to microbiology has been twofold over the last year; working with a Finnish colleague on papers dealing with fungus and endotoxin counts in the UPTECH project and brewing my own alcoholic cider at home. The main fungus paper has been submitted and we’re checking the modelling on the endotoxin paper so that it can be submitted before this colleague leaves in the next few days. I can’t think of a more fitting thing to bring to her farewell party than a drinkable microbiology experiment. Bonus link: &lt;a href=&#34;http://www.reddit.com/r/Homebrewing/comments/1eghjq/op_delivers_wild_harvested_some_microbes_and_made/&#34;&gt;Homebrewing redditor who works in a microbiology lab discovers a new strain of fungus which produces the best beer he’s ever homebrewed&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Meta-analysis? Meta-regression?</title>
      <link>/./2013/05/14/meta-analysis-meta-regression/</link>
      <pubDate>Tue, 14 May 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/05/14/meta-analysis-meta-regression/</guid>
      <description>&lt;p&gt;Meta-analysis with a covariate feels really weird. I’m wanting to compare the relationship between the distributions of the mean concentration of endotoxin in the air and in dust samples across 50 locations. I wasn’t sure I did it the right way but the posterior estimates are consistent with my naïve approach of regressing the means of the air and dust samples. It’s important to account for the variability when doing this sort of &lt;em&gt;post hoc&lt;/em&gt; analysis because a point estimate of the mean doesn’t reflect anywhere near the full set of knowledge you have about your parameters of interest. On an unrelated note, another UPTECH paper has been &lt;a href=&#34;http://pubs.acs.org/doi/abs/10.1021/es400041r&#34;&gt;published&lt;/a&gt;. This one’s looking at spatial variation of particle number concentration in the school environment. Congratulations to Farhad Salimi, the first author of this paper, on the publication of his first paper. Farhad’s one of the PhD students on the UPTECH project and is due to finish his thesis later this year. I’ve worked with him on two of his papers (this one and another which has been submitted) and he’s really thrown himself into learning how to use R. This has not only made it easier for me to collaborate with him but it’s also made his analysis possible.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I know the impact factor&#39;s not the be all and end all, but...</title>
      <link>/./2013/05/09/i-know-the-impact-factors-not-the-be-all-and-end-all-but.../</link>
      <pubDate>Thu, 09 May 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/05/09/i-know-the-impact-factors-not-the-be-all-and-end-all-but.../</guid>
      <description>&lt;p&gt;In Australia, at least, the impact factor of the journals you publish in plays a large role in your advance in academia. Universities are always under pressure to publish their research in more prestigious journals, conflating the impact factor of the journal and the impact of the research published in it. There are many ways journals can game their impact factor, many ways researchers can game the indices that describe the impact of their work, etc. That said, it’s always good to aim to produce research that will be accepted in a high quality journal. I’ve been excited about the PLoS journals since their launch and I believe QUT is a subscribing member, which means our publication fees are covered. It’s one of the best Open Access journal groups around and doesn’t appear to be a cash grab like some other publishers who are attempting to use Open Access as a business model to increase profits rather than because they believe in the free dissemination of research. UPTECH collected fungi and endotoxin data at the 25 schools, and we’re about to submit the fungi paper (which means work must continue on the endotoxin paper). I was considering whether we should submit to &lt;a href=&#34;http://www.plosone.org/&#34;&gt;PLoS One&lt;/a&gt; (IF 2011: 4.092) and then had a look at what other journals they have which may be an appropriate home. I really think once we get the clinical data from &lt;a href=&#34;http://www.woolcock.org.au/&#34;&gt;our Southern collaborators&lt;/a&gt; we should aim to do the best statistical modelling we can. I’m heartened by the fact that the head of the clinical group we’re working with has a strong background in stats and a desire to learn more Bayesian statistics. I don’t know if we can pull it off, but the prospect of having something investigating the role of fungi and endotoxin on child health published in &lt;a href=&#34;http://www.plospathogens.org/&#34;&gt;PLoS Pathogens&lt;/a&gt; (IF 2011: 9.172) is exhilarating.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A proud moment</title>
      <link>/./2013/04/17/a-proud-moment/</link>
      <pubDate>Wed, 17 Apr 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/04/17/a-proud-moment/</guid>
      <description>&lt;p&gt;Today a collaborator of mine started the outline of a new paper in LaTeX, put it on our git repository and emailed it as a PDF file to other people who will be co-authors on this paper. The text of the email included:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I wrote this in LaTeX, which generates pdf files. Therefore, I do not have a Word document for this. Apologies for any inconvenience this may cause.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I’ve been working with this collaborator to get them using Github, R and LaTeX . There was the denial, the anger, bargaining, depression and ultimately acceptance. Watching this collaborator send this email I felt like a proud father watching their child graduate.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Major achievements this week</title>
      <link>/./2013/02/08/major-achievements-this-week/</link>
      <pubDate>Fri, 08 Feb 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/02/08/major-achievements-this-week/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;span style=&#34;line-height:12px;&#34;&gt;final draft of final thesis paper ready to be submitted (pending feedback from one co-author)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;final draft of thesis ready to be submitted (pending feedback from one supervisor)&lt;/li&gt;
&lt;li&gt;got the code working properly for a paper I’m second author on, looking at deposition of ultrafine particles within the lung. While not as big a coding task as the Finnish paper this has been a major slog over the last few weeks.&lt;/li&gt;
&lt;li&gt;pretty much sorted out the analysis and graphs for the paper on fungal counts indoor and outdoor in the UPTECH (for which I’m author some way down the line)&lt;/li&gt;
&lt;li&gt;appointment form submitted for my postdoc.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>GitHub</title>
      <link>/./2012/11/21/github/</link>
      <pubDate>Wed, 21 Nov 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/11/21/github/</guid>
      <description>&lt;p&gt;GitHub for Windows can be such a pain sometimes. I guess it’s partially my fault for attempting to use version control on the compiled PDF of a LaTeX document, but I spent a fair amount of time today attempting to fix up a colleague’s local repository. I’m now a bit more familiar with cherry-pick and rebase but it would have been nice to have it just work. For some reason, GitHub for Windows on this colleague’s computer simply &lt;em&gt;will not sync properly&lt;/em&gt;; my colleague has had to become a bit more familiar with the common commands (push, pull, fetch, commit, merge). It works fine on their Mac, though. I run GitHub for OS X at home and it’s an absolute dream. At work (Windows XP) I have had no end of trouble with various programs like TortoiseGit. I think when I start my post-doc I’ll organise to have my computer converted to a Linux system. After all that, though, we did make some pretty good progress on the modelling in this paper. I’m not quite sure which journal we’ll be sending it to but it’s a really nice piece of work with some personal monitoring data, simple but informative analysis and some very creative use of the base graphics system in R.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I seem to be taking on a lot of work right now</title>
      <link>/./2012/11/12/i-seem-to-be-taking-on-a-lot-of-work-right-now/</link>
      <pubDate>Mon, 12 Nov 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/11/12/i-seem-to-be-taking-on-a-lot-of-work-right-now/</guid>
      <description>&lt;p&gt;For someone who’s meant to be finishing his PhD I sure do have a lot of other peoples’ papers on my plate. Today and Friday I had a chat with another PhD student from the UPTECH project about looking for spatial variation within the UPTECH schools. We’ve got some divergent ideas about how to go about it but we sat down this afternoon and spent some time going over regression modelling versus exploratory/summary statistics and how we can move from using Spearman’s rank correlation coefficient to doing non-parametric function estimation. I wrote some code and commented it as we went, so it should be fairly straightforward to write the accompanying methodology subsection for the paper. I’m going to spend the week focussing on my final thesis paper (a spatio-temporal model for data from a split panel design). I got some comments back from my supervisors last week and there’s a lot to be done checking certain sources of variation. I spent a fair bit of time today looking over a former students’ papers, checking old email threads, discussing a few things with the co-authors of his papers who are still around and have even been tracking down which particular instruments were used to perform the measurements. It’ll be important to check that my spatial analysis of the within-school variation matches up with the other UPTECH student’s analysis. If we come to opposite conclusions then at least one of us is wrong. I’m now tossing up whether I should check my results from the supercomputer or wait until tomorrow.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New paper from Trondheim&#39;s INLA group (arXiv)</title>
      <link>/./2012/10/03/new-paper-from-trondheims-inla-group-arxiv/</link>
      <pubDate>Wed, 03 Oct 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/10/03/new-paper-from-trondheims-inla-group-arxiv/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/1210.0333&#34;&gt;Martins, Simpson, Lindgren and Rue&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The INLA approach for approximate Bayesian inference for latent Gaussian models has been shown to give fast and accurate estimates of posterior marginals and also to be a valuable tool in practice via the R-package R-INLA. In this paper we formalize new developments in the R-INLA package and show how these features greatly extend the scope of models that can be analyzed by this interface. We also discuss the current default method in R-INLA to approximate posterior marginals of the hyperparameters using only a modest number of evaluations of the joint posterior distribution of the hyperparameters, without any need for numerical integration.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I’m using R-INLA for two UPTECH papers at the moment and I’m really interested in the replication functionality in 3.2 and the Kronecker functionality in 3.6, which seems to be a more “official” version of the work I’ve been doing rolling my own precision matrices for use with the generic0 model class. R-INLA is definitely a worthwhile piece of software to learn to use and the stochastic PDE model for spatial inference is one of the coolest things I’ve seen in terms of spatial modelling. I wonder if the Kronecker stuff in 3.6 will let me define a separable product of a stochastic PDE model and a model for the time series component.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Third time&#39;s a charm</title>
      <link>/./2012/09/25/third-times-a-charm/</link>
      <pubDate>Tue, 25 Sep 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/09/25/third-times-a-charm/</guid>
      <description>&lt;p&gt;After getting rejected by an applied statistics journal and a more methodological statistics journal we’ve decided to send the Finnish paper to an environmental statistics journal. I think it’s a more natural home for this paper as it straddles the boundary of model development and the analysis of observational environmental data. A new version of the preprint will appear &lt;a href=&#34;http://arxiv.org/abs/1207.0558&#34;&gt;on arXiv&lt;/a&gt; in about 12 hours. I’m also still redrafting the first UPTECH paper of my thesis. With the Finnish paper resubmitted it’ll be the focus of my work over the next few days. I think the modelling is now much improved (although I might try an alternative specification) after taking reviewer comments into account from its earlier rejection. Once I’ve got this last modelling question sorted out and the analysis is written up (not much has to change in the methodology section) I can restart work on my final thesis paper. There are two similar papers coming up in my immediate post-thesis future where I’m going to have the chance to learn more about the use of Gaussian processes. While INLA does some nice GMRF approximations for Gaussian fields [1] there are some challenges that we’ll face with this data. I might also need to read up on the use of dynamic linear models for modelling the evolution of a parameter with time. [1] Lindgren, F.; Rue, H. &amp;amp; Lindström, J. (2011) An explicit link between Gaussian fields and Gaussian Markov random fields: the stochastic partial differential equation approach, _Journal of the Royal Statistical Society: Series B (Statistical Methodology)__ 73_, 423-498. &lt;a href=&#34;http://www.math.ntnu.no/inla/r-inla.org/papers/spde-jrssb-revised.pdf&#34;&gt;R-INLA&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UPTECH paper review</title>
      <link>/./2012/09/19/uptech-paper-review/</link>
      <pubDate>Wed, 19 Sep 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/09/19/uptech-paper-review/</guid>
      <description>&lt;p&gt;I’m currently reworking the &lt;a href=&#34;http://arxiv.org/abs/1206.3833&#34;&gt;second paper&lt;/a&gt; of my thesis (or at least the paper that was planned as the second paper) to address the reviewers’ comments. Probably the biggest problem with the split panel design we’ve ended up with is that there are no measurements from the long term monitoring stations at the same time as the first ten schools’ measurements are being taken. So, what’s due to annual variation [1] and what’s due to spatial variation, e.g. [2]? This has meant going back to our models and looking at more appropriate ways to model the spatial and temporal trends to allow us to decouple the spatial and temporal variation. I’ve still got to finalise things with my supervisors, but I think if we focus less on model comparison and talk more about the computational issues and put forward a single model (which will be used as the basis for the third paper of my PhD) then we’ll have a good paper for a journal which focuses more on the scientific application than the statistical theory. After all, the paper is not a novel contribution to the field of semi-parametric regression but it’s a very novel application of semi-parametric regression to multi-site observational data recorded from a split panel design. All in all I think the paper is much more at home in the aerosol science literature than the applied statistics literature and the comments we got upon the previous rejection will make it a better paper. I’m certainly much more optimistic about this paper than I was immediately after its rejection. I showed some of the results of the new modelling to some colleagues today and while the interpretation of the graphs wasn’t immediately obvious (I showed them pictures without explanatory text) they seemed interested in what it said about the UPTECH study. [1] Mejía, Jaime F., Wraith, Darren E., Mengersen, Kerrie L., &amp;amp; Morawska, Lidia (2007) Trends in size classified particle number concentration in subtropical Brisbane, Australia, based on a 5 year study. Atmospheric Environment, 41(5), pp. 1064-1079. &lt;a href=&#34;http://eprints.qut.edu.au/5778/&#34;&gt;QUT ePrints&lt;/a&gt;. [2] Mejía, Jaime F., Morawska, Lidia, &amp;amp; Mengersen, Kerrie L. (2008) Spatial variation in particle number size distributions in a large metropolitan area. Atmospheric Chemistry and Physics, 8(5), pp. 1127-1138. &lt;a href=&#34;http://eprints.qut.edu.au/15395/&#34;&gt;QUT ePrints&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rejection of second paper</title>
      <link>/./2012/09/03/rejection-of-second-paper/</link>
      <pubDate>Mon, 03 Sep 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/09/03/rejection-of-second-paper/</guid>
      <description>&lt;p&gt;This morning I heard back from Annals of Applied Stats about the rejection of a paper I’d submitted there maybe a month ago. Again, the reviewer comments are insightful and helpful and will help improve the paper for its publication in an environmental science journal. I find that reading the reviewer comments is a great way to overcome the crushing feeling one gets as a young scientist when dealing with a letter of rejection of a paper.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unfinnished business</title>
      <link>/./2012/08/30/unfinnished-business/</link>
      <pubDate>Thu, 30 Aug 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/08/30/unfinnished-business/</guid>
      <description>&lt;p&gt;I’ve just heard from Kerrie Mengersen that the Finnish paper got rejected by BA for not being novel enough to publish there. So now I’m in a situation where I’ve got a paper which is too methodological for an applied stats journal (and far too methodological for an atmospheric science journal) and not a novel enough methodology for a journal as theoretical as BA. If BA don’t think it’s novel enough and it’s not the first time this data’s been published we’ll struggle to get it into something like &lt;a href=&#34;http://www.rss.org.uk/site/cms/contentviewarticle.asp?article=876&#34;&gt;JRSS B&lt;/a&gt; (IF: 3.645), &lt;a href=&#34;http://rsif.royalsocietypublishing.org/&#34;&gt;JRS: Interface&lt;/a&gt; (IF: 4.402) or &lt;a href=&#34;http://www.plosone.org&#34;&gt;PLoS One&lt;/a&gt; (IF: 4.092). Our options, then, seem to be trying an Elsevier journal like &lt;a href=&#34;http://www.journals.elsevier.com/environmental-modelling-and-software/&#34;&gt;EMS&lt;/a&gt; (IF: 3.114) or &lt;a href=&#34;http://www.journals.elsevier.com/computational-statistics-and-data-analysis/&#34;&gt;CSDA&lt;/a&gt; (IF: 1.028), which I’m not keen to do, somewhere like &lt;a href=&#34;http://www.jstatsoft.org&#34;&gt;JSS&lt;/a&gt; (IF: 2.647) or a more applied journal like &lt;a href=&#34;http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1467-9876&#34;&gt;JRSS C&lt;/a&gt; (IF: 0.828, which is quite low). I’d like to put this in a statistics journal because I want to have a career as a statistician rather than just someone who can only work in aerosols. That’s one of the reasons I’m not keen to publish this somewhere like Atmospheric Environment (where both my article and Bjarke’s, the bases of this work, were published). I’m really kicking myself now for not submitting an abstract for this as a contributed talk at ISBA. I would’ve got a BA article out of it. In happier news, I gave a presentation with two other PhD students to BRAG this morning where we talked about INLA. It went well and I think we’ve convinced a few of the others that INLA is pretty cool and worth using. Edit: and I’m getting a lot of mileage out of the Finnish/finish pun.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Resubmission of Finnish paper</title>
      <link>/./2012/08/08/resubmission-of-finnish-paper/</link>
      <pubDate>Wed, 08 Aug 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/08/08/resubmission-of-finnish-paper/</guid>
      <description>&lt;p&gt;I’ve spent a lot of my time since Healthy Buildings finished revising the semi-parametric forecasting paper. We had submitted to Annals of Applied Statistics but it was rejected. We got some very useful comments back from the reviewer, though, and I think it’s a much stronger paper now. The reviewer encouraged us to rewrite the paper with a focus on the methodology rather than the application and submit it to a more theoretical journal. I have just submitted it to &lt;a href=&#34;http://ba.stat.cmu.edu/&#34;&gt;Bayesian Analysis&lt;/a&gt; (IF: 1.650), the official journal of &lt;a href=&#34;http://bayesian.org&#34;&gt;ISBA&lt;/a&gt;, and uploaded the preprint to arXiv (where it will replace the &lt;a href=&#34;http://arxiv.org/abs/1207.0558&#34;&gt;current version&lt;/a&gt; in the next 24 hours). As an Open Access journal with a well-written LaTeX document class, Bayesian Analysis is a journal I can get behind. Some very good papers have appeared there and as Bayesian statistics continues to grow as a field (and ISBA as a society) I think we can expect to see BA really take off as a journal. So much of modern statistics is algorithms rather than proofs and making these available to people, particularly people who aren’t academics, with freely available, peer reviewed papers will help improve the statistical capabilities of modern science.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Changes in the way my group does work</title>
      <link>/./2012/07/19/changes-in-the-way-my-group-does-work/</link>
      <pubDate>Thu, 19 Jul 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/07/19/changes-in-the-way-my-group-does-work/</guid>
      <description>&lt;p&gt;Yesterday I had a 45 minute chat with two of the PhD students in my group about statistics. I had made some comments at one of their talks at Healthy Buildings about the summary statistics they were using for a ratio of indoor and outdoor particle concentrations and they wanted to catch up to discuss that and my first paper on using GAMs for temporal trends and meteorology. I think I’ve managed to convince both of them that using R is a good idea and that using spline models will help look at non-linear effects. They’re going to read some of the references that I listed. We also had a chat about how ANOVA’s a good start for data analysis and I suggested that they read Gelman’s paper (recently referred to in my &lt;a href=&#34;http://samclifford.info/2012/07/12/plenary-speech/&#34; title=&#34;Plenary speech&#34;&gt;plenary speech&lt;/a&gt;). I think the &lt;a href=&#34;http://cran.r-project.org/web/packages/openair/index.html&#34;&gt;openair&lt;/a&gt; package is something they might look at given one of the other students in their room uses it. Openair uses mgcv to do some of its non-parametric estimation. I’m also starting a paper with one of our postdocs looking at personal exposure to ultrafine particles. We’ve got some very interesting data from quite a new instrument and the way the experiment was designed we are going to be able to answer some very interesting questions. The colleague who will probably be the primary author has to move back overseas for at least a few months so we’ll be collaborating from across an ocean or two. Our meetings recently have involved having an instance of R open to look at some data and a LaTeX document for writing down what we decide. Our postdoc asked me to email them the file but instead we sat down and set up a GitHub account and I showed that it wasn’t particularly hard to operate git in terms of basic functionality. They’ve used MATLAB before and have been getting used to using R, so this shouldn’t be too hard for them. So it’s nice to see a few little changes here and there not just in terms of how we approach a problem but with the tools we use to make our jobs easier and to improve the quality of our output.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Starting more papers</title>
      <link>/./2012/07/17/starting-more-papers/</link>
      <pubDate>Tue, 17 Jul 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/07/17/starting-more-papers/</guid>
      <description>&lt;p&gt;One of the benefits of being a statistically curious researcher is that you get to read about all sorts of cool stuff. The UPTECH project is generating a huge amount of time series data, some of which have change points, non-linear behaviour, trends, and all sorts of other quirks. I’ve spent most of my time learning about the use of splines but over the last year have been exposed to Gaussian processes (and I guess I would say splines are a special case) and Gaussian Markov Random Fields. I’ve been having the occasional chat with the other researchers about how to analyse the time series data they’re working with and have stumbled across some really neat methods. Apart from the work I’ve been doing on spline models with my Finnish collaborators, interesting ideas for analysing time series data include Treed Linear Models, Treed Gaussian Processes [1,2] and Dirichlet Process Mixtures of GLMs [3]. The tree nature of the first two models I mentioned is apparent in its partitioning of the covariate space into regions in which the behaviour is locally linear. Change points are placed where the behaviour changes and each partition has its own linear mean and its own variance estimate. This is a fairly simple model to fit but it’s a bit limited by its only using linear functions. The treed GP relaxes this and spends its time fitting a more GP within each partition, with the focus on the covariance relationship. The third, DP mixtures of GLMs gives much smoother estimates of the mean and credible interval and has some really nice properties courtesy of the DP (which looks to be superior to tree based clustering). I find the tree structure of these models quite interesting and the treed linear model appears to be, conceptually, a mix of a multiple changepoint model and a piecewise linear regression spline with wombling knots. I’m not 100% sure how to apply these but an initial chat makes me think they will be very applicable and I’m looking forward to some exploratory data analysis. [1] [Gramacy, R. B. (2007). tgp: An r package for bayesian nonstationary, semiparametric nonlinear regression and design by treed gaussian process models. Journal of Statistical Software 19(9), 1-46.](&lt;a href=&#34;http://www.jstatsoft.org/v19/i09/&#34; class=&#34;uri&#34;&gt;http://www.jstatsoft.org/v19/i09/&lt;/a&gt;) [2] [Gramacy, R. B. and H. K. H. Lee (2008). Bayesian treed gaussian process models with an application to computer modeling. Journal of the American Statistical Association 103(483), 1119-1130.](&lt;a href=&#34;http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000689&#34; class=&#34;uri&#34;&gt;http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000689&lt;/a&gt;) (&lt;a href=&#34;http://arxiv.org/abs/0710.4536&#34;&gt;arXiv preprint&lt;/a&gt;) [3] [Hannah, L. A., D. M. Blei, and W. B. Powell (2011). Dirichlet process mixtures of generalized linear models. Journal of Machine Learning Research 12, 1923-1953.](&lt;a href=&#34;http://www.cs.princeton.edu/~blei/papers/HannahBleiPowell2011.pdf&#34; class=&#34;uri&#34;&gt;http://www.cs.princeton.edu/~blei/papers/HannahBleiPowell2011.pdf&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My spoon is too big</title>
      <link>/./2012/07/07/my-spoon-is-too-big/</link>
      <pubDate>Sat, 07 Jul 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/07/07/my-spoon-is-too-big/</guid>
      <description>&lt;p&gt;Rejected. I had submitted the Finnish paper to an applied statistics journal and received, within a working day or two, a response from the reviewer. They said the paper doesn’t focus as much on the application and is in fact more methodological. They go on to make a few suggestions as to how we could improve our method (mainly the forecasting and posterior summary stuff) and that we should submit it to a methods journal (I’m thinking Bayesian Analysis). Having not studied much statistics in undergrad and learning Bayesian statistics to any degree after starting my PhD, I have felt like the work I’ve been doing was just applying methods that others had developed and that I wasn’t doing much statistics research. My first paper was more or less just that, fitting a GAM to some air quality data. It’s a nice paper, I’m proud of it, and it was a very valuable piece of work in terms of me understanding GAMs and semi-parametric regression; it took a lot of work to get there. At the same time, it felt a bit like I was using an R package to do some magic. So while the Finnish paper has been rejected by a journal, I am buoyed by the reviewer’s comments about it being a well written paper that outlines a nice method with some solid statistics behind it. We have some changes to make (and I agree with the comments they make about our posterior summaries) but the thought of publishing a methods paper is very exciting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>First arXived paper</title>
      <link>/./2012/06/19/first-arxived-paper/</link>
      <pubDate>Tue, 19 Jun 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/06/19/first-arxived-paper/</guid>
      <description>&lt;p&gt;Given that I’ve had to submit a manuscript for ISBA 2012 I figured I should put it on arXiv just in case anything happens. It’d also be good to point people to it at the conference to get a better idea from the poster that I’m presenting. I’ve put a link to it on my &lt;a href=&#34;http://samclifford.info/publications/&#34; title=&#34;Publications&#34;&gt;publications&lt;/a&gt; page, but the direct link is &lt;a href=&#34;http://arxiv.org/abs/1206.3833&#34;&gt;here&lt;/a&gt; to save you a click. I found the arXiv submission process very easy to use and am very impressed with its LaTeX processing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Just a few quick thoughts</title>
      <link>/./2012/06/19/just-a-few-quick-thoughts/</link>
      <pubDate>Tue, 19 Jun 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/06/19/just-a-few-quick-thoughts/</guid>
      <description>&lt;p&gt;I’m setting up a laptop to take to ISBA with me as I have lots of thesis work to do. I must say, I’m really impressed with GitHub for Windows in regards to how simple it is to set up. It’s a matter of installing the program itself, then entering your github details. Cloning your GitHub repositories to your local machine is as simple as pressing a button. I haven’t had to faff about with ssh, pageant, etc. Now I just have to finish setting up remote INLA (which will require faffing about with ssh), installing LaTeX and figuring out if I can use X forwarding without X-Win. I also have to finish my ISBA poster and organise for it to be printed. Then there’s the two talks I am giving at Healthy Buildings 2012 which need writing and the Student Program work. I leave for Japan on Sunday. I should probably look at train travel from Osaka to Kyoto, find my travel money card, passport, etc. I uploaded a paper to arXiv yesterday. I’ll post about it here when it appears.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LaTeX and git</title>
      <link>/./2012/06/13/latex-and-git/</link>
      <pubDate>Wed, 13 Jun 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/06/13/latex-and-git/</guid>
      <description>&lt;p&gt;At the request of ihrhove I’ve decided to talk a little bit about using git and LaTeX together. I currently have two private git repositories; one for the Finnish paper and the other for all of my thesis work. I’ve talked previously about the Finnish paper so I’ll give a brief overview of how I use it with my thesis but you’ll need to keep in mind that I don’t have it shared with anyone because my supervisors don’t use git and nor do they edit the documents I work on directly (two print out draft papers and write on them, the third (who has used CVS/SVN in the past) uses Foxit to annotate PDFs directly and send them back to me. To start (and possibly end, if you’re easily convinced) with, LaTeX is just code. So to me there’s no reason why you can’t use any service you’d normally use for code for LaTeX. Everything that is directly being used in a paper comes under my version control with git. Each paper in my thesis repository has its own folder. Within that folder there is a LaTeX subfolder, where I keep everything needed for the writing of the paper, and an R or MATLAB folder depending on what program I’m using to do the modelling (and all the code goes into the repository). Within the LaTeX folder I have a whole bunch of .tex files and a folder where I store the images to be included in the paper. One of my favourite commands in LaTeX is . Every section in a paper has its own LaTeX source file. I find that this helps me navigate my work when I’m writing, especially when making corrections. Each file gets worked on separately and I save frequently. If I’m finished dealing with a section or I’m heading off for a break I will save everything and commit the current changes with a note about which section I’ve been focussing on. I picked this based writing up in my Honours degree when I got sick of having screen after screen of text. If I want to omit a section in a draft I can just comment out the line. Reorganising sections and maybe even subsections, becomes an issue of swapping two or three lines of LaTeX rather than copying and pasting giant blocks of text. I’m a sucker for vector graphics so I will use PDF graphs and pdflatex wherever I can. Occasionally I succumb to using PGF/TikZ for a while but usually have to generate so many different styles of plots that I don’t bother. So anyway, PDF graphics. These are really quite small and can be stored in git no trouble at all. I know git’s more or less useless for version control and revision of binary files (but PDF and EPS files are quite different) but I find it useful to be able to overwrite my graphs and still have the older versions available through reverting to a previous commit rather than making endless folders called “oldgraphics”. The root of my thesis repository has a folder called “Bibliography” which is where a monolithic bibtex file called “allpapers.bib” is stored. Because I will cite the same references across multiple papers I find the idea of having separate bibliography databases a bit silly. I use JabRef to edit this, by the way. All my \bibliography commands point to ../../Bibliography/allpapers.bib. I’ve even got a template for papers with that line in it so that I don’t even have to think about how I do my referencing. With regards to the Finnish paper, this compartmentalisation reduces, even further, the risk of conflicts. Committing changes to one section at a time means the commit messages are often quite descriptive without having to be quite long. The mixture of a few lines of changes and a brief summary means it’s easy to see what’s happened in the changelog. I also use git to keep track of side projects that have popped up during my thesis. Coworkers will often come to me with a question about some data analysis or if I can write a script to make a certain repetitive task as automatic as possible. Each coworker gets a subfolder within a /Side Projects/ folder and within those there are folders for each little project. If I worked in a group where use of git was widespread I would consider making a separate project for each person and inviting them as a collaborator. I kind of wish that QUT had a git server (the school of IT had a subversion server but I really dislike SVN after discovering git) and that scientists were encouraged to use R/MATLAB/SAS for their statistics and modelling instead of Excel. I think it’d a great way to foster collaboration and have people be able to work on a project and make changes, share their code with their coworkers, etc. without sending code and draft papers around via email. Actually a private git server without the account level limitations that github imposes would be an invaluable tool, especially if you could just open up your repositories to the QUT community to show what you’re doing and provide colleagues with usable code for statistical analysis, image manipulation tools, etc. And if someone within the university came across your work and liked it, you would potentially have another paper to work on within the uni.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ad hoc collaboration</title>
      <link>/./2012/06/05/ad-hoc-collaboration/</link>
      <pubDate>Tue, 05 Jun 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/06/05/ad-hoc-collaboration/</guid>
      <description>&lt;p&gt;Rbloggers have &lt;a href=&#34;http://www.r-bloggers.com/announcing-rpubs-a-new-web-publishing-service-for-r/&#34;&gt;announced the launch&lt;/a&gt; of &lt;a href=&#34;http://www.rpubs.com/&#34;&gt;RPubs&lt;/a&gt;, a free service which makes it easy to publish code and analysis on the web. It’s based on RStudio and the markdown package and looks like a great way for people to show analysis to co-workers who might not have R on their computers when you don’t feel like writing a report. I really like this idea and might end up using it in my office to show what we can do with statistics. Another thing I’ve been thinking about is the potential to use an Apple TV and its &lt;a href=&#34;http://www.apple.com/au/appletv/airplay/&#34;&gt;screen sharing&lt;/a&gt; capabilities to do presentation work from iPhones, iPads and Mac computers. A lot of people in my office have iPhones, so an Apple TV hooked up to a HDMI screen (surely universities just leave these lying around) might be a good way to get a group of people to take some notes or share prepared slides with a small room of people. For example, if people had a PDF version of slides on their iPhones they could take control of the Apple TV and use their iPhone to flick through the slides, allowing everyone to stay in their seats and control the slideshow from their own device. I was excited by Google Wave when it first launched, as it combined a lot of what I liked about Gmail, Google Docs and Google Chat with an extension system, making it an incredibly powerful and flexible platform for collaborative work. Unfortunately it was released prematurely and died off after a flurry of uptake. Google Plus doesn’t really make up for it, either. I really liked the idea of collaboratively writing a document and being able to add in a voting gadget to resolve whether a section should be included. I used it socially to determine the dates of picnics with friends, which was probably where most of my use was directed. Probably the best example of how useful I found it was in writing a manual of procedures with about ten other volunteers who would ask questions. As we answered the questions, we folded the answers into the main part of the document. This was much more useful than writing a static document and then having a separate email list for discussion, or using track changes in Microsoft Word to email around a huge document that kept on growing. I have high hopes for the internet in terms of &lt;em&gt;ad hoc&lt;/em&gt; collaboration, particularly academic collaboration. I find &lt;a href=&#34;http://github.com/&#34;&gt;GitHub&lt;/a&gt; really exciting because it allows me to work on a private project and then add a collaborator when they come on board. Once a project is finished and the paper published, that private repository can be made public and anyone can fork it and do with it what they will. If they’re intrigued by what’s been done, they could contact me and discuss what they’ve done and we can build a new project based on their fork of my work. With so much of my work being based on R or MATLAB and written up in LaTeX, I find this potential way of working quite sensible. Add in the fact that GitHub gives you a wiki system for each project and you’ve got a great tool at your disposal. A somewhat &lt;em&gt;ad hoc&lt;/em&gt; collaborative tool that I organised is the &lt;a href=&#34;https://wiki.qut.edu.au/display/npbayes/Home&#34;&gt;wiki for QUT’s Bayesian Non-parametrics reading group&lt;/a&gt;. This is a repository for the collective work of the group, including Q&amp;amp;A on the papers we’re reading, notes from the meetings, a list of papers read, code chunks, links to videos explaining what we’re working on, etc. It’s been a really useful tool and I’d hope that others interested in the same work could use it as a resource for their own learning. There’s a lot of really cool stuff out there. It’s a matter of finding useful tools that don’t have particularly high barriers to entry and allow non-experts to view expertly produced material (like on RPubs). The longer it’s been since one was a student, the less likely one seems to be to adopt new workflow practices. I’ve suggested git to my supervisors as a good way for our groups to work but I have a feeling that none of them are interested enough in distributed version control to put the effort in to learning how to use them. So for now it’s annotated PDFs or printed pages with scribbles on them rather than making the edits to a LaTeX source file and committing and pushing their changes to a shared repository.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Working on this Finnish paper</title>
      <link>/./2012/06/05/working-on-this-finnish-paper/</link>
      <pubDate>Tue, 05 Jun 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/06/05/working-on-this-finnish-paper/</guid>
      <description>&lt;p&gt;I figured I might as well describe how git made it possible to write the code and paper for the work I’ve been doing with Bjarke, Tareq, Kaarle and Jukka. Without git, we’d probably have been emailing code back and forth to each other or using something like Dropbox which would freak out over all the little changes we make, making it impossible to both be working on the same file at once. &lt;a href=&#34;http://git-scm.com/&#34;&gt;Git&lt;/a&gt; is a distributed version control system that allows you to track revisions to your code and invite multiple collaborators to the project. I’ve talked about it &lt;a href=&#34;http://samclifford.info/tag/git/&#34;&gt;previously&lt;/a&gt; but basically it’s this great system where you can work on a project with multiple people, making your changes, committing them on your local machine to save them. Once you are happy with the changes you’ve made and they don’t break anything, you can push the changes to the shared repository where all the other members of the project have access to them. If there’s a conflict, git lets you know and you can fix it up then re-commit and push. There are tools for reverting changes, making new branches, merging branches, etc. June 13 2011. It’s still three weeks before I’m due to arrive in Finland. I upload the code from the book chapter on Bayesian Splines that I’ve been writing for BRAG. Bjarke and I spend a bit of time emailing back and forth about how splines work, as he hasn’t used them in a regression framework before. Bjarke has sent me a copy of the draft of his paper on a GLM with autoregressive residuals. I’ve still got the 8BNP workshop to attend before arriving in Finland. July 5 2011. I arrive in Finland and meet Tareq and Bjarke for a meeting. We take a copious amount of notes during a long discussion where we set out what we want to achieve long term and what we want to have finished by the time I leave. The aim is to at least have some working code that combines my splines with Bjarke’s code that does autoregressive residuals. July 6 2011. Bjarke’s code is added to the git repository and we get to work understanding what the other person has written. We’re both still getting to grips with how git works and end up accidentally making new branches. I spend most of my time annotating code so that I know where to look when things inevitably go wrong. Time is spent ensuring we have ways of visualising our results so we know if things are going totally wrong. July 7-8 2011. We spend the next few days attempting to stitch the code together. Bjarke doesn’t use Google Chat or Facebook so there’s a little email correspondence at this time but it’s mostly office conversations. July 9-10 2011. No work happens here as Bjarke and I are holidaying with his in-laws for the weekend at a summer cottage near Lappeenranta (near the Russian border). July 11-16 2011. This is the most creative and chaotic period of working on the paper. Notes are made on A4 paper, transcribed as notes in a text file on git when they are worth following up and abandoned when they don’t lead anywhere. We start really getting to grips with multivariate splines, Metropolis-within-Gibbs, testing out new ideas, making new branches, merging them when they work, deleting them when they don’t, scribbling maths out on pieces of paper and running up and down the corridors whenever there’s a breakthrough. July 19-31 2011. I return to Australia and we spend some time writing about what we managed to get done while I was overseas. We’re back to one branch and are largely discussing the methodology and making sure plotting works. August, September 2011. I continue making changes to the way autoregressive residuals are handled, Bjarke codes up some diagnostics and begins examining a wide range of model specifications for the air quality data we’re working with in order to come up with a way of illustrating how what we’ve done is so cool. October, November 2011. Some changes are made to the way the penalties are handled, the code becomes more functional and most of the focus is on plotting, diagnostics and model choice. Plots are saved as PDF files using &lt;a href=&#34;http://www.mathworks.com/matlabcentral/fileexchange/23629&#34;&gt;export_fig.m&lt;/a&gt; within our script and are brought under the control of git so that we can replace one set of results with another in a single commit. December 2011. Some radical changes are made to the way the autoregressive error structure is passed to the model, making it more flexible. These changes are contained in a separate branch so that Bjarke can continue working on his model comparison knowing that his code will continue to run. He checks it out and offers feedback. January 2012. A lot of work is done on making sure the paper explains what’s going on. A few more features are introduced and the code is commented heavily. February-April 2012. Bjarke spends a lot of time making sure the scripts to call the model fitting, forecasting and diagnostics work properly. May 2012. A draft paper is sent around for feedback, some changes to the description of the method are recommended, as are a few different model specifications. Development on the code itself has stopped but the diagnostics, plotting and inference continues. Much of the work is now happening on QUT’s supercomputer as competing models are tested. Writing about the autoregressive errors is filled out a bit to ensure that the forecasting is highlighted. June 2012. The paper is almost finished. We’re waiting on feedback from a co-author who has been quite sick. There have been some large rewrites based on Kerrie’s feedback, mostly to change the order so that it’s a punchier article which highlights the novelty of the method rather than me just talking about how cool splines are. Support is being canvassed among the authors for uploading the draft to arXiv and releasing the code once the paper is published. And that’s where we stand at the moment. Hopefully I can make the git repository public and you can have a look at what’s happened and where we’ve come from with this. It might need a bit of pruning first to make sure that no data that shouldn’t be publicly available isn’t accidentally made public. There’s a minimal working example in the code where we simulate some data, so hopefully that’s enough to demonstrate what we’ve done. There are some really neat ways of visualising the work done on GitHub, including a network diagram of the committed changes and branches, contributions of each person over time, when commits occur most frequently, what (programming) languages the project uses and how frequent additions and deletions occur (and therefore the growth rate of the project). I hope this sheds some light on the process that’s been used. GitHub was basically a way for the QUT and Helsinki groups to collaborate, with Bjarke and I acting as the conduits for reviews and comments. Git allowed us to write a whole bunch of code together, following up all sorts of crazy ideas without getting in each others’ way. The paper was written as we went and is subject to the same version control (after all, LaTeX is code too). I have found it a really great way of working. I’d like to see how it goes with a few more people programming and whether I can work with a few other people to try to make the changes to the paper directly via git rather me making the changes based on notes scribbled on a printed copy. P.S. Wow, I can’t believe it’s been nearly a year since we started working on this. Well, I can, as we had a few delays where it turned out we needed to rewrite large chunks of code and the paper. P.P.S. I just managed to merge the development branch with the modified way of dealing with the residuals back into the master branch without there being any conflicts. I didn’t expect conflicts but it’s nice to know that everything’s back in the master branch. Below is an image of the commit history. It doesn’t show the number of changes in each commit, but given that commits occur when an idea has been tested or a section written, it’s a good indication of a parcel of working being done. &lt;a href=&#34;commits.png&#34;&gt;&lt;img src=&#34;http://samcliffordinfo.files.wordpress.com/2012/06/commits.png&#34; title=&#34;commits&#34; alt=&#34;Committed changes for the Finnish paper&#34; /&gt;&lt;/a&gt; For interest’s sake &lt;a href=&#34;https://maps.google.com/maps/ms?msid=205578669134200940456.0004a7be06572cca98eec&amp;amp;msa=0&#34;&gt;here’s a map of my time in Finland&lt;/a&gt;. I haven’t got the exact location of the summer cottage but it’s near Taipalsaari. Here’s &lt;a href=&#34;https://plus.google.com/photos/106543548808921798128/albums/5637271904465537361?authkey=CIOgldL52du6vAE&#34;&gt;my collection of photos from my time in Finland&lt;/a&gt;. I had originally uploaded them to Facebook and given detailed captions but the move to Google Plus ended up removing the captions. Leave a comment on them asking a question if you want to know more.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gelman and Robert</title>
      <link>/./2012/05/25/gelman-and-robert/</link>
      <pubDate>Fri, 25 May 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/05/25/gelman-and-robert/</guid>
      <description>&lt;p&gt;Kerrie Mengersen, one of my supervisors, is visiting some colleagues in France at the moment. It appears that one of the outputs of this visit is a discussion paper, “In praise of the referee”, written with Nicolas Chopin, Andrew Gelman and Christian Robert (&lt;a href=&#34;http://arxiv.org/abs/1205.4304&#34;&gt;arXiv&lt;/a&gt;). There’s been a lot of discussion recently about the role of journals, publishers and reviewers in academic publishing ranging from defending the &lt;em&gt;status quo&lt;/em&gt; to totally overhauling the system by shedding paper journals and moving everything online to a distributed network of institutional ePrints repositories. The paper by Chopin et al. makes two recommendations after a lengthy discussion about where we find ourselves&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Post-publication peer review with comment threads on arXiv and/or a filtering service where instead of acting as reviewers, editorial boards pick out worthwhile new research as a list of recommended reading.&lt;/li&gt;
&lt;li&gt;A reviewer commons where academics are taught how to review and the reviewer reports (and their names) are published alongside the article.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The article is worth a read. I figured I’d also share an old Gelman blog post that he’s recently linked to, “&lt;a href=&#34;http://andrewgelman.com/2009/07/advice_on_writi/&#34;&gt;Advice on writing research articles&lt;/a&gt;”. The seven pieces of general advice are well worth remembering. It’s basically “Start with your conclusions and work backwards towards your methods”. Christian Robert has &lt;a href=&#34;http://xianblog.wordpress.com/2012/05/25/xian-australian-tour-2012/&#34;&gt;published the dates for his Australian tour&lt;/a&gt;. I’m not particularly interested in ABC but the talk on Rao-Blackwellisation looks interesting. Anyone who’s interested in learning a bit about what Bayesian simulation is, without necessarily having a statistics background, would do well to attend the public talk entitled “Simulation as a universal tool for statistics”, which you could probably consider as a popular science talk.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bioaerosols</title>
      <link>/./2012/05/17/bioaerosols/</link>
      <pubDate>Thu, 17 May 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/05/17/bioaerosols/</guid>
      <description>&lt;p&gt;One of our researchers asked me to come along to a meeting next week to discuss my potential involvement on a paper as “the statistician”. It’s part of the UPTECH project but it’s not the spatial distribution of outdoor aerosols, which is what I’ve been working on for my PhD. &lt;a href=&#34;http://www.uku.fi/vaitokset/2009/ISBN978.951.802.908.6salonen.htm&#34;&gt;Heidi Salonen&lt;/a&gt;, a visiting researcher from Finland, is working on UPTECH looking at the concentration of microbiological agents in indoor air. Luckily I don’t have to be &lt;a href=&#34;http://en.wikipedia.org/wiki/Egon_Spengler&#34;&gt;an expert on spores, moulds and fungus&lt;/a&gt; but it’ll be interesting to pick up a bit more knowledge from the quite broad field of aerosols. I’ve been asked a few times to be involved with some ILAQH projects but at the moment it’s been limited to helping write the statistical methodology for grant applications or discussing an approach for data from another part of the UPTECH project. As far as I recall, this is the first time that someone from ILAQH has come and said “Hey, I want you to be a co-author on this paper”. This sort of thing is what I’ve been looking forward to as I finish off my PhD, the chance to get out of my specific topic and start looking at other peoples’ work. I’ll probably try to recommend a non-parametric regression technique but I have a feeling it’ll be a classically designed experiment and that all we’ll need is some t-tests.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A mostly Fin(n)ish[ed] paper</title>
      <link>/./2012/05/09/a-mostly-finnished-paper/</link>
      <pubDate>Wed, 09 May 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/05/09/a-mostly-finnished-paper/</guid>
      <description>&lt;p&gt;The paper I started with some collaborators in Finland (&lt;a href=&#34;https://tuhat.halvi.helsinki.fi/portal/en/persons/bjarke-moelgaard(74a8081d-a379-4e4b-9aff-e9b85bc98c78)/publications.html&#34;&gt;Bjarke Mølgaard&lt;/a&gt;, &lt;a href=&#34;http://www.rni.helsinki.fi/~jic/&#34;&gt;Jukka Corander&lt;/a&gt;, &lt;a href=&#34;http://www.atm.helsinki.fi/~khameri/&#34;&gt;Kaarle Hämeri&lt;/a&gt;, &lt;a href=&#34;https://tuhat.halvi.helsinki.fi/portal/en/persons/tareq-hussein(3aacf8e7-f431-41b0-9303-1abaf1897bf7)/publications.html?page=5&amp;amp;rendering=vancouver&#34;&gt;Tareq Hussein&lt;/a&gt;) almost a year ago is nearly done. It’s been nearly done a few times, but now all that remains is to do a little bit of model choice regarding the separability of the effects of meteorology on ultrafine particle number concentration. We’ve been using git to send the paper and code back and forth (well, Bjarke and I have) and I’ve found that to be a really simple way of collaboratively writing code and a paper. To see the changes made, one need only look at the commit details. Much nicer than using tracked changes in Word and emailing a bunch of versions of the same file back and forth and trying to do complicated merges of changes. I am really looking forward to submitting this paper, as it’s probably the most methodological work I’ll get out of my PhD (the other papers are largely applications of some novel techniques to the UPTECH project’s data). It’s quite a nice blending of the &lt;a href=&#34;https://tuhat.halvi.helsinki.fi/portal/en/publications/forecasting-sizefra(9c6ae07d-af34-4909-a4f9-d758bc7a4795).html&#34;&gt;work done by the Finnish authors previously&lt;/a&gt; [1] as part of Bjarke’s PhD and some of the ideas in my first paper [2]. While I don’t know that it will totally revolutionise atmospheric modelling (in the way that I’m sure we all hope it will), it’s quite a nice technique that increases the flexibility of the Generalised Additive Model and hopefully encourage anyone interested in doing Bayesian modelling with the GAM to stop using &lt;a href=&#34;http://www.uow.edu.au/~mwand/&#34;&gt;Matt Wand&lt;/a&gt;’s WinBUGS approach [3, 4]. To be clear, I find GAMs in WinBUGS particularly cumbersome to code given that WinBUGS doesn’t deal with matrix operations very well and the use of P-splines requires a lot of matrix operations. Having said that, though, Wand’s code is a nice intro to Bayesian splines where you don’t have to write your own MCMC sampler. I just think it has some limitations that are not easily overcome. I’d like to present this to a statistics conference but it wasn’t anywhere near ready enough to demonstrate at ISBA 2012 when I was submitting an abstract. [1] B. Mølgaard, T. Hussein, J. Corander, K. Hämeri, Forecasting size-fractionated particle number concentrations in the urban atmosphere, Atmospheric Environment, Volume 46, January 2012, Pages 155-163, ISSN 1352-2310, 10.1016/j.atmosenv.2011.10.004. &lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S1352231011010491&#34;&gt;ScienceDirect&lt;/a&gt; [2] S. Clifford, S. Low Choy, T. Hussein, K. Mengersen, L. Morawska, Using the Generalised Additive Model to model the particle number count of ultrafine particles, Atmospheric Environment, Volume 45, Issue 32, October 2011, Pages 5934-5945, ISSN 1352-2310, 10.1016/j.atmosenv.2011.05.004. &lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S1352231011004766&#34;&gt;ScienceDirect&lt;/a&gt; [3] C. M. Crainiceanu, D. Ruppert. M. P. Wand, Bayesian Analysis for Penalized Spline Regression Using WinBUGS, &lt;a href=&#34;http://www.jstatsoft.org/v14/i14/&#34;&gt;Journal of Statistical Software, Volume 14, Issue 14, September 2005&lt;/a&gt;. [4] J. K. Marley, M. P. Wand, Non-Standard Semiparametric Regression via BRugs, &lt;a href=&#34;http://www.jstatsoft.org/v37/i05&#34;&gt;Journal of Statistical Software, Volume 37, Issue 5, November 2010&lt;/a&gt;. P.S. I apologise for the awful pun, but &lt;a href=&#34;http://en.wikipedia.org/wiki/The_Micallef_P(r)ogram(me)&#34;&gt;Shaun Micallef&lt;/a&gt; has been on my mind recently.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On to the next paper</title>
      <link>/./2012/04/11/on-to-the-next-paper/</link>
      <pubDate>Wed, 11 Apr 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/04/11/on-to-the-next-paper/</guid>
      <description>&lt;p&gt;I’ve just submitted the second paper of my PhD to my supervisors for what I hope to be a final round of feedback before we submit it. It comes in at just under twenty pages, which is what Annals of Applied Statistics sets as their usual maximum page limit. If there’s more to add I can probably turn some of the more theoretical stuff into supplementary material. Once we submit I’ll put the paper up on arXiv and link to it here so you can all have a look at how I think you should do hierarchical modelling in INLA for panel design data. Now that this paper’s more or less out of the way it’s time to go back to working on the paper that I’ve been working on with the Finnish team. It’s been a huge coding project but now the code is at a stage where, more or less, Bjarke and I are happy with it and it’s time to finalise the model we present as an example and write the analysis (the theory, discussion, etc. are mostly done). We still have to finalise where we’re going to send it but my hope is that we don’t have to pare away all the theory and end up sending it to an atmospheric science journal as an applied paper, because there has been an awful lot of work go into the modelling approach. I’m really looking forward to this one being done because it’s been going for about nine to ten months and it’s been on the backburner while I focussed on the INLA papers. It’s a very neat method and I hope the community agrees. I’ll also arXiv it and hope to release the code once the paper’s published.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper 2</title>
      <link>/./2012/04/02/paper-2/</link>
      <pubDate>Mon, 02 Apr 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/04/02/paper-2/</guid>
      <description>&lt;p&gt;The second paper of my PhD is really taking shape. It’s on spatial and temporal modelling of panel design data from the UPTECH project and has been a lot of fun to work on. I said it should be ready by the end of the week for the final comments by my supervisors and it looks like it’ll be on time, too. Once I’ve got that feedback it’s time to submit it and start work on paper 3 (which follows from paper 2). I’ll be so glad when this is over and I can look forward to finishing.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
