<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Clustering on Sam Clifford </title>
    <link>/./tags/clustering/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2014-02-13 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2014/02/13/posterior-samples/</link>
      <pubDate>Thu, 13 Feb 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/02/13/posterior-samples/</guid>
      <description>&lt;p&gt;It’s still ARC writing time so that’s been taking up quite a bit of time recently. A coworker from Maths mentioned she’d started using &lt;a href=&#34;http://literatureandlatte.com/scrivener.php&#34;&gt;Scrivener&lt;/a&gt; to storyboard her papers. Apparently &lt;a href=&#34;http://mosx.tumblr.com/post/51299936501/our-collaborative-scrivener-workflows&#34;&gt;it can be used with tools like git&lt;/a&gt;, &lt;a href=&#34;http://creativityhacker.ca/2013/04/23/scrivener-and-the-cloud-best-practices-2013/&#34;&gt;Google Drive and Dropbox&lt;/a&gt; to work collaboratively but you’ve got to be careful of conflicted copies that won’t be discovered. Another coworker from Maths isn’t so impressed by Scrivener, noting that in terms of using it with version control software you’re better off using LaTeX anyway. At US$40 a license I’m a bit reluctant to try to make Scrivener part of my workflow, and there’s no way I can ask collaborators to fork out that kind of money. I keep being impressed by Matt Wand, Jan Luts and Tamara Broderick’s work on &lt;a href=&#34;http://realtime-semiparametric-regression.net/&#34;&gt;realtime semiparametric regression&lt;/a&gt;. I saw Wand present this work at Bayes on the Beach just over a year ago. I can’t, for the life of me, wrap my head around mean-field variational Bayes, though. Perhaps it’s that I’ve never had to deal with Calculus of Variations and got into inference beyond linear models through MCMC in WinBUGS rather than through machine learning. I’ve got a few more books on my desk about statistical theory now, including Congdon’s “Bayesian Statistical Modelling”, Koller and Friedman’s “Probabilistic Graphical Models”, Casella and Berger’s “Statistical Inference”.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Starting more papers</title>
      <link>/./2012/07/17/starting-more-papers/</link>
      <pubDate>Tue, 17 Jul 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/07/17/starting-more-papers/</guid>
      <description>&lt;p&gt;One of the benefits of being a statistically curious researcher is that you get to read about all sorts of cool stuff. The UPTECH project is generating a huge amount of time series data, some of which have change points, non-linear behaviour, trends, and all sorts of other quirks. I’ve spent most of my time learning about the use of splines but over the last year have been exposed to Gaussian processes (and I guess I would say splines are a special case) and Gaussian Markov Random Fields. I’ve been having the occasional chat with the other researchers about how to analyse the time series data they’re working with and have stumbled across some really neat methods. Apart from the work I’ve been doing on spline models with my Finnish collaborators, interesting ideas for analysing time series data include Treed Linear Models, Treed Gaussian Processes [1,2] and Dirichlet Process Mixtures of GLMs [3]. The tree nature of the first two models I mentioned is apparent in its partitioning of the covariate space into regions in which the behaviour is locally linear. Change points are placed where the behaviour changes and each partition has its own linear mean and its own variance estimate. This is a fairly simple model to fit but it’s a bit limited by its only using linear functions. The treed GP relaxes this and spends its time fitting a more GP within each partition, with the focus on the covariance relationship. The third, DP mixtures of GLMs gives much smoother estimates of the mean and credible interval and has some really nice properties courtesy of the DP (which looks to be superior to tree based clustering). I find the tree structure of these models quite interesting and the treed linear model appears to be, conceptually, a mix of a multiple changepoint model and a piecewise linear regression spline with wombling knots. I’m not 100% sure how to apply these but an initial chat makes me think they will be very applicable and I’m looking forward to some exploratory data analysis. [1] [Gramacy, R. B. (2007). tgp: An r package for bayesian nonstationary, semiparametric nonlinear regression and design by treed gaussian process models. Journal of Statistical Software 19(9), 1-46.](&lt;a href=&#34;http://www.jstatsoft.org/v19/i09/&#34; class=&#34;uri&#34;&gt;http://www.jstatsoft.org/v19/i09/&lt;/a&gt;) [2] [Gramacy, R. B. and H. K. H. Lee (2008). Bayesian treed gaussian process models with an application to computer modeling. Journal of the American Statistical Association 103(483), 1119-1130.](&lt;a href=&#34;http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000689&#34; class=&#34;uri&#34;&gt;http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000689&lt;/a&gt;) (&lt;a href=&#34;http://arxiv.org/abs/0710.4536&#34;&gt;arXiv preprint&lt;/a&gt;) [3] [Hannah, L. A., D. M. Blei, and W. B. Powell (2011). Dirichlet process mixtures of generalized linear models. Journal of Machine Learning Research 12, 1923-1953.](&lt;a href=&#34;http://www.cs.princeton.edu/~blei/papers/HannahBleiPowell2011.pdf&#34; class=&#34;uri&#34;&gt;http://www.cs.princeton.edu/~blei/papers/HannahBleiPowell2011.pdf&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two papers submitted</title>
      <link>/./2012/07/06/two-papers-submitted/</link>
      <pubDate>Fri, 06 Jul 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/07/06/two-papers-submitted/</guid>
      <description>&lt;p&gt;In the last two weeks I’ve sent two papers to Annals of Applied Statistics and uploaded them to the arXiv. This fulfills the minimum publication requirements for a PhD by publication at QUT. So it’s a milestone on the way to completion. Now I just have to sort out a fourth paper as backup, give two presentations at next week’s conference, organise a final seminar session, compile all my work into a thesis and get it to my supervisors. On top of my immediate thesis work on spatio-temporal modelling, I’ve got a paper for Healthy Buildings on methods for analysing the UPTECH survey data that’ll get published, a paper on which I’m coauthor (particle losses in a thermodenuder system) that’s almost finished and the microbiology paper I wrote about previously. Busy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A few things that warrant an update</title>
      <link>/./2012/07/04/a-few-things-that-warrant-an-update/</link>
      <pubDate>Wed, 04 Jul 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/07/04/a-few-things-that-warrant-an-update/</guid>
      <description>&lt;p&gt;The Finnish paper is pretty much ready for submission to Annals of Applied Stats. I’ve updated my publications page to include a link to the preprint on the &lt;a href=&#34;http://arxiv.org/abs/1207.0558&#34;&gt;arXiv&lt;/a&gt;. I will update my CV soon and I’ll add my posters and slides to the publications page. The Higgs boson - what’s the deal with that? &lt;a href=&#34;http://www.northcountrypublicradio.org/news/npr/156221787/cern-discovers-a-new-particle-likely-the-higgs-boson&#34;&gt;NPR has a good article on it&lt;/a&gt;. I spent the evening watching the Higgs announcements at uni and even though I thought the CERN slides were pretty cluttered and didn’t like the layout, nothing had prepared me for the ATLAS slides. Bad colour schemes (if you can call them that) and Comic Sans MS? Yuk. You don’t make science accessible by making people think “Hey. That looks like &lt;em&gt;I&lt;/em&gt; could have designed that. Scientists aren’t so different to me after all!” Some clearly presented slides that weren’t stuffed full of text and images are, in my opinion, the key to a good scientific presentation. 1 slide per minute, no more than 5-6 lines with no more than 5-6 words per line. The slides should touch on the key points so people can, at a glance, get a good idea of what’s going on. The talk that accompanies the slides is what conveys the rest of the information in more natural language. There was a lot of great science presented tonight, but it wasn’t presented well. David Spiegelhalter &lt;a href=&#34;http://understandinguncertainty.org/higgs-it-one-sided-or-two-sided&#34;&gt;explains&lt;/a&gt; the five sigma significance of the ATLAS/CERN results. P-values and confidence intervals are two things where I think frequentist probability stops being conceptually simpler than Bayesian statistics and becomes about questions like “What is the probability of observing the data I have seen given that I have this model and these parameter estimates?” and “If I did an infinite number of trials how many times would I expect this interval for my sample mean to cover the true mean?” and “Something something agriculture”. &lt;a href=&#34;http://hb2012.org/&#34;&gt;Healthy Buildings&lt;/a&gt; is coming up next week. It’s all hands on deck at ILAQH while we put the finishing touches on the program and sort out the behind the scenes stuff. It’s going to be great. I’ll be giving two talks; the first is about how we can use the Dirichlet process for clustering in health survey data and the second is about the need for better statistics in science. ISBA 2012 was a heap of fun and there were lots of good talks. I find meeting other statisticians very inspiring. I will try to write a wrap-up when HB 2012 is over. For now, you can enjoy &lt;a href=&#34;http://xianblog.wordpress.com/2012/06/27/isba-2012-guest-post/&#34;&gt;my preliminary thoughts&lt;/a&gt; on Xian’s ’Og.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Indian Buffet process - I&#39;m full</title>
      <link>/./2012/04/06/indian-buffet-process---im-full/</link>
      <pubDate>Fri, 06 Apr 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/04/06/indian-buffet-process---im-full/</guid>
      <description>&lt;p&gt;My little NP Bayes reading group finally finished with the &lt;a href=&#34;http://samclifford.info/2012/03/28/indian-buffet-process/&#34;&gt;IBP&lt;/a&gt; paper &lt;a href=&#34;https://wiki.qut.edu.au/display/npbayes/2012/04/04/Week+12+Summary&#34;&gt;this week&lt;/a&gt;, having spent five weeks going over it. It’s quite a neat paper which introduces some very interesting approaches to latent feature modelling, such as generating binary matrices which are explicitly in a certain form and using combinatorics to figure out how many other matrices can be reordered into that form. Another neat mathematical trick is the use of the Sherman-Morrison formula to update the inverse of Z’Z (part of the posterior covariance). Each row of Z represents an observation’s latent feature presence and absence. The exchangeable IBP allows us to reorder the observations however we want, so we can sequentially sample rows in Z and then perform the rank one update, rather than block sampling (regenerating the entire Z matrix) and computing the inverse of quite a dense matrix. It turns out that inference for infinite binary matrices can be performed by considering the finite case and recognising that the infinitely many zero columns won’t contribute to the posterior. The example they give, image analysis, is quite effective in illustrating the use of the IBP. One of us suggested that we could spend the next few weeks coding up the Metropolis within Gibbs MCMC sampler and applying it to Fisher’s iris data. We’ve looked at the iris data with the DP for allocating flowers to clusters but perhaps some latent feature analysis will better unearth the underlying species.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Indian Buffet process</title>
      <link>/./2012/03/28/indian-buffet-process/</link>
      <pubDate>Wed, 28 Mar 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/03/28/indian-buffet-process/</guid>
      <description>&lt;p&gt;The NP Bayes reading group that I started is currently working through Griffiths and Ghahramani’s paper on the Indian Buffet process [1], an infinite latent feature model which is used in topic modelling, document clustering, pattern recognition, etc. It’s been pretty hard going because QUT doesn’t teach a lot of combinatorics and we’ve had some patchy attendance recently so we’ve had to go over things a few times (especially the Chinese Restaurant process, the left-ordering function). It’s been a very interesting paper and I am looking forward to the possibility of using it with some survey data. No doubt I’ll get a better understanding of it when I head to ISBA later in the year (&lt;a href=&#34;http://www2.e.u-tokyo.ac.jp/~isba2012/stsessions.html&#34;&gt;Nils Hjort has organised a session&lt;/a&gt; on Beta processes that will include talks from Yongdai Kim, Tamara Broderick and Sinead Williamson). If you’re interested in reading about it, we’ve been making notes on our Wiki. You can start from &lt;a href=&#34;https://wiki.qut.edu.au/display/npbayes/2012/03/&#34;&gt;here&lt;/a&gt; as it’s taken us all of this month so far. I’ve written some code snippets to do some of the things the paper mentions. [1] [Griffiths, T. L., and Ghahramani, Z. “Infinite Latent Feature Models and the Indian Buffet Process”, &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, 2005, &lt;strong&gt;18&lt;/strong&gt;, 475-82](&lt;a href=&#34;http://cocosci.berkeley.edu/tom/papers/ibptr.pdf&#34; class=&#34;uri&#34;&gt;http://cocosci.berkeley.edu/tom/papers/ibptr.pdf&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
