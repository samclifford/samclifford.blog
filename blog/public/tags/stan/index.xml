<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Stan on Sam Clifford </title>
    <link>/./tags/stan/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2014-08-05 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>Running Bayesian models</title>
      <link>/./2014/08/05/running-bayesian-models/</link>
      <pubDate>Tue, 05 Aug 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/08/05/running-bayesian-models/</guid>
      <description>&lt;p&gt;I came across a post via r/Bayes about different ways to run Bayesian hierarchical linear models in R, a topic I talked about recently at a two day workshop on using R for epidemiology. &lt;a href=&#34;http://www.sumsar.net/blog/2013/06/three-ways-to-run-bayesian-models-in-r/&#34;&gt;Rasmus Bååth&lt;/a&gt;’s post details the use of JAGS with rjags, STAN with rstan and LaplacesDemon. JAGS (well, rjags) has been the staple for most of my hierarchical linear modelling needs over the last few years. It runs within R easily, is written in C++ (so is relatively fast), spits out something that the coda package can work with quite easily, and, above all, makes it very easy to specify models and priors. Using JAGS means never having to derive a Gibbs sampler or write out a Metropolis-Hastings algorithm that requires to you to really think about jumping rules. It’s Bayesian statistics for those who don’t have the time/inclination to do it “properly”. It has a few drawbacks, though, such as not being able to specify improper priors (but this could be seen as a feature rather than a bug) with distributions like dflat() and defining a Conditional Autoregressive prior requires specifying it as a multivariate Gaussian. That said, it’s far quicker than using OpenBUGS and JAGS installs fine on any platform. After reading the post’s section on STAN I decided that it was time to give it another go. Downloading the latest version of R and Rtools would surely give me a better experience than last time where it wouldn’t even detect the compiler properly. Putting everything in DOS-friendly file structures with short names meant that everything went off without a hitch and I was able to get the toy example running. Andrew Gelman, one of the developers of STAN, has a &lt;a href=&#34;http://andrewgelman.com/2014/01/21/everything-need-know-bayesian-statistics-learned-eight-schools/&#34;&gt;post on his blog&lt;/a&gt; by Phillip Price about the eight schools example, a really introduction to hierarchical linear modelling and meta-analysis. STAN is a bit more forgiving than JAGS when it comes to priors; any stochastic node that isn’t given a prior is given a flat prior by default. Whether or not &lt;a href=&#34;http://arxiv.org/abs/1403.4630&#34;&gt;Thiago Martins and Dan Simpson&lt;/a&gt; would be happy with that remains to be seen. STAN looks very promising, and it’s been included in the third edition of &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/&#34;&gt;Gelman’s BDA book&lt;/a&gt; (which I still need to buy). The other strategy I tried recently was coding up a Metropolis-Hastings sampler using &lt;a href=&#34;http://theoreticalecology.wordpress.com/2010/09/17/metropolis-hastings-mcmc-in-r/&#34;&gt;the guide from Florian Hartig at Theoretical Ecology&lt;/a&gt;. Choosing a jumping rule was difficult, as I had different parameters to deal with and a single jumping rule wouldn’t do. I tried &lt;a href=&#34;http://projecteuclid.org/euclid.bj/1130077595&#34;&gt;adaptive MCMC&lt;/a&gt; and ended up going down the rabbit hole of log-precisions, block-updates and ended up with very poor mixing and convergence. Finding a decent jumping rule is probably what prevents me from going back to using adaptive MCMC as I did for &lt;a href=&#34;http://eprints.qut.edu.au/72987/&#34;&gt;a book chapter on Bayesian splines&lt;/a&gt;. I eventually settled on writing out a full Gibbs scheme and coding it up in MATLAB. This is very fast (MATLAB’s better at loops than R is) and gives me good convergence. I’m not a fan of MATLAB’s plotting, though, so may end up importing the results into R so I’ve got ggplot2 handy. Big thanks to Zoé van Havre for her help with the Gibbs scheme. I’ve got a PhD student who’s going to be dealing with Bayesian modelling. He’s picking up R quite quickly and is doing his best with Bayesian statistics. It’s all in WinBUGS at the moment, though, which is going to limit the amount of progress we can make. I’d love to be able to code up a bunch of JAGS models and let them run on the supercomputer once we get our great big data set ready for a well-planned set of analyses. I’ve got less time to do the modelling myself these days and find myself wishing I had a clone to do the work. I guess that’s part of the training aspect of PhD supervision, making sure your student can do the implementation when you describe a piece of analysis that you propose. It’s still difficult for me working in a science group as the only statistician, as most of my statistics discussions are people asking for my help rather than us collaborating as equals. I enjoy working with others on interesting modelling problems, and it’d be good to work with other statisticians. While I’m now in the Mathematical Sciences School I don’t think I’ve capitalised yet on the connections I’ve got there in terms of directing my own research down the statistics route. With the UPTECH analysis being the major focus of my research at the moment, it’s tricky to allocate brain space to what I want to be doing next.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2014/02/05/posterior-samples/</link>
      <pubDate>Wed, 05 Feb 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/02/05/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://andrewgelman.com/2014/01/21/everything-need-know-bayesian-statistics-learned-eight-schools/&#34;&gt;Everything I need to know about Bayesian Statistics, I learned in eight schools&lt;/a&gt;. At first I thought this meant not really understanding it until having worked in many places with different people but it’s actually a reference to a particular example of hierarchical modelling. &lt;a href=&#34;http://www.youtube.com/watch?v=qRSfxSRdL5Y&#34;&gt;Hadley Wickham talking about dplyr&lt;/a&gt;. Very fancy. &lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34;&gt;I’m finally attempting to install RStan&lt;/a&gt;. My computer is misbehaving. And it’s ARC Discovery Project writing time.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
