<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Anova on Sam Clifford </title>
    <link>/./tags/anova/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2013-08-23 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>Test statistics and regression parameters</title>
      <link>/./2013/08/23/test-statistics-and-regression-parameters/</link>
      <pubDate>Fri, 23 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/23/test-statistics-and-regression-parameters/</guid>
      <description>&lt;p&gt;This week in SEB113 we’ve started on regression with some simple linear models with one explanatory variable. As not everyone has a particularly strong statistics background (high school Maths B) there are definitely some challenges.&lt;/p&gt;
&lt;p&gt;The big one seems to be moving from the Normal distribution, which everyone seems to get, for estimating the confidence interval of the mean, towards the t distribution for calculating confidence intervals for regression parameters. Putting the t distribution in the context of estimating quality between batches of Guiness helps a little with the question “Where did this even come from?” but doesn’t address the mathematics of it. Plotting a few different t distributions with varying degrees of freedom helps make the point that the t approaches the Normal when the degrees of freedom goes to infinity but does nothing to explain what the degrees of freedom actually are.&lt;/p&gt;
&lt;p&gt;I’ve found that explaining the data as a resource for fitting the regression model can be handy. For a data set with &lt;em&gt;n&lt;/em&gt; points you have a maximum of &lt;em&gt;n&lt;/em&gt; degrees of freedom. Each time you add a parameter to your regression model you consume a degree of freedom because you’re imposing a constraint, such as “there is a straight line”. If we had one data point in our data set and wanted to know the mean of the data we would know it exactly, there would be no uncertainty left in our estimate (and therefore zero degrees of freedom). If we had two data points and wanted the mean there would be some amount of uncertainty left because there’s now some variation in our data (we would have one degree of freedom left). If we had two points and wanted a line of best fit we would be back to zero degrees of freedom because we have completely characterised the trend in the data set by joining the two points.&lt;/p&gt;
&lt;p&gt;If we fit a regression with a total of &lt;em&gt;k&lt;/em&gt; parameters on the right hand side of &lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt; = &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;0&lt;/sub&gt; + &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt; &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;1_i_&lt;/sub&gt; + &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt; &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;2_i_&lt;/sub&gt; + … + &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;k-1&lt;/sub&gt; &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;-1_i _&lt;/sub&gt; + ϵ&lt;sub&gt;i&lt;/sub&gt; (a mean and some effects of explanatory variables) we would have &lt;em&gt;n&lt;/em&gt;-&lt;em&gt;k&lt;/em&gt; degrees of freedom. As you have fewer degrees of freedom left your t distribution ends up with more mass further from the mean. This means that you’re more uncertain about the value of the parameter because you’re using your data to estimate other parameters.&lt;/p&gt;
&lt;p&gt;Coming from a Bayesian perspective and looking at this sort of mathematics it’s easy to conclude that the model states that the parameters have a t distribution. This is, of course, completely incorrect because the parameters are fixed constants with unknown values. This idea totally throws me as I’ve been working with Bayesian analysis for the last few years (with the exception of perhaps one paper) and I’m used to thinking of all parameters as being random variables.&lt;/p&gt;
&lt;p&gt;This is related to the hypothesis testing and confidence interval issues that I have with the way first year statistics is taught. Confidence intervals, as I’ve mentioned previously, are counter-intuitive. They are the things that are random in our estimates of the true values of parameters. I like the approach that we’re taking where we look at whether the 95% confidence interval covers zero in order to make statements about whether or not the parameter plays a role in explaining the variation that we are modelling. I don’t like that we then calculate p values for testing the hypothesis that that parameter is equal to zero. These tests are statements about the probability of seeing the test statistic or more extreme given the model that we’re working with. It’s all backwards and leads inexperienced students to make statements such as “We accept the null hypothesis” and “The variable is statistically insignificant”, both of which are nails on a chalkboard to my ears.&lt;/p&gt;
&lt;p&gt;I know that we can’t teach statistics the way I would like to teach it, as these are science students who will be entering a field where ANOVA and t tests are still commonly used not as exploratory data analysis but as the basis for inference. I am very thankful for the fact that we are moving away from testing and towards modelling, and I’ve been trying to make the point in lectures that modelling allows us to do prediction, whereas testing allows us to only talk about what we’ve seen. If we can make sure the students can fit a model in R and use it to predict and/or make inferences I think we’ll have done our jobs because that is far more than I was able to do when I finished MAB101 ten years ago, when everything required we look through page after page of statistical table, hunting the right p value.&lt;/p&gt;
&lt;p&gt;Edit: 150 posts!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistics in Ecology</title>
      <link>/./2012/12/12/statistics-in-ecology/</link>
      <pubDate>Wed, 12 Dec 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/12/12/statistics-in-ecology/</guid>
      <description>&lt;p&gt;The author of this post argues that 1) ecology is bad at prediction and 2) ecology needs to get better at prediction. I’d expand this beyond ecology to air quality and most other sciences based on observations of environmental phenomenon. I’ve railed against ANOVA previously but haven’t spent as much time as the author delving into the philosophy of science. I like that they suggest GAMs. I don’t like the focus on only p values, R-squared and effect sizes but I do note that they’ve mentioned AIC, so perhaps this isn’t aimed at people getting PhDs in statistical modelling.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graphs again</title>
      <link>/./2012/11/19/graphs-again/</link>
      <pubDate>Mon, 19 Nov 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/11/19/graphs-again/</guid>
      <description>&lt;p&gt;Now that I’ve handed a draft of my final thesis paper to my supervisors/co-authors, I’ve got a little head space to work on another paper that’s been sitting on my to do list for a while. One of the challenges with this paper is coming up with a way to represent data relating to a total of over 100 students at 24 schools. Summarising at the school level ignores a lot of the within-school variation but attempting to use standard plotting approaches can lead to some very complex and visually busy graphs. Add to this that we can’t really use colour and it’s getting a bit tricky. I’ve had a few more looks at the &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/research/published/dodhia.pdf&#34;&gt;Gelman, Pasarica and Dodhia paper&lt;/a&gt; &lt;a href=&#34;http://samclifford.info/2012/10/30/turning-tables-into-graphs/&#34; title=&#34;Turning tables into graphs&#34;&gt;that I’ve previously talked about&lt;/a&gt;. While it doesn’t have an example of what I actually want to plot, it does make me think more about what kind of data I’ve got, whether they’re continuous, categorical or count, what their ranges are and what sort of variation occurs. With 24 schools it’s possible to do a 4 x 6 or 6 x 4 grid of sub-plots, and within those subplots we can generally get across what sort of variation there is at the school level. Not everyone likes such a layout, though, so I’ve been looking into grouped/stacked barplots, changing the ordering of the grouping (variable by school vs school by variable) and combining time series and barplots in the same graph (which is actually quite a good way of visualising the data we have, but can’t be done for 100 students). In the end, it’s going to come down to being creative enough to come up with a few alternatives and asking my co-authors which version they think sells the message best. I pretty much refuse to resort to pie graphs (because the scaling in area can be misleading) and feel really uncomfortable about using box-plots to summarise school-level variation. I have nothing against box plots, but for the size of the data we have at each school, summarising with a minimum, maximum and the 25th, 50th and 75th percentiles is going to be very difficult without shifting to a &lt;a href=&#34;http://samclifford.info/2012/10/25/anova-again/&#34; title=&#34;ANOVA again&#34;&gt;Bayesian ANOVA&lt;/a&gt;. Still, it’s a really interesting piece of science with some quite unique challenges in terms of the analysis and representation of that analysis (and the raw data). Edit: and it also makes me appreciate what designers (including my two housemates) have to go through, with people saying “No, I don’t like it” but not always having some constructive advice that’s actually possible to put into practice. Edit 2: &lt;a href=&#34;http://andrewgelman.com/2012/11/tradeoffs-in-information-graphics/&#34;&gt;Relevant new Gelman blog post&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ANOVA again</title>
      <link>/./2012/10/25/anova-again/</link>
      <pubDate>Thu, 25 Oct 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/10/25/anova-again/</guid>
      <description>&lt;p&gt;I’ve been out for an after work drink with some people from ILAQH. Rather than going to bed after getting home I decided that what I &lt;em&gt;urgently&lt;/em&gt; need to do is write about the exchangeability in this hierarchical prior for the microbiology paper I’m writing with Heidi Salonen. I got a bit carried away and have reformulated the model a bit to properly represent the hierarchy while still making the code as similar in its working across all branches. Now I’ve got a bit carried away and am looking at how best to visually represent an estimate of the difference between indoor and outdoor fungal counts. It’s nice that I can generate these estimates from MCMC output rather than having to do formal testing that assumes a particular distribution. I’m looking forward to presenting these plots and results to my co-authors because it really shows the power of Bayesian hierarchical modelling as a tool for examining random effects across groups where there is little data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ANOVA</title>
      <link>/./2012/10/19/anova/</link>
      <pubDate>Fri, 19 Oct 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/10/19/anova/</guid>
      <description>&lt;p&gt;ANOVA is one of those things that all the scientists in my group do when writing a paper where there’s more than one group, which is totally natural and a good first step for data analysis. Whether it’s looking at the mean concentration of some aerosol at the UPTECH schools, the level of diesel engine emissions by fuel type or some other experimental setup, ANOVA will typically make it into a paper (even if only as a &lt;em&gt;t&lt;/em&gt; test). ANOVA (or a &lt;em&gt;t&lt;/em&gt; test) may not always be an appropriate test to use, e.g. if the data is not normal, has a few large outliers or exhibits some sort of reliance on a covariate. In such cases it may be better to use a regression model with a non-Gaussian likelihood. This week I’ve spent a bit of time getting to grips with the Mann-Whitney U test as a way of testing medians, another summary which is used for aerosol concentrations. It’s not featured in Excel, so the person I was helping had to dust off their SPSS skills and we eventually made our way through and figured out how to run the test. But descriptive statistics of quantiles or measures of central tendency aren’t nearly as exciting as something I’ve come across, functional ANOVA. I met Cari Kaufman at ISBA earlier this year and we had a bit of a chat about spatio-temporal models of climate data with Gaussian processes and Gaussian Markov Random Fields. When I got back to Brisbane I decided to have a look at what she’s written and whether I should think about applying to work with her at Berkeley. Kaufman has &lt;a href=&#34;http://ba.stat.cmu.edu/journal/2010/vol05/issue01/kaufman.pdf&#34;&gt;a 2010 paper&lt;/a&gt; [1], which appears to have its genesis &lt;a href=&#34;http://andrewgelman.com/2007/10/anova/&#34;&gt;at least as far back as 2007&lt;/a&gt;, where functional ANOVA is discussed as a way of testing whether some observed effect (which may be a nonlinear function) is the same across groups. The examples given include temperature records in Canada and spatio-temporal modelling of regional climate in the UK. I would like to go over the functional ANOVA paper with the QUT NP Bayes reading group, as it’s a very interesting use of the Gaussian process prior. I’d also like to use it in my own work, as the question “Is the daily trend the same at each school?” is of interest to me. [1] Cari G. Kaufman and Stephan R. Sain, “Bayesian Functional ANOVA Modeling Using Gaussian Process Prior Distributions”, &lt;em&gt;Bayesian Analysis&lt;/em&gt; 5, 2010, pages 123-150. [&lt;a href=&#34;http://ba.stat.cmu.edu/journal/2010/vol05/issue01/kaufman.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
