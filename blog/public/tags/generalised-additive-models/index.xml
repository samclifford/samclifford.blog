<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Generalised Additive Models on Sam Clifford </title>
    <link>/./tags/generalised-additive-models/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2013-08-29 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>More on regression</title>
      <link>/./2013/08/29/more-on-regression/</link>
      <pubDate>Thu, 29 Aug 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/08/29/more-on-regression/</guid>
      <description>&lt;p&gt;This week we moved on to regression with a categorical covariate, which we’ve used in the context of estimating the mean dissolved oxygen across three spatial locations or the mean grain size of different types of rock. I think one of the biggest stumbling blocks to our learning with R is the way that lm() and associated functions deal with factor terms. Personally, I am not a fan of the way it chooses the first alphabetic level as the baseline. I know you can relevel the factor levels to choose another one but I’d be much happier with a sum to zero constraint so that the effects are all departures from the mean. This is the way that R-INLA does it and, to me, it makes a lot more sense. Factor terms (categorical variables) are essentially a random effects mean, especially in a Bayesian setting where everything can be treated as a random effect. That lm() makes us choose a baseline and treats the other effects as differences to that baseline means we end up with a coefficients table which is more difficult to interpret. One option is to omit the intercept term from the model, with a function call like lm(data=my.data, y ∼ factor(x) - 1) but that still doesn’t give you a sense of the overall mean. In any case, the mixture of new regression techniques and hypothesis testing for whether or not some parameter is equal to zero is proving difficult. The difference between the t and standard Normal distributions seems to not be particularly well understood and while I’ve tried to make the link between a 95% confidence interval and hypothesis testing at a 5% level of significance quite explicit, the fact remains that these are both new concepts which are being taught by a relatively inexperienced lecturer to students whose mathematical literacy is generally not at the level of those I’ve tutored in subjects where Maths B was explicitly a pre-requisite rather than assumed knowledge. I managed to explain the heavy tails issue in class with a little bit of pantomime, showing how one might paint the tails of the t and Normal distributions and run out of paint at different values of t (or x) based on how much paint you had to use at values far away from the mean. I think about a quarter of the class was struggling with the diagram of overlapping triangles which was meant to be a “zoomed in” version of the point where the density functions of the Normal and t cross over as they get further away from the mean. The lectorials are recorded so I’ll be interested to see how it translates to a radio play setting. The computer labs are apparently quite dense at the moment, with a lot of fairly new ideas being reinforced in a 50 minute block. We’re quite fortunate to have all the labs before all the workshops this semester, so the lab is basically “here’s the code to do what was shown in the lectorials” and then the workshops are designed to implement the code for some problem and generate a bit of discussion. I think this week was probably one of the hardest, conceptually, because it brings together regression for categorical explanatory variables (a straight line is easier to understand than mutually exclusive sets of points), hypothesis testing, the t distribution and confidence intervals. I have uploaded some of last semester’s slides on the central limit theorem for those who may need them, but I think it’s more a familiarity and practice thing than the material being inherently inaccessible. Next week we’ll be moving on to different families for Generalised Linear Models and the use of the nls() function to fit non-linear models such as asymptotic, compartment and bi-exponential. I’m not such a fan of nls (or even nlme) but we can hardly teach them how to use something like WinBUGS to define their own custom mean functions (because if you struggle with the t distribution you’re going to have kittens trying to deal with using the Beta distribution as a conjugate prior for the Binomial model) or even throw them into using gam() from mgcv. I wouldn’t be averse to teaching Generalised Additive Models in an advanced follow-up unit for this subject. If we did that, we could remove the GLMs from SEB113 (which we’ve only introduced this semester) and spend some time on random effects models. I think such a subject would require a much stronger background in mathematics, so students may need to take MAB120/125 and MAB121/126 before attempting such a unit. Still, food for thought as QUT continues to develop the new Bachelor of Science course.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I seem to be taking on a lot of work right now</title>
      <link>/./2012/11/12/i-seem-to-be-taking-on-a-lot-of-work-right-now/</link>
      <pubDate>Mon, 12 Nov 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/11/12/i-seem-to-be-taking-on-a-lot-of-work-right-now/</guid>
      <description>&lt;p&gt;For someone who’s meant to be finishing his PhD I sure do have a lot of other peoples’ papers on my plate. Today and Friday I had a chat with another PhD student from the UPTECH project about looking for spatial variation within the UPTECH schools. We’ve got some divergent ideas about how to go about it but we sat down this afternoon and spent some time going over regression modelling versus exploratory/summary statistics and how we can move from using Spearman’s rank correlation coefficient to doing non-parametric function estimation. I wrote some code and commented it as we went, so it should be fairly straightforward to write the accompanying methodology subsection for the paper. I’m going to spend the week focussing on my final thesis paper (a spatio-temporal model for data from a split panel design). I got some comments back from my supervisors last week and there’s a lot to be done checking certain sources of variation. I spent a fair bit of time today looking over a former students’ papers, checking old email threads, discussing a few things with the co-authors of his papers who are still around and have even been tracking down which particular instruments were used to perform the measurements. It’ll be important to check that my spatial analysis of the within-school variation matches up with the other UPTECH student’s analysis. If we come to opposite conclusions then at least one of us is wrong. I’m now tossing up whether I should check my results from the supercomputer or wait until tomorrow.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
