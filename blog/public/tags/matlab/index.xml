<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Matlab on Sam Clifford </title>
    <link>/./tags/matlab/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2014-08-05 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>Running Bayesian models</title>
      <link>/./2014/08/05/running-bayesian-models/</link>
      <pubDate>Tue, 05 Aug 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/08/05/running-bayesian-models/</guid>
      <description>&lt;p&gt;I came across a post via r/Bayes about different ways to run Bayesian hierarchical linear models in R, a topic I talked about recently at a two day workshop on using R for epidemiology. &lt;a href=&#34;http://www.sumsar.net/blog/2013/06/three-ways-to-run-bayesian-models-in-r/&#34;&gt;Rasmus Bååth&lt;/a&gt;’s post details the use of JAGS with rjags, STAN with rstan and LaplacesDemon. JAGS (well, rjags) has been the staple for most of my hierarchical linear modelling needs over the last few years. It runs within R easily, is written in C++ (so is relatively fast), spits out something that the coda package can work with quite easily, and, above all, makes it very easy to specify models and priors. Using JAGS means never having to derive a Gibbs sampler or write out a Metropolis-Hastings algorithm that requires to you to really think about jumping rules. It’s Bayesian statistics for those who don’t have the time/inclination to do it “properly”. It has a few drawbacks, though, such as not being able to specify improper priors (but this could be seen as a feature rather than a bug) with distributions like dflat() and defining a Conditional Autoregressive prior requires specifying it as a multivariate Gaussian. That said, it’s far quicker than using OpenBUGS and JAGS installs fine on any platform. After reading the post’s section on STAN I decided that it was time to give it another go. Downloading the latest version of R and Rtools would surely give me a better experience than last time where it wouldn’t even detect the compiler properly. Putting everything in DOS-friendly file structures with short names meant that everything went off without a hitch and I was able to get the toy example running. Andrew Gelman, one of the developers of STAN, has a &lt;a href=&#34;http://andrewgelman.com/2014/01/21/everything-need-know-bayesian-statistics-learned-eight-schools/&#34;&gt;post on his blog&lt;/a&gt; by Phillip Price about the eight schools example, a really introduction to hierarchical linear modelling and meta-analysis. STAN is a bit more forgiving than JAGS when it comes to priors; any stochastic node that isn’t given a prior is given a flat prior by default. Whether or not &lt;a href=&#34;http://arxiv.org/abs/1403.4630&#34;&gt;Thiago Martins and Dan Simpson&lt;/a&gt; would be happy with that remains to be seen. STAN looks very promising, and it’s been included in the third edition of &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/&#34;&gt;Gelman’s BDA book&lt;/a&gt; (which I still need to buy). The other strategy I tried recently was coding up a Metropolis-Hastings sampler using &lt;a href=&#34;http://theoreticalecology.wordpress.com/2010/09/17/metropolis-hastings-mcmc-in-r/&#34;&gt;the guide from Florian Hartig at Theoretical Ecology&lt;/a&gt;. Choosing a jumping rule was difficult, as I had different parameters to deal with and a single jumping rule wouldn’t do. I tried &lt;a href=&#34;http://projecteuclid.org/euclid.bj/1130077595&#34;&gt;adaptive MCMC&lt;/a&gt; and ended up going down the rabbit hole of log-precisions, block-updates and ended up with very poor mixing and convergence. Finding a decent jumping rule is probably what prevents me from going back to using adaptive MCMC as I did for &lt;a href=&#34;http://eprints.qut.edu.au/72987/&#34;&gt;a book chapter on Bayesian splines&lt;/a&gt;. I eventually settled on writing out a full Gibbs scheme and coding it up in MATLAB. This is very fast (MATLAB’s better at loops than R is) and gives me good convergence. I’m not a fan of MATLAB’s plotting, though, so may end up importing the results into R so I’ve got ggplot2 handy. Big thanks to Zoé van Havre for her help with the Gibbs scheme. I’ve got a PhD student who’s going to be dealing with Bayesian modelling. He’s picking up R quite quickly and is doing his best with Bayesian statistics. It’s all in WinBUGS at the moment, though, which is going to limit the amount of progress we can make. I’d love to be able to code up a bunch of JAGS models and let them run on the supercomputer once we get our great big data set ready for a well-planned set of analyses. I’ve got less time to do the modelling myself these days and find myself wishing I had a clone to do the work. I guess that’s part of the training aspect of PhD supervision, making sure your student can do the implementation when you describe a piece of analysis that you propose. It’s still difficult for me working in a science group as the only statistician, as most of my statistics discussions are people asking for my help rather than us collaborating as equals. I enjoy working with others on interesting modelling problems, and it’d be good to work with other statisticians. While I’m now in the Mathematical Sciences School I don’t think I’ve capitalised yet on the connections I’ve got there in terms of directing my own research down the statistics route. With the UPTECH analysis being the major focus of my research at the moment, it’s tricky to allocate brain space to what I want to be doing next.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior Samples</title>
      <link>/./2014/07/30/posterior-samples/</link>
      <pubDate>Wed, 30 Jul 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/07/30/posterior-samples/</guid>
      <description>&lt;p&gt;Interested in collaborative use of R, MATLAB, etc. for analysis and visualisation within a webpage? &lt;a href=&#34;http://nbviewer.ipython.org/gist/msund/403910de45e282d658fa&#34;&gt;Combining plotly and iPython&lt;/a&gt; can help you with that. Cosmopolitan (yes, &lt;em&gt;that&lt;/em&gt; Cosmopolitan) has &lt;a href=&#34;http://www.cosmopolitan.com/career/news/a29534/get-that-life-emily-graslie-science/&#34;&gt;a great article&lt;/a&gt; interviewing Emily Graslie, Chief Curiosity Officer at the Field Museum in Chicago. She discusses being an artist and making the transition into science, science education and YouTube stardom. A few of the PhD students in my lab have asked if I could run an introduction to R session. I’d mentioned the CAR workshop that I’d be doing but the cost had put them off. Luckily, there are alternatives like &lt;a href=&#34;https://www.datacamp.com/courses/introduction-to-r&#34;&gt;Datacamp&lt;/a&gt;, &lt;a href=&#34;https://www.coursera.org/&#34;&gt;Coursera&lt;/a&gt; and &lt;a href=&#34;http://www.lynda.com/R-tutorials/R-Statistics-Essential-Training/142447-2.html&#34;&gt;Lynda&lt;/a&gt;. Coursera’s next round of “Data Science”, delivered by Johns Hopkins University, starts next Monday (Course 1 - &lt;a href=&#34;https://www.coursera.org/specialization/jhudatascience/1&#34;&gt;R Programming&lt;/a&gt;). So get in there and learn some R! I’m considering recommending some of these Coursera courses to my current SEB113 students who want to go a bit further with R, but the approach that they take in these online modules is quite different to what we do in SEB113 and I don’t want them to confuse themselves.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/10/04/posterior-samples/</link>
      <pubDate>Fri, 04 Oct 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/10/04/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://dynamicecology.wordpress.com/2013/09/26/wiwacs-vs-zombie-ideas/&#34;&gt;Zombie ideas in ecology&lt;/a&gt; make it difficult to publish results that go against them, but how prevalent are they? &lt;a href=&#34;http://mathesaurus.sourceforge.net/octave-r.html&#34;&gt;R for MATLAB users&lt;/a&gt;. This would have been handy in SEB113 if we were still using MATLAB. Perhaps we should give it as a general resource for stats classes in the mathematics program at QUT. The US government shutdown is having &lt;a href=&#34;http://edition.cnn.com/2013/10/03/opinion/urry-nasa-shutdown/index.html?hpt=hp_c2&#34;&gt;quite severe effects on science&lt;/a&gt;, such as threatening the launch of the next &lt;a href=&#34;http://edition.cnn.com/2013/10/03/us/shutdown-mars-mission/index.html&#34;&gt;Mission to Mars&lt;/a&gt;. This is pretty awful given it was &lt;a href=&#34;http://www.news.com.au/technology/sci-tech/happy-birthday-nasa-shut-it-down/story-fn5fsgyc-1226731242175&#34;&gt;NASA’s 50th birthday&lt;/a&gt; earlier this week.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Abnormal normals</title>
      <link>/./2013/09/05/abnormal-normals/</link>
      <pubDate>Thu, 05 Sep 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/09/05/abnormal-normals/</guid>
      <description>&lt;p&gt;I’m doing thesis revisions at the moment and one of the examiners has asked that I refit some of the models in one of the chapters. The chapter corresponds to a book chapter that I wrote with one of my supervisors where we looked at Bayesian spline regression. This was pretty early in my PhD and represented a stepping stone from the frequentist approach that Simon Wood uses in his mgcv pacakge (used for my first paper) and the Bayesian approach to Generalised Additive Models with autoregressive errors (based on Siddartha Chib’s 1993 paper). One thing that I’ve discovered that really bugs me in MATLAB is that normpdf (density function of a normal distribution) is specified in terms of a mean and standard deviation whereas mvnpdf (density function of a multivariate normal) is specified in terms of a mean vector and covariance matrix. The result is that normpdf(0,0,1.5) and mvnpdf(0,0,1.5) give different results! In my mind, the univariate normal is a special case of the multivariate normal and the usage should be consistent.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New things in Science and Engineering at QUT</title>
      <link>/./2013/02/18/new-things-in-science-and-engineering-at-qut/</link>
      <pubDate>Mon, 18 Feb 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/02/18/new-things-in-science-and-engineering-at-qut/</guid>
      <description>&lt;p&gt;Today was the first day of O week at QUT, a time when the relative calm of the summer break is disturbed by an influx of 17 year olds and university-run activities that always seem to generate a lot of noise. Is it possible to be a grumpy old man a week shy of 29? I received an email from my supervisor this morning asking if I could take over from one of the other PhD students in our group who had fallen ill last week and not recovered in time for a presentation this morning. The presentation, scheduled for 9am, was to be the first of the inaugural Nanotechnology and Molecular Science HDR (Higher Degree Research, i.e. Doctoral and Masters students) symposium. I’ve been moaning quietly, since starting my PhD in the School of Physical and Chemical Sciences, that the physics discipline had nothing like the School of Mathematics’ &lt;a href=&#34;http://samclifford.info/2012/09/07/qut-school-of-mathematics-postgrad-day-day-1/&#34; title=&#34;QUT School of Mathematics Postgrad Day - Day 1&#34;&gt;Postgrad Day&lt;/a&gt;. I really like Postgrad Day as it’s a good way to see what the other postgrad students are working on, what the research foci are within the school, and for students to improve their public speaking skills by delivering their research to a room of their peers and the other researchers in the school in an environment which is much more supportive than any conference is likely to be. The NMS HDR symposium brought together a number of students and staff from optics, aerosol science, nanomaterials, biotechnology, forensics and other fields within the discipline and allows them to see, perhaps for the first time, the research that others around them are doing. Even though my lab, ILAQH, is part of the Institute for Health and Biomedical Innovation, the distance between us and the remainder of IHBI is probably greater than just the physical distance between the two campuses. We do not seem to be particularly engaged with the culture of the remainder of IHBI and it’s very rare that our group will make the trek across to Kelvin Grove to see a presentation that is a short elevator ride away from the bulk of the IHBI membership. I have really only been to IHBI a few times. The two most recent appearances have been for the IHBI Olympics (a week of activities where research domains compete against each other in fun activities such as Iron Chef and photo scavenger hunt) in 2011 where I performed as part of the Health and Human Wellbeing domain’s talent quest entry, a four person improvisation troupe called “Ha ha… what?”, and to present the work that the PhD students of the UPTECH project had been working on (where we killed half an hour of time before the presentations by playing impro warm-up games). Continuing in this spirit of improvising in front of scientists, I spoke to the NMS HDR symposium at 45 minutes’ notice and in an eight minute talk managed to touch on the key points of the UPTECH project, explaining a small fraction of the science and discussing the richness of the dataset, the questions it will allow us to answer, and the diverse range of people we have involved in the project. I was told by one of the research staff in our group afterwards that it was refreshing to see a talk with no slides and that they were impressed at the quality of a talk that contained such a small amount of preparation and wondered whether I could give a presentation without speaking. Professor Dennis Arnold, the organiser of the symposium, is now based on the same floor as me; he is one of a handful of people on our floor who are not members of ILAQH. I asked him if he thought the day was a success and he was very positive. I sincerely hope that the NMS HDR symposium continues next year and well into the future, as a way to foster interest across the traditional divide of physics vs chemistry. I had to duck out of the symposium early to attend a meeting about one of the new units in the revamped Bachelor of Science degree. Dr Sama Low Choy, one of my supervisors, has asked me to run one of the collaborative workshops in the new &lt;a href=&#34;http://www.qut.edu.au/study/unit-search/unit?unitCode=SEB113&amp;amp;idunit=44499&#34;&gt;quantitative methods unit&lt;/a&gt; (she says it’s because of my impro skills). Today was one of the planning days where we got to grips with the structure of the unit, the way the workshops are to be run and how what we are doing is significantly different to anything we’ve done before. I’ll write more about it later, such as after my first tutorial, but it’s very exciting to see QUT break with tradition and make this unit happen. Through case studies with data sets relevant to their discipline, students will learn about quantitative methods in mathematics and statistics. We are ditching &lt;em&gt;t&lt;/em&gt; tests, removing the need for statistical tables, adding structure to the group work to ensure people don’t get to ride on the effort of others and teaching R and MATLAB in a first year unit that only supposes Maths B. I’m really excited that we’re teaching first year students how to use software that is free (well, at least R is) and far more powerful than Microsoft Excel. One of the problems with MAB101, the old unit, was that the computation was done in Minitab, a piece of software that I’ve never known any researcher to use. One of the workshop leaders said that they want to go back and do undergrad again knowing that this unit now exists; I don’t blame them. This will definitely be an exciting year for me, academically. A new course with new units, new facilities in the Science and Engineering Centre, new collaboration opportunities and the chance to pick somewhere new to move to at the end of the year.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LaTeX and git</title>
      <link>/./2012/06/13/latex-and-git/</link>
      <pubDate>Wed, 13 Jun 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/06/13/latex-and-git/</guid>
      <description>&lt;p&gt;At the request of ihrhove I’ve decided to talk a little bit about using git and LaTeX together. I currently have two private git repositories; one for the Finnish paper and the other for all of my thesis work. I’ve talked previously about the Finnish paper so I’ll give a brief overview of how I use it with my thesis but you’ll need to keep in mind that I don’t have it shared with anyone because my supervisors don’t use git and nor do they edit the documents I work on directly (two print out draft papers and write on them, the third (who has used CVS/SVN in the past) uses Foxit to annotate PDFs directly and send them back to me. To start (and possibly end, if you’re easily convinced) with, LaTeX is just code. So to me there’s no reason why you can’t use any service you’d normally use for code for LaTeX. Everything that is directly being used in a paper comes under my version control with git. Each paper in my thesis repository has its own folder. Within that folder there is a LaTeX subfolder, where I keep everything needed for the writing of the paper, and an R or MATLAB folder depending on what program I’m using to do the modelling (and all the code goes into the repository). Within the LaTeX folder I have a whole bunch of .tex files and a folder where I store the images to be included in the paper. One of my favourite commands in LaTeX is . Every section in a paper has its own LaTeX source file. I find that this helps me navigate my work when I’m writing, especially when making corrections. Each file gets worked on separately and I save frequently. If I’m finished dealing with a section or I’m heading off for a break I will save everything and commit the current changes with a note about which section I’ve been focussing on. I picked this based writing up in my Honours degree when I got sick of having screen after screen of text. If I want to omit a section in a draft I can just comment out the line. Reorganising sections and maybe even subsections, becomes an issue of swapping two or three lines of LaTeX rather than copying and pasting giant blocks of text. I’m a sucker for vector graphics so I will use PDF graphs and pdflatex wherever I can. Occasionally I succumb to using PGF/TikZ for a while but usually have to generate so many different styles of plots that I don’t bother. So anyway, PDF graphics. These are really quite small and can be stored in git no trouble at all. I know git’s more or less useless for version control and revision of binary files (but PDF and EPS files are quite different) but I find it useful to be able to overwrite my graphs and still have the older versions available through reverting to a previous commit rather than making endless folders called “oldgraphics”. The root of my thesis repository has a folder called “Bibliography” which is where a monolithic bibtex file called “allpapers.bib” is stored. Because I will cite the same references across multiple papers I find the idea of having separate bibliography databases a bit silly. I use JabRef to edit this, by the way. All my \bibliography commands point to ../../Bibliography/allpapers.bib. I’ve even got a template for papers with that line in it so that I don’t even have to think about how I do my referencing. With regards to the Finnish paper, this compartmentalisation reduces, even further, the risk of conflicts. Committing changes to one section at a time means the commit messages are often quite descriptive without having to be quite long. The mixture of a few lines of changes and a brief summary means it’s easy to see what’s happened in the changelog. I also use git to keep track of side projects that have popped up during my thesis. Coworkers will often come to me with a question about some data analysis or if I can write a script to make a certain repetitive task as automatic as possible. Each coworker gets a subfolder within a /Side Projects/ folder and within those there are folders for each little project. If I worked in a group where use of git was widespread I would consider making a separate project for each person and inviting them as a collaborator. I kind of wish that QUT had a git server (the school of IT had a subversion server but I really dislike SVN after discovering git) and that scientists were encouraged to use R/MATLAB/SAS for their statistics and modelling instead of Excel. I think it’d a great way to foster collaboration and have people be able to work on a project and make changes, share their code with their coworkers, etc. without sending code and draft papers around via email. Actually a private git server without the account level limitations that github imposes would be an invaluable tool, especially if you could just open up your repositories to the QUT community to show what you’re doing and provide colleagues with usable code for statistical analysis, image manipulation tools, etc. And if someone within the university came across your work and liked it, you would potentially have another paper to work on within the uni.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Working on this Finnish paper</title>
      <link>/./2012/06/05/working-on-this-finnish-paper/</link>
      <pubDate>Tue, 05 Jun 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/06/05/working-on-this-finnish-paper/</guid>
      <description>&lt;p&gt;I figured I might as well describe how git made it possible to write the code and paper for the work I’ve been doing with Bjarke, Tareq, Kaarle and Jukka. Without git, we’d probably have been emailing code back and forth to each other or using something like Dropbox which would freak out over all the little changes we make, making it impossible to both be working on the same file at once. &lt;a href=&#34;http://git-scm.com/&#34;&gt;Git&lt;/a&gt; is a distributed version control system that allows you to track revisions to your code and invite multiple collaborators to the project. I’ve talked about it &lt;a href=&#34;http://samclifford.info/tag/git/&#34;&gt;previously&lt;/a&gt; but basically it’s this great system where you can work on a project with multiple people, making your changes, committing them on your local machine to save them. Once you are happy with the changes you’ve made and they don’t break anything, you can push the changes to the shared repository where all the other members of the project have access to them. If there’s a conflict, git lets you know and you can fix it up then re-commit and push. There are tools for reverting changes, making new branches, merging branches, etc. June 13 2011. It’s still three weeks before I’m due to arrive in Finland. I upload the code from the book chapter on Bayesian Splines that I’ve been writing for BRAG. Bjarke and I spend a bit of time emailing back and forth about how splines work, as he hasn’t used them in a regression framework before. Bjarke has sent me a copy of the draft of his paper on a GLM with autoregressive residuals. I’ve still got the 8BNP workshop to attend before arriving in Finland. July 5 2011. I arrive in Finland and meet Tareq and Bjarke for a meeting. We take a copious amount of notes during a long discussion where we set out what we want to achieve long term and what we want to have finished by the time I leave. The aim is to at least have some working code that combines my splines with Bjarke’s code that does autoregressive residuals. July 6 2011. Bjarke’s code is added to the git repository and we get to work understanding what the other person has written. We’re both still getting to grips with how git works and end up accidentally making new branches. I spend most of my time annotating code so that I know where to look when things inevitably go wrong. Time is spent ensuring we have ways of visualising our results so we know if things are going totally wrong. July 7-8 2011. We spend the next few days attempting to stitch the code together. Bjarke doesn’t use Google Chat or Facebook so there’s a little email correspondence at this time but it’s mostly office conversations. July 9-10 2011. No work happens here as Bjarke and I are holidaying with his in-laws for the weekend at a summer cottage near Lappeenranta (near the Russian border). July 11-16 2011. This is the most creative and chaotic period of working on the paper. Notes are made on A4 paper, transcribed as notes in a text file on git when they are worth following up and abandoned when they don’t lead anywhere. We start really getting to grips with multivariate splines, Metropolis-within-Gibbs, testing out new ideas, making new branches, merging them when they work, deleting them when they don’t, scribbling maths out on pieces of paper and running up and down the corridors whenever there’s a breakthrough. July 19-31 2011. I return to Australia and we spend some time writing about what we managed to get done while I was overseas. We’re back to one branch and are largely discussing the methodology and making sure plotting works. August, September 2011. I continue making changes to the way autoregressive residuals are handled, Bjarke codes up some diagnostics and begins examining a wide range of model specifications for the air quality data we’re working with in order to come up with a way of illustrating how what we’ve done is so cool. October, November 2011. Some changes are made to the way the penalties are handled, the code becomes more functional and most of the focus is on plotting, diagnostics and model choice. Plots are saved as PDF files using &lt;a href=&#34;http://www.mathworks.com/matlabcentral/fileexchange/23629&#34;&gt;export_fig.m&lt;/a&gt; within our script and are brought under the control of git so that we can replace one set of results with another in a single commit. December 2011. Some radical changes are made to the way the autoregressive error structure is passed to the model, making it more flexible. These changes are contained in a separate branch so that Bjarke can continue working on his model comparison knowing that his code will continue to run. He checks it out and offers feedback. January 2012. A lot of work is done on making sure the paper explains what’s going on. A few more features are introduced and the code is commented heavily. February-April 2012. Bjarke spends a lot of time making sure the scripts to call the model fitting, forecasting and diagnostics work properly. May 2012. A draft paper is sent around for feedback, some changes to the description of the method are recommended, as are a few different model specifications. Development on the code itself has stopped but the diagnostics, plotting and inference continues. Much of the work is now happening on QUT’s supercomputer as competing models are tested. Writing about the autoregressive errors is filled out a bit to ensure that the forecasting is highlighted. June 2012. The paper is almost finished. We’re waiting on feedback from a co-author who has been quite sick. There have been some large rewrites based on Kerrie’s feedback, mostly to change the order so that it’s a punchier article which highlights the novelty of the method rather than me just talking about how cool splines are. Support is being canvassed among the authors for uploading the draft to arXiv and releasing the code once the paper is published. And that’s where we stand at the moment. Hopefully I can make the git repository public and you can have a look at what’s happened and where we’ve come from with this. It might need a bit of pruning first to make sure that no data that shouldn’t be publicly available isn’t accidentally made public. There’s a minimal working example in the code where we simulate some data, so hopefully that’s enough to demonstrate what we’ve done. There are some really neat ways of visualising the work done on GitHub, including a network diagram of the committed changes and branches, contributions of each person over time, when commits occur most frequently, what (programming) languages the project uses and how frequent additions and deletions occur (and therefore the growth rate of the project). I hope this sheds some light on the process that’s been used. GitHub was basically a way for the QUT and Helsinki groups to collaborate, with Bjarke and I acting as the conduits for reviews and comments. Git allowed us to write a whole bunch of code together, following up all sorts of crazy ideas without getting in each others’ way. The paper was written as we went and is subject to the same version control (after all, LaTeX is code too). I have found it a really great way of working. I’d like to see how it goes with a few more people programming and whether I can work with a few other people to try to make the changes to the paper directly via git rather me making the changes based on notes scribbled on a printed copy. P.S. Wow, I can’t believe it’s been nearly a year since we started working on this. Well, I can, as we had a few delays where it turned out we needed to rewrite large chunks of code and the paper. P.P.S. I just managed to merge the development branch with the modified way of dealing with the residuals back into the master branch without there being any conflicts. I didn’t expect conflicts but it’s nice to know that everything’s back in the master branch. Below is an image of the commit history. It doesn’t show the number of changes in each commit, but given that commits occur when an idea has been tested or a section written, it’s a good indication of a parcel of working being done. &lt;a href=&#34;commits.png&#34;&gt;&lt;img src=&#34;http://samcliffordinfo.files.wordpress.com/2012/06/commits.png&#34; title=&#34;commits&#34; alt=&#34;Committed changes for the Finnish paper&#34; /&gt;&lt;/a&gt; For interest’s sake &lt;a href=&#34;https://maps.google.com/maps/ms?msid=205578669134200940456.0004a7be06572cca98eec&amp;amp;msa=0&#34;&gt;here’s a map of my time in Finland&lt;/a&gt;. I haven’t got the exact location of the summer cottage but it’s near Taipalsaari. Here’s &lt;a href=&#34;https://plus.google.com/photos/106543548808921798128/albums/5637271904465537361?authkey=CIOgldL52du6vAE&#34;&gt;my collection of photos from my time in Finland&lt;/a&gt;. I had originally uploaded them to Facebook and given detailed captions but the move to Google Plus ended up removing the captions. Leave a comment on them asking a question if you want to know more.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A few thoughts on the various software packages for stats</title>
      <link>/./2012/05/29/a-few-thoughts-on-the-various-software-packages-for-stats/</link>
      <pubDate>Tue, 29 May 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/05/29/a-few-thoughts-on-the-various-software-packages-for-stats/</guid>
      <description>&lt;p&gt;I had a drink with a friend who works in health statistics. She uses SAS at work and asked me what kind of software I use to do my statistics. R and MATLAB, I responded. MATLAB because it’s fast and good and R because it’s free and has &lt;a href=&#34;http://cran.r-project.org/web/packages/available_packages_by_name.html&#34;&gt;heaps of additional packages&lt;/a&gt; to extend its use. A few days later she asked me if I’d seen &lt;a href=&#34;http://www.r-bloggers.com/will-2015-be-the-beginning-of-the-end-for-sas-and-spss/&#34;&gt;an article on R-bloggers&lt;/a&gt; predicting the end of the use of SAS in academic circles, with R overtaking SAS some time in 2015/16. Even discounting the R package system, the fact remains that R is far less cheaper than SAS or SPSS. As the GFC continues to bite hard and governments, universities and other large institutions look to shed unnecessary costs, perhaps R’s price ($0) will lead to its adoption. Institutional licenses for SAS and SPSS (and let’s throw MINITAB in there, as it’s also used) can’t be cheap and cutting out expensive software when a mature, free statistics environment (R+RStudio) is available would be a very simple way to reduce some ongoing costs. Support is available through companies like &lt;a href=&#34;http://www.revolutionanalytics.com&#34;&gt;Revolution Analytics&lt;/a&gt; if the argument is that SAS support the software they sell. Yes, I’m a bit of an R evangelist, particularly in my research group where people don’t use SAS, Stata, SPSS, MINITAB or MATLAB but instead use Microsoft Excel (one of the worst pieces of software for statistical analysis). I would love to see R displace SAS, SPSS and other proprietary software packages in the next few years, but there’s another parallel objective; the quest to stop scientists using Excel for data analysis and modelling. It’s slow and based on an accounting spreadsheet. If we can get people off Excel and on to R (rather than SAS or SPSS, which would be attractive choices because they’ve heard of them somewhere and because they cost a lot of money they must be good) then academia as a whole will benefit immensely.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux</title>
      <link>/./2012/05/16/linux/</link>
      <pubDate>Wed, 16 May 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/05/16/linux/</guid>
      <description>&lt;p&gt;I swear, when I finish this PhD and start my new position I’ll either be asking for a Mac or installing Linux on my current computer. A lot of my work at the moment revolves around remotely connecting to the university’s supercomputer and using git to do version control for documents stored locally. Both of these require decent ssh and X forwarding and I am sick to death of the way Windows XP deals with both of these .The faculty was going to upgrade us all to Windows 7 but then it got merged with another faculty and I guess it slipped down the list of priorities. I’ve used Mac OS X and/or Linux as my home operating system for quite a while now (must be at least ten years) and while I’m generally okay to use Windows and there are some nice implementations of the programs I need to use (LaTeX, MATLAB, RStudio), the fact is that these programs are available on other operating systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A mostly Fin(n)ish[ed] paper</title>
      <link>/./2012/05/09/a-mostly-finnished-paper/</link>
      <pubDate>Wed, 09 May 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/05/09/a-mostly-finnished-paper/</guid>
      <description>&lt;p&gt;The paper I started with some collaborators in Finland (&lt;a href=&#34;https://tuhat.halvi.helsinki.fi/portal/en/persons/bjarke-moelgaard(74a8081d-a379-4e4b-9aff-e9b85bc98c78)/publications.html&#34;&gt;Bjarke Mølgaard&lt;/a&gt;, &lt;a href=&#34;http://www.rni.helsinki.fi/~jic/&#34;&gt;Jukka Corander&lt;/a&gt;, &lt;a href=&#34;http://www.atm.helsinki.fi/~khameri/&#34;&gt;Kaarle Hämeri&lt;/a&gt;, &lt;a href=&#34;https://tuhat.halvi.helsinki.fi/portal/en/persons/tareq-hussein(3aacf8e7-f431-41b0-9303-1abaf1897bf7)/publications.html?page=5&amp;amp;rendering=vancouver&#34;&gt;Tareq Hussein&lt;/a&gt;) almost a year ago is nearly done. It’s been nearly done a few times, but now all that remains is to do a little bit of model choice regarding the separability of the effects of meteorology on ultrafine particle number concentration. We’ve been using git to send the paper and code back and forth (well, Bjarke and I have) and I’ve found that to be a really simple way of collaboratively writing code and a paper. To see the changes made, one need only look at the commit details. Much nicer than using tracked changes in Word and emailing a bunch of versions of the same file back and forth and trying to do complicated merges of changes. I am really looking forward to submitting this paper, as it’s probably the most methodological work I’ll get out of my PhD (the other papers are largely applications of some novel techniques to the UPTECH project’s data). It’s quite a nice blending of the &lt;a href=&#34;https://tuhat.halvi.helsinki.fi/portal/en/publications/forecasting-sizefra(9c6ae07d-af34-4909-a4f9-d758bc7a4795).html&#34;&gt;work done by the Finnish authors previously&lt;/a&gt; [1] as part of Bjarke’s PhD and some of the ideas in my first paper [2]. While I don’t know that it will totally revolutionise atmospheric modelling (in the way that I’m sure we all hope it will), it’s quite a nice technique that increases the flexibility of the Generalised Additive Model and hopefully encourage anyone interested in doing Bayesian modelling with the GAM to stop using &lt;a href=&#34;http://www.uow.edu.au/~mwand/&#34;&gt;Matt Wand&lt;/a&gt;’s WinBUGS approach [3, 4]. To be clear, I find GAMs in WinBUGS particularly cumbersome to code given that WinBUGS doesn’t deal with matrix operations very well and the use of P-splines requires a lot of matrix operations. Having said that, though, Wand’s code is a nice intro to Bayesian splines where you don’t have to write your own MCMC sampler. I just think it has some limitations that are not easily overcome. I’d like to present this to a statistics conference but it wasn’t anywhere near ready enough to demonstrate at ISBA 2012 when I was submitting an abstract. [1] B. Mølgaard, T. Hussein, J. Corander, K. Hämeri, Forecasting size-fractionated particle number concentrations in the urban atmosphere, Atmospheric Environment, Volume 46, January 2012, Pages 155-163, ISSN 1352-2310, 10.1016/j.atmosenv.2011.10.004. &lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S1352231011010491&#34;&gt;ScienceDirect&lt;/a&gt; [2] S. Clifford, S. Low Choy, T. Hussein, K. Mengersen, L. Morawska, Using the Generalised Additive Model to model the particle number count of ultrafine particles, Atmospheric Environment, Volume 45, Issue 32, October 2011, Pages 5934-5945, ISSN 1352-2310, 10.1016/j.atmosenv.2011.05.004. &lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S1352231011004766&#34;&gt;ScienceDirect&lt;/a&gt; [3] C. M. Crainiceanu, D. Ruppert. M. P. Wand, Bayesian Analysis for Penalized Spline Regression Using WinBUGS, &lt;a href=&#34;http://www.jstatsoft.org/v14/i14/&#34;&gt;Journal of Statistical Software, Volume 14, Issue 14, September 2005&lt;/a&gt;. [4] J. K. Marley, M. P. Wand, Non-Standard Semiparametric Regression via BRugs, &lt;a href=&#34;http://www.jstatsoft.org/v37/i05&#34;&gt;Journal of Statistical Software, Volume 37, Issue 5, November 2010&lt;/a&gt;. P.S. I apologise for the awful pun, but &lt;a href=&#34;http://en.wikipedia.org/wiki/The_Micallef_P(r)ogram(me)&#34;&gt;Shaun Micallef&lt;/a&gt; has been on my mind recently.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dear Mr Supercomputer</title>
      <link>/./2012/04/17/dear-mr-supercomputer/</link>
      <pubDate>Tue, 17 Apr 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/04/17/dear-mr-supercomputer/</guid>
      <description>&lt;p&gt;After ensuring that my colleagues were pushing their results to our git repository it’s now all steam ahead on this Finnish paper. I’ve got my MATLAB code running on the QUT supercomputer right now. It’s all very exciting watching it go and not run out of memory. Hopefully I can get these results sorted, do some model comparison, plotting and then send the document around for some final drafting within the next two or three weeks (the bulk of the theory section of the paper is finished). It would’ve been nice to be able to present this work at ISBA but I think I’d rather talk about the applied paper that discusses the spatio-temporal trends from the UPTECH project.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On to the next paper</title>
      <link>/./2012/04/11/on-to-the-next-paper/</link>
      <pubDate>Wed, 11 Apr 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/04/11/on-to-the-next-paper/</guid>
      <description>&lt;p&gt;I’ve just submitted the second paper of my PhD to my supervisors for what I hope to be a final round of feedback before we submit it. It comes in at just under twenty pages, which is what Annals of Applied Statistics sets as their usual maximum page limit. If there’s more to add I can probably turn some of the more theoretical stuff into supplementary material. Once we submit I’ll put the paper up on arXiv and link to it here so you can all have a look at how I think you should do hierarchical modelling in INLA for panel design data. Now that this paper’s more or less out of the way it’s time to go back to working on the paper that I’ve been working on with the Finnish team. It’s been a huge coding project but now the code is at a stage where, more or less, Bjarke and I are happy with it and it’s time to finalise the model we present as an example and write the analysis (the theory, discussion, etc. are mostly done). We still have to finalise where we’re going to send it but my hope is that we don’t have to pare away all the theory and end up sending it to an atmospheric science journal as an applied paper, because there has been an awful lot of work go into the modelling approach. I’m really looking forward to this one being done because it’s been going for about nine to ten months and it’s been on the backburner while I focussed on the INLA papers. It’s a very neat method and I hope the community agrees. I’ll also arXiv it and hope to release the code once the paper’s published.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
