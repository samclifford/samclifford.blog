<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Andrew Gelman on Sam Clifford </title>
    <link>/./tags/andrew-gelman/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2014-10-17 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>posterior samples</title>
      <link>/./2014/10/17/posterior-samples/</link>
      <pubDate>Fri, 17 Oct 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/10/17/posterior-samples/</guid>
      <description>&lt;p&gt;I probably should have put this post up earlier because it’s now a huge collection of stuff from the last month. Here we go! It appears that &lt;a href=&#34;http://hilaryparker.com/2012/08/16/the-setup-part-1/&#34;&gt;Hilary Parker&lt;/a&gt; and I have similar (but by no means identical) work setups for doing stats (or at least we did two years ago). It’s never too late to come up with a sensible way of organising your work and collection of references/downloaded papers. Applied statisticians should probably &lt;a href=&#34;http://simplystatistics.org/2014/09/15/applied-statisticians-people-want-to-learn-what-we-do-lets-teach-them/&#34;&gt;teach scientists what it is we do&lt;/a&gt;, rather than just the mathematics behind statistics. This is a difference I’ve noticed between SEB113 and more traditional statistics classes; we spend a lot less time discussion F distributions and a lot more time on model development and visualisation. Speaking of visualisation, here’s a really great article on visualisation and how we can use small multiples and colour, shape, etc. to highlight the interesting differences so that it’s very clear what our message is. Jeff Leek has compiled &lt;a href=&#34;http://simplystatistics.org/2014/09/09/a-non-comprehensive-list-of-awesome-female-data-people-on-twitter/&#34;&gt;a list of some of the most awesome data people&lt;/a&gt; on Twitter who happen to be female. In the ongoing crusade against abuse of p-values, &lt;a href=&#34;http://simplystatistics.org/2014/09/30/you-think-p-values-are-bad-i-say-show-me-the-data/&#34;&gt;we may want to instead focus on reproducibility&lt;/a&gt; to show that our results say what we say they do. Andrew Gelman and Eric Loken have &lt;a href=&#34;http://www.americanscientist.org/issues/feature/2014/6/the-statistical-crisis-in-science/99999&#34;&gt;an article in The American Statistician&lt;/a&gt; reminding us that p-values have a context and we need to be aware of issues like sample size, p-hacking, multiple comparisons, etc.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper helicopters</title>
      <link>/./2014/08/15/paper-helicopters/</link>
      <pubDate>Fri, 15 Aug 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/08/15/paper-helicopters/</guid>
      <description>&lt;p&gt;There is no textbook for SEB113 - Quantitative Methods in Science. It’s not that we haven’t bothered to prescribe one, it’s that no one seemed to be taking the same approach we decided upon two years ago when the planning for the unit started. There are books on statistics for chemistry, statistics for ecology, statistics for physics, statistics for mathematics, etc. but trying to find a general “statistics for science” book that focuses on modelling rather than testing has been difficult. That said, there are some amazing resources out there if you know where to look, not just for learning statistics but for teaching statistics. One of the most useful that we’ve come across is “&lt;a href=&#34;http://ukcatalogue.oup.com/product/9780198572244.do&#34;&gt;Teaching Statistics&lt;/a&gt;”, by Andrew Gelman and Deborah Nolan. The book itself is full of advice for things like groupwork, topic order, structure of learning activities, etc. but my favourite thing so far is the paper helicopter experiment. &lt;a href=&#34;http://www.tandfonline.com/doi/pdf/10.1080/08982119208918925&#34;&gt;George Box&lt;/a&gt; introduced engineering students to statistical design with the paper helicopter. The experiment itself is quite simple and motivates the idea of using statistics to optimise some design by varying the dimensions of the helicopter. As an activity, it’s a fun way to collect some data that can be used in analysis. By dividing the class up into groups and getting each group to do one or two different designs it’s possible to collect quite a large amount of data, with replication used to identify any group-level effects that may be explaining the variation within the data. There’s a great &lt;a href=&#34;http://www.tandfonline.com/doi/abs/10.1198/000313005X70777#.U-1UePmSx8E&#34;&gt;paper by David Annis&lt;/a&gt; which simplifies the experiment by only varying the length and width of the helicopter’s “blade”, and explains that fitting polynomials and their interactions may yield a regression surface which explains the variation but ignores the physics of the helicopter. In a class teaching statistics to scientists, any chance to tie the statistics to some scientific context must be leapt upon. One of the biggest challenges in teaching statistics is making it relevant to students so that it doesn’t come across as a dry technique for crunching numbers but as a way of probing deeper into a scientific question. I was discussing the experiment with a colleague yesterday who mentioned that when she was learning mathematics at university as part of her science degree it was at the hands of mathematicians who were teaching the unit as if it were for other mathematicians. It was only at the end of the course that a lecturer said “Oh, and by the way, these methods can be used to analyse scientific data”. This is the message that needs to come at the start of the class. Statistics (and more generally, mathematics) gives the scientist a set of tools to ask questions of their data. Being able to ask the right question is therefore very valuable. Ignoring the science means you’re throwing out all the hard work that went into the experimental design and data collection. This is one reason we focus so much on modelling instead of testing. Stopping your analysis at ANOVA doesn’t do justice to your data. Annis’ paper shows the derivation of a mathematical model of the motion of the helicopter from force balance equations, terminal velocities and rotational inertia. This mathematical model is then converted to a non-linear regression model. In SEB113 we cover non-linear regression after linear regression and then show where the regression models come from with a week of mathematical modelling. Even though students enrolled in the unit may not have Senior Maths B (assumed knowledge, rather than a formal pre-requisite) many enjoy peeking behind the curtain to see where the models come from. More than that, they are learning that application-specific non-linear models include what is known about the particular application. We explain first order compartment models (pharmacokinetics), asymptotic growth (ecology), the biexponential model (biology) and logistic growth (ecology) models and show the mathematical models that lead to their existence. We’ve also shown the Lotka-Volterra equations (ecology) in the past as an example of emulating a system, which students seem to enjoy (some are even comforted by the idea that there’s no exact solution). This year we’ll be adding the paper helicopter model to the mix, performing the experiment in week 5’s workshops and analysing the data in the Problem Solving Tasks. I’ll try to get some feedback on whether the students enjoy the experiment and can understand and complete the outcomes; I think it’s neat, but does it appeal to them? I really like that in the past we’ve had students who feel ownership of their data by collecting it in SEB114 - Experimental Science and analysing it in SEB113. SEB114 doesn’t run in second semester, though, so we have had to figure out ways of collecting data for analysis and I think the helicopter’s the best one we’ve come up with yet. We’ve modified the design found in Annis’ paper and I’ve used Adobe InDesign to come up with a printable A4 design where students don’t have to do any measuring, just cutting, folding and paper clipping. We have 12 designs available to us, which gives us a lot of flexibility when it comes to parallelising the experiment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Running Bayesian models</title>
      <link>/./2014/08/05/running-bayesian-models/</link>
      <pubDate>Tue, 05 Aug 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/08/05/running-bayesian-models/</guid>
      <description>&lt;p&gt;I came across a post via r/Bayes about different ways to run Bayesian hierarchical linear models in R, a topic I talked about recently at a two day workshop on using R for epidemiology. &lt;a href=&#34;http://www.sumsar.net/blog/2013/06/three-ways-to-run-bayesian-models-in-r/&#34;&gt;Rasmus Bååth&lt;/a&gt;’s post details the use of JAGS with rjags, STAN with rstan and LaplacesDemon. JAGS (well, rjags) has been the staple for most of my hierarchical linear modelling needs over the last few years. It runs within R easily, is written in C++ (so is relatively fast), spits out something that the coda package can work with quite easily, and, above all, makes it very easy to specify models and priors. Using JAGS means never having to derive a Gibbs sampler or write out a Metropolis-Hastings algorithm that requires to you to really think about jumping rules. It’s Bayesian statistics for those who don’t have the time/inclination to do it “properly”. It has a few drawbacks, though, such as not being able to specify improper priors (but this could be seen as a feature rather than a bug) with distributions like dflat() and defining a Conditional Autoregressive prior requires specifying it as a multivariate Gaussian. That said, it’s far quicker than using OpenBUGS and JAGS installs fine on any platform. After reading the post’s section on STAN I decided that it was time to give it another go. Downloading the latest version of R and Rtools would surely give me a better experience than last time where it wouldn’t even detect the compiler properly. Putting everything in DOS-friendly file structures with short names meant that everything went off without a hitch and I was able to get the toy example running. Andrew Gelman, one of the developers of STAN, has a &lt;a href=&#34;http://andrewgelman.com/2014/01/21/everything-need-know-bayesian-statistics-learned-eight-schools/&#34;&gt;post on his blog&lt;/a&gt; by Phillip Price about the eight schools example, a really introduction to hierarchical linear modelling and meta-analysis. STAN is a bit more forgiving than JAGS when it comes to priors; any stochastic node that isn’t given a prior is given a flat prior by default. Whether or not &lt;a href=&#34;http://arxiv.org/abs/1403.4630&#34;&gt;Thiago Martins and Dan Simpson&lt;/a&gt; would be happy with that remains to be seen. STAN looks very promising, and it’s been included in the third edition of &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/&#34;&gt;Gelman’s BDA book&lt;/a&gt; (which I still need to buy). The other strategy I tried recently was coding up a Metropolis-Hastings sampler using &lt;a href=&#34;http://theoreticalecology.wordpress.com/2010/09/17/metropolis-hastings-mcmc-in-r/&#34;&gt;the guide from Florian Hartig at Theoretical Ecology&lt;/a&gt;. Choosing a jumping rule was difficult, as I had different parameters to deal with and a single jumping rule wouldn’t do. I tried &lt;a href=&#34;http://projecteuclid.org/euclid.bj/1130077595&#34;&gt;adaptive MCMC&lt;/a&gt; and ended up going down the rabbit hole of log-precisions, block-updates and ended up with very poor mixing and convergence. Finding a decent jumping rule is probably what prevents me from going back to using adaptive MCMC as I did for &lt;a href=&#34;http://eprints.qut.edu.au/72987/&#34;&gt;a book chapter on Bayesian splines&lt;/a&gt;. I eventually settled on writing out a full Gibbs scheme and coding it up in MATLAB. This is very fast (MATLAB’s better at loops than R is) and gives me good convergence. I’m not a fan of MATLAB’s plotting, though, so may end up importing the results into R so I’ve got ggplot2 handy. Big thanks to Zoé van Havre for her help with the Gibbs scheme. I’ve got a PhD student who’s going to be dealing with Bayesian modelling. He’s picking up R quite quickly and is doing his best with Bayesian statistics. It’s all in WinBUGS at the moment, though, which is going to limit the amount of progress we can make. I’d love to be able to code up a bunch of JAGS models and let them run on the supercomputer once we get our great big data set ready for a well-planned set of analyses. I’ve got less time to do the modelling myself these days and find myself wishing I had a clone to do the work. I guess that’s part of the training aspect of PhD supervision, making sure your student can do the implementation when you describe a piece of analysis that you propose. It’s still difficult for me working in a science group as the only statistician, as most of my statistics discussions are people asking for my help rather than us collaborating as equals. I enjoy working with others on interesting modelling problems, and it’d be good to work with other statisticians. While I’m now in the Mathematical Sciences School I don’t think I’ve capitalised yet on the connections I’ve got there in terms of directing my own research down the statistics route. With the UPTECH analysis being the major focus of my research at the moment, it’s tricky to allocate brain space to what I want to be doing next.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2014/04/26/posterior-samples/</link>
      <pubDate>Sat, 26 Apr 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/04/26/posterior-samples/</guid>
      <description>&lt;p&gt;I’m teaching science students how to do statistics. It would be great if we could turn them into Bayesians, especially seeing as we’ve just covered the Agresti-Coull correction for estimating proportions from small experiments. &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/research/published/teachingbayes.pdf&#34;&gt;Andrew Gelman has an interesting paper&lt;/a&gt; on teaching Bayesian statistics to non-statisticians that focuses on the delivery of skills rather than concepts. I would definitely agree with his approach, especially when you consider how he stresses that discussing the model is probably the most important part. NASA have done some work simulating global aerosols and it’s been compiled into &lt;a href=&#34;http://www.itsokaytobesmart.com/post/82630633966/one-of-my-favorite-gifs-of-one-of-my-favorite-nasa&#34;&gt;a neat video&lt;/a&gt; (via &lt;a href=&#34;http://www.itsokaytobesmart.com/&#34;&gt;It’s Okay To Be Smart’s Joe Hanson&lt;/a&gt;). CSIRO have been doing some interesting stuff looking at the production of organic aerosols as well, so this is something I’m paying a bit more attention to at the moment. &lt;a href=&#34;https://www.datacamp.com/&#34;&gt;Datacamp&lt;/a&gt; is a set of online labs for learning to use R, covering the basics of R, data analysis and statistical inference, and computational finance and econometrics. &lt;a href=&#34;https://medium.com/of-games-and-code/d90a50c5d58e&#34;&gt;Learn to be a better coder&lt;/a&gt; by improving your communication skills. The most practical (in terms of coding, at least) aspect of this includes using meaningful names and writing comments that describe what the code does when it’s not clear from the code itself.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2014/02/05/posterior-samples/</link>
      <pubDate>Wed, 05 Feb 2014 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2014/02/05/posterior-samples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://andrewgelman.com/2014/01/21/everything-need-know-bayesian-statistics-learned-eight-schools/&#34;&gt;Everything I need to know about Bayesian Statistics, I learned in eight schools&lt;/a&gt;. At first I thought this meant not really understanding it until having worked in many places with different people but it’s actually a reference to a particular example of hierarchical modelling. &lt;a href=&#34;http://www.youtube.com/watch?v=qRSfxSRdL5Y&#34;&gt;Hadley Wickham talking about dplyr&lt;/a&gt;. Very fancy. &lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34;&gt;I’m finally attempting to install RStan&lt;/a&gt;. My computer is misbehaving. And it’s ARC Discovery Project writing time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples - July wrap</title>
      <link>/./2013/07/29/posterior-samples---july-wrap/</link>
      <pubDate>Mon, 29 Jul 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/07/29/posterior-samples---july-wrap/</guid>
      <description>&lt;p&gt;I had some Posterior Samples to share before going on leave but didn’t get around to posting them. Here’s what’s been on my mind this month: Maths and science units are popular with (Kentucky) students until they realise that &lt;a href=&#34;http://blogs.wsj.com/economics/2013/07/08/math-science-popular-until-students-realize-theyre-hard/&#34;&gt;they’re hard&lt;/a&gt;. While not directly relevant to the Australian university education model it’s probably an important thing for the Science and Engineering Faculty to keep in mind. &lt;a href=&#34;http://blogs.wsj.com/economics/2013/07/08/math-science-popular-until-students-realize-theyre-hard/&#34;&gt;&lt;/a&gt; I’m looking at ggplot2 more these days, so the idea of a “grammar of graphics” is beginning to resonate with me. &lt;a href=&#34;//www.youtube.com/watch?v=xyGggdg31mc&#34;&gt;Here’s a talk&lt;/a&gt; about building one for &lt;a href=&#34;http://clojure.org/&#34;&gt;clojure&lt;/a&gt; (which I don’t use). Something for me to keep in mind when delivering SEB113 slides this semester is &lt;a href=&#34;https://www.dropbox.com/s/p0sgdgo7mxkoj4h/What%20your%20math%20slides%20dont%20need.pdf&#34;&gt;what your maths slides don’t need&lt;/a&gt;. Probably also good pointers for any PhD students graduating soon. &lt;a href=&#34;http://well.blogs.nytimes.com/2013/07/22/the-kitchen-as-a-pollution-hazard/&#34;&gt;An interesting article in the New York Times&lt;/a&gt; about air pollution from cooking. This is something that ILAQH has a research interest in and our nanotracer paper contains a bit of analysis of inhaled surface area dose from particles that originate from cooking. &lt;a href=&#34;http://www.nytimes.com/2013/07/22/business/in-climbing-income-ladder-location-matters.html&#34;&gt;Another NYT article&lt;/a&gt;, this time with a delicious visualisation of the geographical trends in income disparity and social mobility. &lt;a href=&#34;http://www.slate.com/articles/health_and_science/science/2013/07/statistics_and_psychology_multiple_comparisons_give_spurious_results.html&#34;&gt;Andrew Gelman writes at Slate&lt;/a&gt; about some of the problems with scientific publishing and the publication of spurious findings (which isn’t always willingly dishonest). A special “Big Bayes Stories” issue of “Statistical Science” will be published soon, focussing on the real world application of Bayesian statistics where other methods were inapplicable. &lt;a href=&#34;http://xianblog.wordpress.com/2013/07/29/big-bayes-stories/&#34;&gt;Christian Robert has written the preface&lt;/a&gt;; the issue is being edited by Robert, Kerrie Mengersen (one of my PhD supervisors) and Sharon McGrayne, author of “&lt;a href=&#34;http://www.amazon.com/Theory-That-Would-Not-Die/dp/1452636850&#34;&gt;The Theory That Would Not Die&lt;/a&gt;”. Also I went to &lt;a href=&#34;http://www.questacon.edu.au/&#34;&gt;Questacon&lt;/a&gt; and it was awesome.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stop, collaborate and listen</title>
      <link>/./2013/06/27/stop-collaborate-and-listen/</link>
      <pubDate>Thu, 27 Jun 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/06/27/stop-collaborate-and-listen/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://simplystatistics.org/2013/06/25/doing-statistical-research/&#34;&gt;Roger Peng posted at Simply Statistics&lt;/a&gt; about what it is to &lt;a href=&#34;http://stattrak.amstat.org/2013/06/01/how-to-do-statistical-research/&#34;&gt;do statistical research&lt;/a&gt; and how research is essentially solving problems that can’t be solved with the current methods. The message I took from Peng’s post is that often you can 90% solve a problem with current methods and that a lot of the time this is “good enough” and you can come back to the problem later with some new approaches that go beyond the current methods. As part of the UPTECH project I’ve been doing a lot of work with Bayesian hierarchical linear models. While our data has been collected from a panel design (25 schools, two weeks at each) it’s not always appropriate to use a full-blown spatial model. For example, the microbiological work I’ve been doing with my Finnish collaborator is mostly solvable by using exchangeable means priors to estimate classroom level and school level effects. Recently I’ve also had to start looking at clustering techniques, meta-analysis, spatial modelling of high-resolution data, estimating personal exposure, large surveys, and many other applied science problems that require a novel statistical approach. This sort of collaboration/consulting work, according to Terry Speed (whose post Peng is discussing), is a chance to meet lots of people and work on some interesting problems. For me, it has involved learning about existing techniques and trying to figure out how my collaborators and I can apply them to our data to do the best inference we can. With the UPTECH work, there’s always going to be a large list of authors due to the size of the project and the number of people involved in collecting data. Authorship will always be an issue with our papers, both in terms of inclusion and ordering, and we’ve got a decent process in place which makes people aware of papers as they’re finishing up (but not yet ready for submission). My personal belief is that one should always be able to point to a published paper and say “I did that”. Collaboration in applied physics and chemistry seems to be a very different beast to collaboration in statistics and mathematics. Many of the postgraduate students I know in Mathematics have tended to write methods papers with their supervisor(s) and that’s it. There’s the occasional collaboration to apply the method to a problem, but unless you’re working on cross-disciplinary modelling work or a large project involving numerical simulation there doesn’t appear to be much scope for multi-author work. Look back at some of the foundational statistical papers and you’ll see they’ve been written by a single author (some (non-parametric) Bayesian foundations spring to mind: &lt;a href=&#34;http://www.numdam.org/item?id=AIHP_1937__7_1_1_0&#34;&gt;de Finetti, 1937&lt;/a&gt;; &lt;a href=&#34;http://projecteuclid.org/DPubS?service=UI&amp;amp;version=1.0&amp;amp;verb=Display&amp;amp;handle=euclid.pjm/1102992601&#34;&gt;Kingman, 1967&lt;/a&gt;; and &lt;a href=&#34;http://projecteuclid.org/DPubS?service=UI&amp;amp;version=1.0&amp;amp;verb=Display&amp;amp;handle=euclid.aos/1176342360&#34;&gt;Ferguson, 1973&lt;/a&gt;). The question of when to collaborate, with whom, and what it will add is part and parcel of modern science but there are some fields where collaboration is rare and keeping the author list short &lt;a href=&#34;http://andrewgelman.com/2013/06/25/is-there-too-much-coauthorship-in-economics-and-science-more-generally-or-too-little/&#34;&gt;can lead to problems&lt;/a&gt;. Statistical research is necessary when there’s a problem to be solved that is 0% solvable with the current methods. Some of what I’m doing is novel, within the context of aerosol science, but I haven’t done as much stats research in my postdoc as in my PhD. This is no doubt a result of my doing as much collaboration as I am. I get to work on a lot of problems but there’s not much original statistical work in these papers; if I’m lucky I get to do some of the “10%” research. It’s hard to do statistical research in a physics group, especially as the only statistician here. I think if we had a second statistician in the group there’d be a lot more statistics being done both in terms of collaboration/consultation with the scientists and the methods we use to solve problems. The “Airports of the Future” project has quite a number of statisticians working on, among other things, Bayesian Networks, and they’re extending the BN methodology as well as applying it to a novel problem. Two of the members of this team gave a talk at BRAG this morning about visualisation of BN results. This is something I’ll no doubt need to learn about sooner or later as we plan on using BNs with another project that ILAQH is putting together. Four and a half years ago I was under the impression I was joining a physics group to do computational fluid dynamics. Since then I have been learning statistics almost constantly. It’s opened up many more opportunities for collaboration than CFD would have. The trick for me now is to try and put myself in a position where I’m working with other statisticians on statistics. We’ve got some work coming up soon with a more senior statistician at &lt;a href=&#34;http://www.ihbi.qut.edu.au/&#34;&gt;IHBI&lt;/a&gt;, which I hope will bring with it some opportunities for more statistical methodology work. Unrelated PS: The &lt;a href=&#34;http://andrewgelman.com/2013/06/26/dont-buy-bayesian-data-analysis/&#34;&gt;3rd edition of Gelman’s Bayesian Data Analysis&lt;/a&gt; is being released soon, with contributions from David Dunson and Aki Vehtari.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Maturing from student to researcher</title>
      <link>/./2013/06/17/maturing-from-student-to-researcher/</link>
      <pubDate>Mon, 17 Jun 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/06/17/maturing-from-student-to-researcher/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://samclifford.info/2013/06/06/trips-interstate/&#34; title=&#34;Trips interstate&#34;&gt;The other week, when I was in Sydney&lt;/a&gt;, I caught up a friend who’s moved down there and is working in a similar role to me (albeit with a much larger group). He’s got a similar background to me; we both studied mathematics at QUT and focussed on computational and applied mathematics units but we now find ourselves working in (bio-)statistics&lt;em&gt;. I stayed in academia when he went off to work in industry but he has earned a Masters in computational statistics and has picked up Bayesian stats. We both learned Bayesian stats through Gelman, Carlin, Stern and Rubin’s “&lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/&#34;&gt;Bayesian Data Analysis&lt;/a&gt;”, a book which is known to the Bayesian PhD students at QUT as “The Bible”; it’s been used by just about every lecturer that has taught the Honours level Bayesian Data Analysis class. In addition to The Bible, other Bayesian resources I’ve leaned on over the last few years are &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/arm/&#34;&gt;Gelman and Hill’s book on hierarchical models&lt;/a&gt; and &lt;a href=&#34;http://andrewgelman.com/&#34;&gt;Gelman’s blog&lt;/a&gt;. My friend and I got talking about Gelman’s work and how of late we seem to be disagreeing with some of the choices he makes in modelling. For my part, I don’t agree with (or is it understand?) the decisions in Gelman’s Bayesian approach to &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/research/published/econanova3.pdf&#34;&gt;ANOVA&lt;/a&gt; (focussing more on the variance parameters than the means) and the particular parameterisation of the &lt;a href=&#34;http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;amp;id=pdf_1&amp;amp;handle=euclid.ba/1340371048&#34;&gt;global variance parameter&lt;/a&gt; when he discusses the use of a folded non-central &lt;em&gt;t&lt;/em&gt; distribution. Now, it’s not that I think Gelman is wrong where he was previously right or that he’s losing the plot (after all, these papers are years old), but as I read his blog about the models he’s fitting now I’m coming to the realisation that I had been following what he’d been saying and am now looking elsewhere and seeing other ways of doing things. There are many different approaches that each have their strengths and weaknesses and philosophical (and practical) idiosyncrasies. One of the strengths of the Bayesian approach is that the incorporation of priors in the modelling approach gives you a very flexible class of models (hierarchical Bayesian modelling is one of the most useful tools I’ve picked up) and allows you a great amount of freedom in choosing how to build your priors. There is no one correct prior for each problem&lt;sup&gt;§&lt;/sup&gt;; you can use a Jeffreys’ prior if you really want to go down the path of non-informativity or if you’re content with (and can justify) a weakly informative Normal(0, 1e-6) or Gamma(0.001, 0.001). Sometimes you can even choose an appropriately flat prior that results in the posteriors of your parameters having the same distribution as the frequentist approach (where the 95% confidence interval and credible intervals have the same values, but not the same interpretations of course). Sometimes it’s appropriate to elicit a prior from experts or the literature and go for a very informative prior if you don’t have much data in your experiment/observation^. There are lots of different ways to do things, lots of papers pushing different approaches. As a student you tend to look up to people as paragons of the field and go “Well if Gelman did it that way then I’d better do that too”; after four years of study I feel more comfortable looking at something and saying “No, I disagree”. I may not always be doing it the best way possible but I’ll always try to justify what I’ve done both to my collaborators and to the editor/reader of my papers. If it turns out I’ve done something wrong, so be it; I can always try again and learn from the experience. &lt;/em&gt; I’m yet to hear a satisfactory explanation as to what the difference is between a biostatistician and a statistician. § It’s worth checking out some of the ideas of so-called &lt;a href=&#34;http://ba.stat.cmu.edu/journal/2006/vol01/issue03/berger.pdf&#34;&gt;Objective Bayes&lt;/a&gt; if subjectivity is something you’re concerned about. ^ Whatever you do, check your sensitivity to your choice of priors.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2013/04/29/posterior-samples/</link>
      <pubDate>Mon, 29 Apr 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/04/29/posterior-samples/</guid>
      <description>&lt;p&gt;Often we don’t &lt;a href=&#34;http://dynamicecology.wordpress.com/2013/03/18/whats-your-best-technical-trick/&#34;&gt;discuss the various technical tricks&lt;/a&gt; that make the science easier in the methodology sections of our papers. Randall Munroe made &lt;a href=&#34;http://xkcd.com/1205/&#34;&gt;a chart&lt;/a&gt; that helps you figure out whether you’re spending more time working on your time-saving technical trick than you’d actually save. SAMSI &lt;a href=&#34;http://samsiatrtp.wordpress.com/2013/04/25/samsi-hosts-simons-foundation-public-lecture-the-public-health-impact-of-air-pollution-and-climate-change/&#34;&gt;hosted a talk&lt;/a&gt; about the public health impact of climate change. Wish I could have been there. A few (somewhat) recent posts from Andrew Gelman regarding p values&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&#34;line-height:12px;&#34;&gt;&lt;a href=&#34;http://andrewgelman.com/2013/04/28/plain-old-everyday-bayesianism/&#34;&gt;reporting probabilities instead of p values&lt;/a&gt; is very Bayesian&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://andrewgelman.com/2013/02/p-values-and-statistical-practice/&#34;&gt;p values and statistical practice&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I find myself agreeing with a lot of what Gelman says on p values. I think they put the focus of statistics on testing rather than modelling, which is part of why we end up with scientists and others with such poor statistical skills and next to no creativity in terms of the models they fit.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An argument for why careful coding in R is better than an Excel spreadsheet</title>
      <link>/./2013/04/18/an-argument-for-why-careful-coding-in-r-is-better-than-an-excel-spreadsheet/</link>
      <pubDate>Thu, 18 Apr 2013 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2013/04/18/an-argument-for-why-careful-coding-in-r-is-better-than-an-excel-spreadsheet/</guid>
      <description>&lt;p&gt;I spend a fair amount of energy encouraging my coworkers to use R because you can write reusable code that means checking for errors is simple and you can re-run it on different data sets without having to manually copy and paste all your data and fix up any indexing if it’s the wrong length. There’s been a bit of buzz recently about how two economists really dropped the ball when they published a paper showing that high levels of national debt as a percentage of GDP slows economic growth. I won’t write any more about it because it’s been done by better writers and statisticians, but I’d like to bring your attention to the following articles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&#34;line-height:12px;&#34;&gt;&lt;a href=&#34;http://www.nber.org/papers/w15639.pdf&#34;&gt;The original working paper&lt;/a&gt; (National Bureau of Economic Research)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.peri.umass.edu/236/hash/31e2ff374b6377b2ddec04deaa6388b1/publication/566/&#34;&gt;Replicating the results&lt;/a&gt; (Political Economy Research Institute) &lt;a href=&#34;http://www.nextnewdeal.net/rortybomb/researchers-finally-replicated-reinhart-rogoff-and-there-are-serious-problems&#34;&gt;was only possible by making huge methodological and data selection mistakes&lt;/a&gt; (Next New Deal)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.guardian.co.uk/commentisfree/2013/apr/16/unemployment-reinhart-rogoff-arithmetic-cause&#34;&gt;This may have led to massive avoidable unemployment around the world&lt;/a&gt; (The Guardian)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://andrewgelman.com/2013/04/16/memo-to-reinhart-and-rogoff-i-think-its-best-to-admit-your-errors-and-go-on-from-there/&#34;&gt;The authors should gracefully retract, admit they were wrong and move on&lt;/a&gt; (Andrew Gelman’s blog)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://andrewgelman.com/2013/04/17/data-problems-coding-errors-what-can-be-done/?utm_source=feedly&#34;&gt;And to use R&lt;/a&gt; (Phil at Gelman’s blog) &lt;a href=&#34;http://andrewgelman.com/2013/04/17/excel-bashing/?utm_source=feedly&#34;&gt;rather than Excel&lt;/a&gt; (Andrew Gelman)&lt;/li&gt;
&lt;li&gt;There are many reasons to not use Excel, such as it providing the wrong answer for &lt;a href=&#34;http://people.stern.nyu.edu/jsimonof/classes/1305/pdf/excelreg.pdf&#34;&gt;paired t-tests&lt;/a&gt; and &lt;a href=&#34;http://or.nps.edu/faculty/PaulSanchez/oa4333/handouts/Excel/excel2000.pdf&#34;&gt;being terrible at generating Normal random numbers&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2012/11/09/posterior-samples/</link>
      <pubDate>Fri, 09 Nov 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/11/09/posterior-samples/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://simplystatistics.org/post/34563838584/computing-for-data-analysis-simply-statistics-edition&#34;&gt;&lt;span style=&#34;line-height:12px;&#34;&gt;Content from very popular online R course run through Coursera to be published online soon&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.statisticsblog.com/2012/10/recommendation-of-the-week/&#34;&gt;Convinced your fancy analysis technique is the right approach? Run it on noise and see what you get&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://scienceseeker.org/&#34;&gt;Science news from science newsmakers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://statisfaction.wordpress.com/2012/11/06/just-for-the-fun-of-it/&#34;&gt;Should ecologists become Bayesians?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you’ve got a colour graph but want to print it black and white, make sure you check what it looks like first. &lt;a href=&#34;http://i.imgur.com/krzQu.jpg&#34;&gt;Otherwise you might struggle to make your point.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://andrewgelman.com/2012/11/poll-aggregation-and-election-forecasting/&#34;&gt;Andrew Gelman on poll aggregation and election forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.amazon.com/The-Signal-Noise-Predictions-Fail-but/dp/159420411X/ref=zg_bs_books_2&#34;&gt;Nate Silver’s book is #2 on the Amazon best-seller list at the moment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://simplystatistics.org/post/34635539704/on-weather-forecasts-nate-silver-and-the&#34;&gt;Speaking of Nate Silver, here’s a blog post on the politicisation of statistical literacy. (Bonus round: Ctrl-F “Schwimmer”. Extra bonus round: Andrew Gelman comments, saying “I wrote about this on my blog”, just like everyone else on the internet)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Turning tables into graphs</title>
      <link>/./2012/10/30/turning-tables-into-graphs/</link>
      <pubDate>Tue, 30 Oct 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/10/30/turning-tables-into-graphs/</guid>
      <description>&lt;p&gt;One of the things I’ve noticed from working with scientists (of various background) is that they love tables full of means and standard deviations as a way of summarising the variability in some data or regression parameters. &lt;a href=&#34;http://andrewgelman.com/2012/10/communication-is-a-central-task-of-statistics-and-ideally-a-state-of-the-art-data-analysis-can-have-state-of-the-art-displays-to-match/&#34;&gt;Andrew Gelman’s latest discussion of a paper&lt;/a&gt; makes the point that tables of numbers are awful and that a well made graphic does a good job of conveying the uncertainty. He refers in his comment to a paper he wrote, “Let’s Practice What We Preach: Turning Tables into Graphs” [1], which shows how graphs can be better at summarising variability, often in less space than a table. Another thing I really like about the paper is that it endorses the use of R/S/S+ for plotting and faults Excel for not offering enough control to the user (and it makes ugly graphs anyway). I’m a big fan of using graphs because numbers don’t really mean that much to me, especially when dealing with things like splines and random walk models for non-linear function estimation. The UPTECH papers I’m writing on the fungal data and nanotracer measurements have a lot of graphs where previously there were tables or Excel plots which weren’t as easy to interpret. I’ve been spending quite a bit of time on them so that we can present to our readers, for example, just how different the means are in our hierarchical Bayesian model. I think tables have a place and I use them in my own papers. I’m using a table in a spatial modelling paper to describe the prevailing winds and local geography at each of 13 measurement locations. There’s a map of the locations so that I don’t have to put things like “location” in the table. A list of features doesn’t translate as well to a plot as spatial locations do. I don’t think it’s appropriate to list row upon row of means, standard deviations, quantiles, etc. Long/wide tables of model fit criteria such as MSE, AIC, R&lt;sup&gt;2&lt;/sup&gt;, adjusted R&lt;sup&gt;2&lt;/sup&gt;, etc. are incredibly boring and do not scale well when you’re comparing more than, say, three models. I think I might try to send this paper around my group as an attempt to convince them to abandon tables in favour of concise graphs. With the uptake of R among some of the more senior researchers/staff looking promising, I think it’s a message that might actually get some traction. [1] Gelman, Andrew, Pasarica, C., and Dodhia, R. (2002). Let’s practice what we preach: turning tables into graphs. American Statistician 56, 121-130. [&lt;a href=&#34;http://www.stat.columbia.edu/~gelman/research/published/dodhia.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posterior samples</title>
      <link>/./2012/10/27/posterior-samples/</link>
      <pubDate>Sat, 27 Oct 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/10/27/posterior-samples/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;span style=&#34;line-height:12px;&#34;&gt;&lt;a href=&#34;http://www.theatlantic.com/technology/archive/2012/10/scientists-recover-the-sounds-of-19th-century-music-and-laughter-from-the-oldest-playable-american-recording/264147/&#34;&gt;A group at the Lawrence Berkeley National Laboratory manage to digitally model an 1878 recording from one of Edison’s phonographs.&lt;/a&gt; &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.priceonomics.com/post/33713850223/a-dragnet-for-pee-wee&#34;&gt;Using economics to reduce bike theft&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://themonkeycage.org/blog/2012/10/25/steven-levitt-says-that-he-has-a-good-indicator-that-aaron-edlin-noah-kaplan-nate-silver-and-i-are-not-so-smart/&#34;&gt;Andrew Gelman responds to Steven Levitt’s criticism of people who vote thinking that their vote matters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Where to start if you&#39;re going to revise statistics</title>
      <link>/./2012/07/13/where-to-start-if-youre-going-to-revise-statistics/</link>
      <pubDate>Fri, 13 Jul 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/07/13/where-to-start-if-youre-going-to-revise-statistics/</guid>
      <description>&lt;p&gt;I’d like to think it was my plenary speech that has spurred this, but given that they didn’t see it I’m not sure that it was, but one of the academics in my lab has decided it’s time to refresh their statistical knowledge. I think this is great, because they got their PhD a long time ago and have probably been using the same statistical methods for at least the last ten years. The book they’ve decided to use is the &lt;a href=&#34;http://www.amazon.com/Schaums-Outline-Statistics-Murray-Spiegel/dp/0070602816&#34;&gt;Schaum’s Outline of Statistics&lt;/a&gt;. I’ve used books from this range before to revise linear algebra, differential equations, etc. and have even taught small courses (privately) based on the topics they cover and in the order they cover them. A flick through the book confirmed that it was full of frequentist testing and other similar statistical methods that I spent my talk saying were a good start but not the end of the analysis when writing a scientific paper. The book’s online summary says it covers the use of MINITAB. I’m glad to see that it discusses the use of software other than Excel to perform analysis but I recommend people use books in Springer’s “&lt;a href=&#34;http://www.springer.com/series/6991?detailsPage=titles&#34;&gt;Use R!&lt;/a&gt;” range because R is free (in terms of both speech and beer) and is much more flexible than MINITAB in terms of programming it and running different types of analysis. Where MINITAB is based on a point and click GUI, making it great for a first year statistics class where students may not be familiar with programming, R is driven by the command line and is more easily scripted. Learning to use R means giving yourself the opportunity to use the many &lt;a href=&#34;http://cran.r-project.org/web/packages/available_packages_by_name.html&#34;&gt;packages&lt;/a&gt; that extend its functionality. The books I’ve used in the Use R! range include &lt;a href=&#34;http://www.springer.com/statistics/computational+statistics/book/978-0-387-93836-3&#34;&gt;A Beginner’s Guide to R&lt;/a&gt; and &lt;a href=&#34;http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-92297-3&#34;&gt;Bayesian Computation with R&lt;/a&gt;. While I’d definitely recommend these Use R! books, as they’re aimed at people wanting to use R to do better analysis, there are a few others that I’ve found incredibly useful. It’s important for me to point out that my background differs from my colleague’s so they may not find the books as relevant or accessible. &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/&#34;&gt;Gelman et. al - Bayesian Data Analysis&lt;/a&gt;. Around my stats group, this book is called “The Bible”. It’s probably the best textbook I’ve come across. It’s full of information, tutorials, detailed descriptions of the theory and methods and how they can be applied. This is certainly a graduate level statistics textbook, though, and it assumes calculus at what I’d say is probably a second year mathematics degree level. You may find this book difficult if you don’t feel comfortable with multiplying integrals together (which is really what Bayesian analysis is). I sent my colleague a link to &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/research/published/AOS259.pdf&#34;&gt;Gelman’s Annals of Applied Statistics article on ANOVA&lt;/a&gt;. Might as well plug &lt;a href=&#34;http://andrewgelman.com/&#34;&gt;Gelman’s blog&lt;/a&gt; while I’m talking about him. &lt;a href=&#34;http://www.amazon.com/Biostatistics-Bayesian-Introduction-Probability-Statistics/dp/0471468428&#34;&gt;Woodworth - Biostatistics: a Bayesian introduction&lt;/a&gt;. I found this very useful in giving a more applied approach to introductory Bayesian statistics. There’s a good review of the book &lt;a href=&#34;http://www.theannals.com/content/39/7/1376.2.full&#34;&gt;here&lt;/a&gt; and I agree with the reviewers about the importance of the preface in that it talks about the philosophy of statistics in science and discusses the differences between frequentist and Bayesian statistics. The book walks the reader through a lot of the topics which frequentist statistics deals with but in a Bayesian setting. I find this sort of comparison very useful (and appreciate when Mike Jordan says that a lot of machine learning techniques are just Bayesian statistics with a different name) as most people who have taken a statistics class will have seen linear modelling, ANOVA and a little about statistical design. The book also introduces the use of &lt;a href=&#34;http://www.openbugs.info/w/&#34;&gt;WinBUGS&lt;/a&gt; as a tool for Bayesian modelling. As an aside, I attended an introductory course run by my supervisor, Kerrie Mengersen, where she was teaching us how to use R to write a Gibbs sampler for a very simple problem and how to do it in WinBUGS as well. One of the other attendees, the leader of a medical science research group, had it in their head that they would use Excel to write the Gibbs sampler because it provides nice reports (summary stats, plots, etc.) through a plugin they had. Comparing the time it took WinBUGS and R to run the code against the Excel’s run time was probably what convinced me that Excel was one of the worst pieces of software that one could use for statistics. Great for spreadsheets, awful for statistics. A non-technical book which does a good job extending the philosophical discussion to the history of Bayesian statistics and its use in solving some very complex problems is &lt;a href=&#34;http://www.amazon.com/Theory-That-Would-Not-Die/dp/1452636850&#34;&gt;Sharon Bertsch McGrayne’s The Theory That Would Not Die&lt;/a&gt; (which I like to think of as the “A Brief History of Time” of Bayesian statistics). It’s very readable and really drives home the importance of Bayesian statistics and the profundity of Bayes and Laplace in developing this approach. I really don’t think there’s much use revising the basics of frequentism as I disagree with its interpretation of probability and find the idea of confidence intervals problematic. Hypothesis testing is also another problem that I have with frequentism and I think we’re going to see a lot of scientific papers in the near future converting the p values for their ANOVA into a “sigma” level as a result of CERN’s announcement of the 5 sigma certainty of their search for a new boson. Tony O’Hagan has &lt;a href=&#34;http://bayesian.org/forums/news/3648&#34;&gt;a good post&lt;/a&gt; about the “sigma” issue on the ISBA forums. Edited to add: TL;DR? Got a maths degree and some familiarity with stats? Read Gelman. Don’t have a maths degree? Read Woodworth. Want to understand what Bayesian stats is but don’t want to read a textbook? Read McGrayne. Want to know how to use R? Read a Use R! book. Once you’ve got a decent understanding of what statistics is, read papers for specific topics because there is almost never a book about what you want.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gelman and Robert</title>
      <link>/./2012/05/25/gelman-and-robert/</link>
      <pubDate>Fri, 25 May 2012 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>/./2012/05/25/gelman-and-robert/</guid>
      <description>&lt;p&gt;Kerrie Mengersen, one of my supervisors, is visiting some colleagues in France at the moment. It appears that one of the outputs of this visit is a discussion paper, “In praise of the referee”, written with Nicolas Chopin, Andrew Gelman and Christian Robert (&lt;a href=&#34;http://arxiv.org/abs/1205.4304&#34;&gt;arXiv&lt;/a&gt;). There’s been a lot of discussion recently about the role of journals, publishers and reviewers in academic publishing ranging from defending the &lt;em&gt;status quo&lt;/em&gt; to totally overhauling the system by shedding paper journals and moving everything online to a distributed network of institutional ePrints repositories. The paper by Chopin et al. makes two recommendations after a lengthy discussion about where we find ourselves&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Post-publication peer review with comment threads on arXiv and/or a filtering service where instead of acting as reviewers, editorial boards pick out worthwhile new research as a list of recommended reading.&lt;/li&gt;
&lt;li&gt;A reviewer commons where academics are taught how to review and the reviewer reports (and their names) are published alongside the article.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The article is worth a read. I figured I’d also share an old Gelman blog post that he’s recently linked to, “&lt;a href=&#34;http://andrewgelman.com/2009/07/advice_on_writi/&#34;&gt;Advice on writing research articles&lt;/a&gt;”. The seven pieces of general advice are well worth remembering. It’s basically “Start with your conclusions and work backwards towards your methods”. Christian Robert has &lt;a href=&#34;http://xianblog.wordpress.com/2012/05/25/xian-australian-tour-2012/&#34;&gt;published the dates for his Australian tour&lt;/a&gt;. I’m not particularly interested in ABC but the talk on Rao-Blackwellisation looks interesting. Anyone who’s interested in learning a bit about what Bayesian simulation is, without necessarily having a statistics background, would do well to attend the public talk entitled “Simulation as a universal tool for statistics”, which you could probably consider as a popular science talk.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
